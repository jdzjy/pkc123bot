from operator import inv
from pickle import NONE
import requests
import os
import shutil
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import time
import sqlite3
from datetime import datetime, timedelta
from datetime import time as time_datetime
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from p123client import P123Client
# å°è¯•å¼•å…¥å·¥å…·å‡½æ•°ï¼Œå¦‚æœæ–°ç‰ˆä½ç½®æ”¹å˜åˆ™åšå…¼å®¹
try:
    from p123client import check_response
except ImportError:
    # å¦‚æœ check_response ä¸åœ¨é¡¶å±‚ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„å…¼å®¹å‡½æ•°æˆ–æŸ¥æ‰¾æ­£ç¡®è·¯å¾„
    def check_response(resp):
        if resp.get("code") != 0:
            raise Exception(resp.get("message") or "Unknown Error")
from urllib.parse import urlsplit, parse_qs
import re
import telebot
from telebot.types import InlineKeyboardMarkup, InlineKeyboardButton
import threading
import schedule
import json
import logging
from logging.handlers import TimedRotatingFileHandler
from collections import defaultdict
from content_check import check_porn_content
try:
    from pyrogram import Client, filters, idle 
except ImportError:
    logging.error("æœªå®‰è£… pyrogramï¼Œäººå½¢æ¨¡å—å°†æ— æ³•å¯åŠ¨ã€‚è¯· pip install pyrogram tgcrypto")

# è®¾ç½®httpxæ—¥å¿—çº§åˆ«ä¸ºWARNINGï¼Œé¿å…INFOçº§åˆ«çš„è¾“å‡º
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("urllib3.connectionpool").setLevel(logging.ERROR)
logging.getLogger("telebot").setLevel(logging.ERROR)

version = "8.0.4"  
newest_id = 50
# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv(dotenv_path="db/user.env",override=True)
load_dotenv(dotenv_path="sys.env",override=True)
# 1. ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = os.path.join("db", "log")
os.makedirs(log_dir, exist_ok=True)
class MsFormatter(logging.Formatter):
    # é‡å†™æ—¶é—´æ ¼å¼åŒ–æ–¹æ³•
    def formatTime(self, record, datefmt=None):
        # å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºåŒ…å«æ¯«ç§’çš„datetimeå¯¹è±¡
        dt = datetime.fromtimestamp(record.created)
        # æ ¼å¼åŒ–åˆ°æ¯«ç§’ï¼ˆå–å¾®ç§’çš„å‰3ä½ï¼‰
        return dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  # ä¿ç•™åˆ°æ¯«ç§’
# ä½¿ç”¨è‡ªå®šä¹‰çš„Formatter
formatter = MsFormatter(
    fmt='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S.%f'  # è¿™é‡Œå¯ä»¥æ­£å¸¸ä½¿ç”¨%fäº†
)

root_logger = logging.getLogger()  # è·å–æ ¹æ—¥å¿—å™¨
root_logger.setLevel(logging.INFO)  # å…¨å±€æ—¥å¿—çº§åˆ«

# ================= [å¢å¼ºç‰ˆ] å±è”½ Pyrogram Peer id é”™è¯¯ =================
class IgnorePeerIdError(logging.Filter):
    """
    å¼ºåŠ›å±è”½ Pyrogram çš„ Peer id invalid é”™è¯¯
    è¦†ç›–æ—¥å¿—æ¶ˆæ¯æœ¬ä½“å’Œå¼‚å¸¸å †æ ˆè¯¦æƒ…
    """
    def filter(self, record):
        # 1. æ£€æŸ¥æ—¥å¿—çš„ä¸»ä½“æ¶ˆæ¯ (ä½¿ç”¨ getMessage è·å–å®Œæ•´æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²)
        if "Peer id invalid" in record.getMessage():
            return False

        # 2. æ£€æŸ¥å¼‚å¸¸å †æ ˆä¿¡æ¯
        if record.exc_info:
            exc_type, exc_value, exc_traceback = record.exc_info
            # ç›´æ¥æ£€æŸ¥å¼‚å¸¸å¯¹è±¡çš„å€¼
            if exc_value and "Peer id invalid" in str(exc_value):
                return False
            #ä»¥æ­¤é˜²ä¸‡ä¸€ï¼Œæ£€æŸ¥å †æ ˆæ–‡æœ¬
            if "Peer id invalid" in str(exc_traceback):
                return False
                
        return True

# å®šä¹‰éœ€è¦å±è”½çš„ Logger åˆ—è¡¨ (ç²¾å‡†æ‰“å‡»)
target_loggers = [
    "pyrogram", 
    "pyrogram.dispatcher", 
    "pyrogram.session.session",
    "asyncio"
]

# å¾ªç¯åº”ç”¨è¿‡æ»¤å™¨
for logger_name in target_loggers:
    logging.getLogger(logger_name).addFilter(IgnorePeerIdError())

# é¢å¤–ä¿é™©ï¼šå°è¯•ç»™æ ¹ Logger ä¹ŸåŠ ä¸Šï¼ˆé˜²æ­¢æœ‰æ¼ç½‘ä¹‹é±¼å†’æ³¡ä¸Šæ¥ï¼‰
logging.getLogger().addFilter(IgnorePeerIdError())

# ================= æ–°å¢ä»£ç ç»“æŸ =================

if __name__ == "__mp_main__":
    file_handler = TimedRotatingFileHandler(
        filename=os.path.join(log_dir, "log.log"),
        when='D',          # æ¯å¤©è½®è½¬
        interval=1,        # é—´éš”1å¤©
        backupCount=3,     # æœ€å¤šä¿ç•™3å¤©æ—¥å¿—
        encoding='utf-8',
        atTime=time_datetime(0, 0, 1)
    )
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().date()
    # è®¡ç®—ä»Šå¤©çš„atTimeæ—¶é—´æˆ³
    today_at_time = datetime.combine(today, file_handler.atTime).timestamp()
    # å½“å‰æ—¶é—´æˆ³
    now = datetime.now().timestamp()
    # å¦‚æœå½“å‰æ—¶é—´åœ¨ä»Šå¤©çš„atTimeä¹‹å‰ï¼Œåˆ™é¦–æ¬¡è½®è½¬æ—¶é—´ä¸ºä»Šå¤©atTime
    # å¦‚æœå½“å‰æ—¶é—´å·²è¿‡ä»Šå¤©çš„atTimeï¼Œåˆ™é¦–æ¬¡è½®è½¬æ—¶é—´ä¸ºæ˜å¤©atTime
    if now < today_at_time:
        target_rollover = today_at_time
    else:
        target_rollover = datetime.combine(today + timedelta(days=1), file_handler.atTime).timestamp()
    # å¼ºåˆ¶ä¿®æ­£ä¸‹ä¸€æ¬¡è½®è½¬æ—¶é—´
    file_handler.rolloverAt = target_rollover
    
if __name__ == "__main__":
    file_handler = logging.FileHandler(
                        filename=os.path.join(log_dir, "start-log.log"),
                        encoding='utf-8'
                    )
console_handler = logging.StreamHandler()

# 4. å®šä¹‰å…¨å±€æ—¥å¿—æ ¼å¼ï¼ˆæ‰€æœ‰æ—¥å¿—å…±ç”¨ï¼‰
file_handler.setFormatter(formatter)
console_handler.setFormatter(formatter)
# 6. å°†å¤„ç†å™¨æ·»åŠ åˆ°æ ¹æ—¥å¿—å™¨ï¼ˆå…³é”®ï¼šæ ¹æ—¥å¿—å™¨çš„é…ç½®ä¼šè¢«æ‰€æœ‰å­loggerç»§æ‰¿ï¼‰
root_logger.addHandler(file_handler)
root_logger.addHandler(console_handler)
# ----------------------
# æµ‹è¯•ï¼šä»»æ„æ¨¡å—çš„loggeréƒ½ä¼šéµå¾ªå…¨å±€é…ç½®
# ----------------------
# ç¤ºä¾‹1ï¼šå½“å‰æ¨¡å—çš„logger
logger = logging.getLogger(__name__)
import threading
import concurrent.futures
# åˆ›å»ºå¤§å°ä¸º1çš„çº¿ç¨‹æ± ç”¨äºå‘é€æ¶ˆæ¯
reply_thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=20)
# å®‰å…¨åœ°è·å–æ•´æ•°å€¼ï¼Œé¿å…å¼‚å¸¸
def get_int_env(env_name, default_value=0):
    try:
        value = os.getenv(env_name, str(default_value))
        return int(value) if value else default_value
    except (ValueError, TypeError):
        reply_thread_pool.submit(send_message,f"[è­¦å‘Š] ç¯å¢ƒå˜é‡ {env_name} å€¼ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_value}")
        logger.warning(f"ç¯å¢ƒå˜é‡ {env_name} å€¼ä¸æ˜¯æœ‰æ•ˆçš„æ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default_value}")
        return default_value
CHANNEL_URL = os.getenv("ENV_TG_CHANNEL", "")

AUTO_MAKE_JSON = get_int_env("ENV_AUTO_MAKE_JSON", 1)

#TG BOTçš„token
TG_BOT_TOKEN = os.getenv("ENV_TG_BOT_TOKEN", "")
#TG ç”¨æˆ·ID
TG_ADMIN_USER_ID = get_int_env("ENV_TG_ADMIN_USER_ID", 0)

#æ˜¯å¦å¼€å¯ç›‘æ§åŠŸèƒ½ï¼Œ1ä¸ºå¼€å¯ï¼Œ0ä¸ºå…³é—­
AUTHORIZATION = get_int_env("ENV_AUTHORIZATION", 0)
#123è´¦å·
CLIENT_ID = os.getenv("ENV_123_CLIENT_ID", "")
DIY_LINK_PWD = os.getenv("ENV_DIY_LINK_PWD", "")
#123å¯†ç 
CLIENT_SECRET = os.getenv("ENV_123_CLIENT_SECRET", "")
FILTER = os.getenv("ENV_FILTER", "")
filter_pattern = re.compile(FILTER, re.IGNORECASE)
#éœ€è¦è½¬å­˜çš„123ç›®å½•ID
UPLOAD_TARGET_PID = get_int_env("ENV_123_UPLOAD_PID", 0)
# è·å–éœ€è¦è¿‡æ»¤çš„åç¼€åï¼Œé»˜è®¤ä¸ºç©ºï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”
ENV_EXT_FILTER = os.getenv("ENV_EXT_FILTER", "")
# é¢„å¤„ç†ä¸ºå°å†™åˆ—è¡¨ï¼Œä¾‹å¦‚ ['.nfo', '.jpg', '.png']
SKIP_EXTENSIONS = [ext.strip().lower() for ext in ENV_EXT_FILTER.split(',') if ext.strip()]

UPLOAD_JSON_TARGET_PID = get_int_env("ENV_123_JSON_UPLOAD_PID", 0)
UPLOAD_LINK_TARGET_PID = get_int_env("ENV_123_LINK_UPLOAD_PID", UPLOAD_JSON_TARGET_PID)
USERBOT_HELP = '''â•â•â•â•â•å‘½ä»¤â€æè¿°â•â•â•â•

1. æœç´¢å¹¶ç”Ÿæˆå…ƒæ•°æ®å¡ç‰‡ï¼š
å‘é€ï¼š-s123 å…³é”®è¯
(ä¾‹ï¼š-s123 æƒåŠ›çš„æ¸¸æˆ)
åŠŸèƒ½ï¼šæœç´¢èµ„æºï¼Œé€‰æ‹©åè‡ªåŠ¨æŠ“å–TMDBä¿¡æ¯ç”Ÿæˆç²¾ç¾å¡ç‰‡å’ŒJSONã€‚

2. åª’ä½“/é“¾æ¥/JSONè½¬å­˜ï¼š
å‘é€ï¼š-mc (å›å¤ç›®æ ‡æ¶ˆæ¯)
åŠŸèƒ½ï¼šå›å¤ä¸€æ¡åŒ…å« 123é“¾æ¥ æˆ– JSONæ–‡ä»¶ çš„æ¶ˆæ¯å‘é€ -mcï¼Œè‡ªåŠ¨è§£æè½¬å­˜å¹¶ç”Ÿæˆæˆ˜æŠ¥ã€‚'''

DISCLAIMER_TEXT = '''âš ï¸ å…è´£å£°æ˜ & åˆè§„è¯´æ˜

        æœ¬å·¥å…·ä»…ä¸ºæ–¹ä¾¿ç½‘ç›˜åˆ†äº«ä¸è½¬å­˜ï¼Œæ‰€æœ‰èµ„æºå‡æ¥è‡ªç½‘ç»œç”¨æˆ·çš„å…¬å¼€åˆ†äº«å†…å®¹ï¼š
        - å¼€å‘è€…éèµ„æºçš„ä¸Šä¼ è€…ã€æ‰€æœ‰è€…æˆ–ç‰ˆæƒæ–¹ï¼Œä¸å¯¹èµ„æºçš„åˆæ³•æ€§ã€å‡†ç¡®æ€§ã€å®Œæ•´æ€§æ‰¿æ‹…è´£ä»»ã€‚
        - å·¥å…·å†…ç½®AIå†…å®¹è¯†åˆ«æœºåˆ¶ï¼Œè‡ªåŠ¨è¿‡æ»¤æ¶‰æ”¿ã€è‰²æƒ…ã€æš´åŠ›ç­‰è¿è§„èµ„æºçš„åˆ†äº«åˆ›å»ºï¼Œåšå†³æŠµåˆ¶éæ³•å†…å®¹ä¼ æ’­ã€‚

        ç”¨æˆ·åœ¨ä½¿ç”¨æœ¬å·¥å…·æ—¶éœ€çŸ¥æ‚‰ï¼š
        - éœ€è‡ªè¡Œæ ¸å®èµ„æºç‰ˆæƒå½’å±ï¼Œç¡®ä¿åˆè§„ä½¿ç”¨ï¼Œé¿å…ä¾µçŠ¯ç¬¬ä¸‰æ–¹æƒç›Šï¼›
        - å¯¹ä¸‹è½½ã€å­˜å‚¨ã€ä¼ æ’­èµ„æºå¯èƒ½å¼•å‘çš„æ³•å¾‹çº çº·ã€æ•°æ®å®‰å…¨é£é™©ï¼ˆå¦‚ç—…æ¯’æ„ŸæŸ“ï¼‰ç­‰ï¼Œç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…å…¨éƒ¨è´£ä»»ï¼›
        - å¼€å‘è€…ä¸å¯¹ä¸Šè¿°é£é™©å¯¼è‡´çš„ä»»ä½•æŸå¤±æ‰¿æ‹…è´£ä»»ï¼›

        - å¦‚æ‚¨ç»§ç»­ä½¿ç”¨æœ¬å·¥å…·ï¼Œåˆ™è§†ä¸ºå·²å®Œæ•´é˜…è¯»ã€ç†è§£å¹¶æ¥å—ä»¥ä¸Šæ‰€æœ‰å£°æ˜å†…å®¹ã€‚'''
USE_METHOD="ğŸ” ä½¿ç”¨æ–¹æ³•ï¼š\n      1ã€åˆ›å»ºåˆ†äº«è¯·ä½¿ç”¨ /share å…³é”®è¯ æ¥æœç´¢æ–‡ä»¶å¤¹ï¼Œä¾‹å¦‚ï¼š/share æƒåŠ›çš„æ¸¸æˆ\n      2ã€è½¬å­˜åˆ†äº«å¯ç›´æ¥æŠŠ123ã€115ã€å¤©ç¿¼é“¾æ¥è½¬å‘è‡³æ­¤ï¼Œæ”¯æŒé¢‘é“ä¸­å¸¦å›¾ç‰‡çš„é‚£ç§åˆ†äº«\n      3ã€è½¬å­˜ç§’ä¼ jsonå¯ç›´æ¥æŠŠjsonè½¬å‘è‡³æ­¤\n      4ã€è½¬å­˜ç§’ä¼ é“¾æ¥å¯ç›´æ¥æŠŠç§’ä¼ é“¾æ¥è½¬å‘è‡³æ­¤\n      5ã€123æ‰¹é‡ç¦»çº¿ç£åŠ›é“¾è¯·ç›´æ¥æŠŠç£åŠ›é“¾å‘é€è‡³æ­¤\n      6ã€åˆ›å»ºå®Œæˆåˆ†äº«é“¾æ¥åå¯ä¸€é”®å‘å¸–è‡³123èµ„æºç¤¾åŒº\n      7ã€123ã€115ã€å¤©ç¿¼ç­‰é¢‘é“ç›‘æ§è½¬å­˜åœ¨åå°å®šæ—¶æ‰§è¡Œ\n      8ã€PTä¸Šä¸‹è½½çš„æœ¬åœ°æ–‡ä»¶æ— é™å°è¯•ç§’ä¼ 123æˆ–115ç½‘ç›˜ï¼Œä»¥é¿å…è¿è¥å•†åˆ¶è£ï¼Œéœ€è¦é…ç½®composeé‡Œçš„è·¯å¾„æ˜ å°„\n      9ã€è®¿é—® http://127.0.0.1:12366/d/file (ä¾‹å¦‚ http://127.0.0.1:12366/d/æƒåŠ›çš„æ¸¸æˆ.mp4) å³å¯è·å–123æ–‡ä»¶ä¸‹è½½ç›´é“¾\n      10ã€æ”¯æŒmisaka_danmu_serverå¼¹å¹•æœåŠ¡ï¼Œå½“è§¦å‘302æ’­æ”¾æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨misaka_danmu_server APIæ¥ä¸‹è½½å¯¹åº”é›†ä»¥åŠä¸‹ä¸€é›†çš„å¼¹å¹•\n      11ã€æ”¯æŒ123è½¬å­˜å¤¸å…‹åˆ†äº«ï¼ˆåŸç†æ˜¯ä»å¤¸å…‹åˆ†äº«ç”Ÿæˆç§’ä¼ ç»™123è½¬å­˜ï¼‰\nâš ï¸ æ³¨ï¼šä»¥ä¸ŠåŠŸèƒ½çš„ä½¿ç”¨éœ€è¦åœ¨ NasIP:12366ï¼ˆå¦‚192.168.1.1:12366ï¼‰çš„é…ç½®é¡µé¢å®ŒæˆåŠŸèƒ½é…ç½®"
# æ•°æ®åº“è·¯å¾„ï¼ˆä¿æŒä¸å˜ï¼‰
DB_DIR = "db"
if not os.path.exists(DB_DIR):
    os.makedirs(DB_DIR)
DATABASE_FILE = os.path.join(DB_DIR, "TG_monitor-123.db")
USER_STATE_DB = os.path.join(DB_DIR, "user_states.db")
CHECK_INTERVAL = get_int_env("ENV_CHECK_INTERVAL", 0)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15"
]
RETRY_TIMES = 3
TIMEOUT = 15

TOKENSHARE = os.getenv("TOKEN", "")
if TOKENSHARE:
    botshare = telebot.TeleBot(TOKENSHARE)
#TG ç”¨æˆ·ID
    TARGET_CHAT_ID_SHARE = get_int_env("TARGET_CHAT_ID", 0)

from share import get_quality
import re
from urllib.parse import urlparse, parse_qs

def check_ext_filter(filename):
    """
    æ£€æŸ¥æ–‡ä»¶åç¼€æ˜¯å¦åœ¨é»‘åå•ä¸­
    è¿”å› True è¡¨ç¤ºéœ€è¦è·³è¿‡ï¼ŒFalse è¡¨ç¤ºå…è®¸å¤„ç†
    """
    if not SKIP_EXTENSIONS or not filename:
        return False
    
    # è·å–æ–‡ä»¶åç¼€ï¼ˆè½¬å°å†™ï¼‰
    _, ext = os.path.splitext(filename)
    ext = ext.lower()
    
    if ext in SKIP_EXTENSIONS:
        return True
    return False

def parse_share_url(share_url):
    """è§£æåˆ†äº«é“¾æ¥ï¼Œæå–ShareKeyå’Œæå–ç """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ†äº«é“¾æ¥
        pattern = r'(https?://(?:[a-zA-Z0-9-]+\.)*123[a-zA-Z0-9-]*\.[a-z]{2,6}/s/([a-zA-Z0-9\-_]+))'
        match = re.search(pattern, share_url, re.IGNORECASE)

        if match:
            # å®Œæ•´URL
            full_url = match.group(1)
            # ShareKey
            share_key = match.group(2)
            # å°è¯•ä»åŸå§‹URLæŸ¥è¯¢å‚æ•°ä¸­è·å–æå–ç 
            parsed = urlparse(share_url)
            query_params = parse_qs(parsed.query)
            share_pwd = query_params.get('pwd', [''])[0]
            return share_key, share_pwd

        logger.warning(f"æ— æ³•è§£æåˆ†äº«é“¾æ¥: {share_url}")
        return '', ''
    except Exception as e:
        logger.error(f"è§£æåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
        return '', ''

def get_formatted_size(size_in_bytes):
    """å…¨å±€å·¥å…·ï¼šæ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_in_bytes < 1024:
        return f"{size_in_bytes} B"
    elif size_in_bytes < 1024**2:
        return f"{size_in_bytes / 1024:.2f} KB"
    elif size_in_bytes < 1024**3:
        return f"{size_in_bytes / (1024**2):.2f} MB"
    elif size_in_bytes < 1024**4:
        return f"{size_in_bytes / (1024**3):.2f} GB"
    else:
        return f"{size_in_bytes / (1024**4):.2f} TB"

def recursive_count_files(client: P123Client, parent_file_id, share_key, share_pwd):
    """é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶å¹¶ç»Ÿè®¡è§†é¢‘æ–‡ä»¶æ•°é‡"""
    logger.info(f"å¼€å§‹é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶æ•°é‡ï¼Œæ–‡ä»¶å¤¹ID: {parent_file_id}")
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}
    video_count = 0
    try:
        page = 1
        while True:
            # --- ä¿®å¤ï¼šå›é€€åˆ°å…¼å®¹æ€§æ›´å¥½çš„ share_fs_list ---
            resp = client.share_fs_list({
                "ShareKey": share_key,
                "SharePwd": share_pwd,
                "parentFileId": parent_file_id,
                "limit": 100,
                "Page": page
            })
            check_response(resp)
            data = resp["data"]

            if data and "InfoList" in data:
                for item in data["InfoList"]:
                    if item["Type"] == 1:  # ç›®å½•
                        # é€’å½’è®¡ç®—å­ç›®å½•ä¸­çš„è§†é¢‘æ–‡ä»¶
                        video_count += recursive_count_files(client, item["FileId"], share_key, share_pwd)
                    else:  # æ–‡ä»¶
                        # æ£€æŸ¥æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶
                        ext = os.path.splitext(item["FileName"])[1].lower()
                        if ext in video_extensions:
                            video_count += 1
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€åä¸€é¡µ
            if not data or len(data.get("InfoList", [])) < 100:
                break            
            page += 1
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
        raise
    return video_count

def build_share_message(metadata, client, file_id, folder_name, file_name, share_info):
    # ä½¿ç”¨å…ƒæ•°æ®ç¾åŒ–æ¶ˆæ¯
    #logger.info(get_first_video_file(client, file_id))
    get_quality(file_name)

    poster_url = metadata.get('backdrop', '').strip('` ') or metadata.get('poster', '').strip('` ')
    # å†…å®¹ç±»å‹åˆ¤æ–­ 
    content_type = 'ğŸ“º ç”µè§†å‰§' if 'seasons' in metadata and 'episodes' in metadata else 'ğŸ¬ ç”µå½±' 
    # æ„å»ºæ ‡é¢˜è¡Œ 
    share_message = f"{content_type}ï½œ{metadata.get('title')} ({metadata.get('year')})\n\n" 
    # è¯„åˆ† 
    genres = metadata.get('genres', [])[0] if metadata.get('genres', []) else ''
    share_message += f"â­ï¸ è¯„åˆ†: {metadata.get('rating')} / åœ°åŒº: {', '.join(metadata.get('countries', []))} / ç±»å‹: {genres[:15]}{'...' if len(genres) > 15 else ''}\n" 
    # ç±»å‹ 
    #genres = ', '.join(metadata.get('genres', []))
    #share_message += f"ğŸ“½ï¸ ç±»å‹: {genres[:15]}{'...' if len(genres) > 15 else ''}\n" 
    # åœ°åŒº 
    #share_message += f"ğŸŒ åœ°åŒº: {', '.join(metadata.get('countries', []))}\n" 
    # è¯­è¨€ 
    # share_message += f"ğŸ—£ è¯­è¨€: {', '.join(metadata.get('languages', ['æœªçŸ¥']))}\n" 
    # å¯¼æ¼” 
    if metadata.get('director'): 
        share_message += f"ğŸ¬ å¯¼æ¼”: {metadata.get('director', '')[:10]}{'...' if len(metadata.get('director', '')) > 10 else ''}\n" 
    # ä¸»æ¼” 
    share_message += f"ğŸ‘¥ ä¸»æ¼”: {metadata.get('cast', '')[:10]}{'...' if len(metadata.get('cast', '')) > 10 else ''}\n" 
    # é›†æ•°ï¼ˆå¦‚é€‚ç”¨ï¼‰ 
    if 'seasons' in metadata and 'episodes' in metadata: 
        share_message += f"ğŸ“º å…±{metadata.get('seasons')}å­£ ({metadata.get('episodes')}é›†)\n" 
    # ç®€ä»‹ï¼ˆä½¿ç”¨blockquoteï¼‰ 
    # ä»åˆ†äº«é“¾æ¥ä¸­è§£æShareKeyå’Œæå–ç 
    share_key, share_pwd = parse_share_url(share_info['url'])
    share_pwd = share_pwd or share_info.get('password','')  
    # è·å–æ–‡ä»¶å¤¹å†…æ–‡ä»¶åˆ—è¡¨
    files = get_directory_files(client, file_id, folder_name)
    logger.info(f"è·å–å®é™…æ–‡ä»¶æ•°é‡: {len(files)}")
    actual_video_count = recursive_count_files(client, file_id, share_key, share_pwd)
    logger.info(f"è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶æ•°é‡: {actual_video_count}")
    # å®šä¹‰è§†é¢‘æ–‡ä»¶æ‰©å±•å
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}
    # ç­›é€‰è§†é¢‘æ–‡ä»¶
    video_files = []
    for file_info in files:
        filename = file_info["path"]
        ext = os.path.splitext(filename)[1].lower()
        if ext in video_extensions:
            video_files.append(file_info)
    
    if not video_files:
        file_info_text = f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶ | å®é™…è§†é¢‘æ•°é‡: {actual_video_count}"
        file_info_text2 = f"ğŸ“ æ²¡æœ‰æ‰¾åˆ°è§†é¢‘æ–‡ä»¶"
    else:
        total_files_count = len(video_files)
        total_size = sum(file_info["size"] for file_info in video_files)
        # è®¡ç®—å¹³å‡å¤§å°
        avg_size = total_size / total_files_count if total_files_count > 0 else 0
        # æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        if total_size < 1024:
            size_str = f"{total_size} B"
        elif total_size < 1024 * 1024:
            size_str = f"{total_size / 1024:.2f} KB"
        elif total_size < 1024 * 1024 * 1024:
            size_str = f"{total_size / (1024 * 1024):.2f} MB"
        elif total_size < 1024 * 1024 * 1024 * 1024:
            size_str = f"{total_size / (1024 * 1024 * 1024):.2f} GB"
        else:
            size_str = f"{total_size / (1024 * 1024 * 1024 * 1024):.2f} TB"

        avg_size_str = get_formatted_size(avg_size)
        file_info_text = f"ğŸ¬ è§†é¢‘æ•°é‡: {total_files_count} | æ€»å¤§å°: {size_str} | å¹³å‡å¤§å°ï¼š{avg_size_str} | å®é™…è§†é¢‘æ•°é‡: {actual_video_count} | å·²å’Œè°ï¼š{total_files_count-actual_video_count}"
        file_info_text2 = f"ğŸ¬ è§†é¢‘æ•°é‡: {total_files_count} | æ€»å¤§å°: {size_str} | å¹³å‡å¤§å°ï¼š{avg_size_str}" 
    share_message2 = share_message
    share_message2 += f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:500]}{'...' if len(metadata.get('plot')) > 500 else ''}</blockquote>\n\n{file_info_text2}\n"
    share_message += f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:500]}{'...' if len(metadata.get('plot')) > 500 else ''}</blockquote>\n\n{file_info_text}\n" 
    quality = get_quality(get_first_video_file(client, file_id))
    if quality:
        share_message += f"ğŸ· è§†é¢‘è´¨é‡: {quality}\n"
        share_message2 += f"ğŸ· è§†é¢‘è´¨é‡: {quality}\n"
    share_message += f"ğŸ”— é“¾æ¥: {share_info['url']}{'?pwd=' + share_info['password'] if share_info.get('password') else ''}\n" 
    #share_message += f"ğŸ”— é“¾æ¥: <a href=\"{share_info['url']}{'?pwd=' + share_info['password'] if share_info.get('password') else ''}\" target=\"_blank\" rel=\"noopener\" onclick=\"return confirm('Open this link?\n\n'+this.href);\">æŸ¥çœ‹é“¾æ¥</a>\n"
    share_message += f"ğŸ™‹ æ¥è‡ª123botè‡ªåŠ¨åˆ›å»ºçš„åˆ†äº«" 
    share_message2 += f"ğŸ™‹ æ¥è‡ª123botè‡ªåŠ¨åˆ›å»ºçš„åˆ†äº«" 
    return share_message, share_message2, poster_url, files

def get_directory_files(client: P123Client, directory_id, folder_name, current_path="", is_root=True):
    """
    è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆä½¿ç”¨V2 APIï¼‰
    directory_id: ç›®å½•ID
    folder_name: æ–‡ä»¶å¤¹åç§°
    current_path: å½“å‰è·¯å¾„ï¼Œç”¨äºæ„å»ºå®Œæ•´çš„ç›¸å¯¹è·¯å¾„
    """
    # å¯¹äºæ ¹ç›®å½•ï¼ŒcommonPathå°±æ˜¯folder_name
    # å¯¹äºå­ç›®å½•ï¼Œcurrent_pathæ˜¯ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    if is_root:
        common_path = folder_name
        # æ ¹ç›®å½•çš„current_pathä¸ºç©º
        current_path = ""
    else:
        common_path = current_path.split('/')[0] if current_path else folder_name

    # æ„å»ºå½“å‰ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    # å¯¹äºæ ¹ç›®å½•ï¼Œrelative_pathä¸ºç©º
    # å¯¹äºå­ç›®å½•ï¼Œrelative_pathæ˜¯ç›¸å¯¹äºcommonPathçš„è·¯å¾„
    if is_root:
        relative_path = ""
    else:
        relative_path = f"{current_path}/{folder_name}" if current_path else folder_name
        # ç§»é™¤å¼€å¤´å¯èƒ½çš„/
        relative_path = relative_path.lstrip('/')
    logger.info(f"è·å–ç›®å½•å†…å®¹ (ID: {directory_id}, commonPath: '{common_path}', ç›¸å¯¹è·¯å¾„: '{relative_path}')")
    all_files = []
    OPEN_API_HOST = "https://open-api.123pan.com"
    API_PATHS = {
        'LIST_FILES_V2': '/api/v2/file/list'
    }
    retry_delay = 31  # é‡è¯•å»¶è¿Ÿç§’æ•°

    # ä½¿ç”¨V2 APIè·å–ç›®å½•å†…å®¹
    last_file_id = 0  # åˆå§‹å€¼ä¸º0
    while True:
        url = f"{OPEN_API_HOST}{API_PATHS['LIST_FILES_V2']}"
        params = {
            "parentFileId": directory_id,
            "trashed": 0,  # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
            "limit": 100,   # æœ€å¤§ä¸è¶…è¿‡100
            "lastFileId": last_file_id
        }
        headers = {
            "Authorization": f"Bearer {client.token}",
            "Platform": "open_platform",
            "Content-Type": "application/json"
        }

        try:
            logger.info(f"è¯·æ±‚ç›®å½•åˆ—è¡¨: {url}, å‚æ•°: {params}")
            response = requests.get(url, params=params, headers=headers, timeout=30)
            if not response:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥")
                return all_files

            if response.status_code != 200:
                logger.error(f"è·å–ç›®å½•åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}")
                return all_files

            try:
                data = response.json()
            except json.JSONDecodeError as e:
                logger.error(f"å“åº”JSONè§£æå¤±è´¥: {str(e)}")
                logger.error(f"å®Œæ•´å“åº”: {response.text}")
                return all_files

            if data.get("code") != 0:
                error_msg = data.get("message", "æœªçŸ¥é”™è¯¯")
                
                # å¦‚æœæ˜¯é™æµé”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                if "æ“ä½œé¢‘ç¹" in error_msg or "é™æµ" in error_msg:
                    logger.warning(f"APIé™æµ: {error_msg}, ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                
                logger.error(f"APIé”™è¯¯: {error_msg}")
                return all_files

            # å¤„ç†å½“å‰é¡µçš„æ–‡ä»¶
            for item in data["data"].get("fileList", []):
                # æ’é™¤å›æ”¶ç«™æ–‡ä»¶
                if item.get("trashed", 1) != 0:
                    continue
                
                # æ„å»ºæ–‡ä»¶ç›¸å¯¹è·¯å¾„
                item_path = item['filename']
                
                if item["type"] == 0:  # æ–‡ä»¶
                    # æ„å»ºç›¸å¯¹äºcommonPathçš„è·¯å¾„ï¼ˆä½¿ç”¨/ä½œä¸ºåˆ†éš”ç¬¦ï¼‰
                    # æ³¨æ„ï¼šä¸åŒ…å«commonPath
                    if relative_path:
                        full_item_path = f"{relative_path}/{item_path}"
                    else:
                        full_item_path = item_path
                    # ç¡®ä¿ä½¿ç”¨/ä½œä¸ºåˆ†éš”ç¬¦
                    full_item_path = full_item_path.replace('\\', '/')
                    file_info = {
                        "path": full_item_path,  # å­˜å‚¨ç›¸å¯¹äºcommonPathçš„è·¯å¾„
                        "etag": item["etag"],
                        "size": item["size"]
                    }
                    all_files.append(file_info)
                elif item["type"] == 1:  # æ–‡ä»¶å¤¹
                    # é€’å½’è·å–å­ç›®å½•ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é™æµï¼‰
                    #time.sleep(0.05)  # å¢åŠ å»¶è¿Ÿ
                    sub_files = get_directory_files(
                        client,
                        item["fileId"],
                        item['filename'],
                        relative_path,
                        False
                    )
                    all_files.extend(sub_files)

            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šé¡µé¢
            last_file_id = data["data"].get("lastFileId", -1)
            #time.sleep(0.05)
            if last_file_id == -1:
                break
                
        except Exception as e:
            logger.error(f"è·å–ç›®å½•åˆ—è¡¨å‡ºé”™: {str(e)}")
            return all_files

    logger.info(f"æ‰¾åˆ° {len(all_files)} ä¸ªæ–‡ä»¶ (ID: {directory_id})")
    return all_files

# å…¨å±€å˜é‡ï¼ˆä½¿ç”¨å®‰å…¨çš„æ–¹å¼åˆå§‹åŒ–botï¼‰
# å¤„ç†JSONæ–‡ä»¶è½¬å­˜

import time
# åˆ›å»ºé”å¯¹è±¡ç¡®ä¿æ–‡ä»¶ä¾æ¬¡è½¬å­˜
json_process_lock = threading.Lock()

# è·Ÿè¸ªä¸Šæ¬¡å‘é€æ¶ˆæ¯çš„æ—¶é—´
last_send_time = 0
RETRY_DELAY = 60  # é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
MAX_RETRIES = 30  # æœ€å¤§é‡è¯•æ¬¡æ•°
# å®šä¹‰çº¿ç¨‹æ± ä¸­çš„å‘é€å‡½æ•°
def send_message(text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.send_message(TG_ADMIN_USER_ID, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {TG_ADMIN_USER_ID}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_message_with_id(chatid, text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.send_message(chatid, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {chatid}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_reply(message, text):
    send_retry_count = 0
    while send_retry_count < MAX_RETRIES:
        try:
            bot.reply_to(message, text)
            logger.info(f"æ¶ˆæ¯ '{text.replace('\n', '').replace('\r', '')[:20]}...' ï¼Œå·²æˆåŠŸå‘é€ç»™ç”¨æˆ· {message.chat.id}ï¼ˆç¬¬{send_retry_count+1}/{MAX_RETRIES}æ¬¡å°è¯•ï¼‰")
            break
        except Exception as e:
            logger.error(f"å‘é€å›å¤å¤±è´¥ï¼Œ{RETRY_DELAY}ç§’åé‡å‘ï¼Œæ¶ˆæ¯ï¼š{text}ï¼Œé”™è¯¯ï¼š{str(e)}")
            time.sleep(RETRY_DELAY)
            send_retry_count += 1

def send_reply_delete(message, text):
    global last_send_time
    current_time = time.time()
    if current_time - last_send_time < 10:
        #logger.info(f"[èŠ‚æµ] 10ç§’å†…å·²å‘é€æ¶ˆæ¯ï¼Œå¿½ç•¥å½“å‰æ¶ˆæ¯: {text}")
        return
    # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œä¿ç•™å¼€å¤´å’Œæœ«å°¾çš„200å­—ç¬¦
    max_length = 400
    if len(text) > max_length:
        text = text[:200] + '\n     ......\n' + text[-200:]  
    try:
        sent_message = bot.reply_to(message, text)
        # æ›´æ–°ä¸Šæ¬¡å‘é€æ—¶é—´
        last_send_time = current_time
        time.sleep(12)  # ç­‰å¾…10ç§’ååˆ é™¤æ¶ˆæ¯
        bot.delete_message(chat_id=sent_message.chat.id, message_id=sent_message.message_id)
    except Exception as e:
        logger.error(f"å‘é€å›å¤å¤±è´¥: {str(e)}")
bot = telebot.TeleBot(TG_BOT_TOKEN)
from telebot.types import BotCommand
# å®‰å…¨åˆå§‹åŒ–TeleBot
while True and __name__ == "__mp_main__":
    try:
        bot = telebot.TeleBot(TG_BOT_TOKEN)
        # å®šä¹‰å‘½ä»¤èœå•ï¼ˆåŒ…å«/startå’Œ/shareï¼‰
        commands = [
            BotCommand("start", "å¼€å§‹ä½¿ç”¨æœºå™¨äºº"),
            BotCommand("share", "åˆ›å»ºåˆ†äº«é“¾æ¥"),
            BotCommand("sync189", "å¤©ç¿¼è½¬å­˜æ–‡ä»¶å¤¹ç§’ä¼ åˆ°123ç›˜è½¬å­˜æ–‡ä»¶å¤¹"),
            BotCommand("info", "æ‰“å°å½“å‰è´¦æˆ·çš„ä¿¡æ¯"),
            BotCommand("add", "æ·»åŠ 123ç›‘æ§è¿‡æ»¤è¯ï¼Œå‘é€/addå¯æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•"),
            BotCommand("remove", "åˆ é™¤123ç›‘æ§è¿‡æ»¤è¯ï¼Œå‘é€/removeå¯æŸ¥çœ‹ä½¿ç”¨æ–¹æ³•")
        ]
        # è®¾ç½®å‘½ä»¤èœå•
        bot.set_my_commands(commands)
        logger.info("å·²è®¾ç½®Botå‘½ä»¤èœå•ï¼š/start, /share, /info, /add, /remove")
        logger.info("TeleBotåˆå§‹åŒ–æˆåŠŸ")
        break  # åˆå§‹åŒ–æˆåŠŸï¼Œé€€å‡ºå¾ªç¯
    except Exception as e:
        logger.error(f"ç”±äºç½‘ç»œç­‰åŸå› æ— æ³•ä¸TG Botå»ºç«‹é€šä¿¡ï¼Œ30ç§’åé‡è¯•...: {str(e)}")
        time.sleep(30)

# åˆå§‹åŒ–123å®¢æˆ·ç«¯
def init_123_client(retry: bool = False) -> P123Client:
    import requests
    token_path = os.path.join(DB_DIR, "config.txt")
    token = None
    
    # å°è¯•åŠ è½½æŒä¹…åŒ–çš„token
    if os.path.exists(token_path):
        try:
            with open(token_path, "r", encoding="utf-8") as f:
                token = f.read().strip()
            logger.info("å·²åŠ è½½æŒä¹…åŒ–token")
        except Exception as e:
            logger.warning(f"è¯»å–tokenæ–‡ä»¶å¤±è´¥ï¼š{e}ï¼Œå°†é‡æ–°è·å–")
    
    # å°è¯•ä½¿ç”¨tokenåˆå§‹åŒ–å®¢æˆ·ç«¯
    if token:
        while True:
            try:
                # --- ä¿®æ­£ï¼šç§»é™¤ app='web' å‚æ•° ---
                client = P123Client(token=token)
                # éªŒè¯tokenæœ‰æ•ˆæ€§
                try:
                    # å°è¯•è°ƒç”¨ç”¨æˆ·ä¿¡æ¯æ¥å£éªŒè¯
                    res = client.passport_user_info()
                except AttributeError:
                    # å…¼å®¹æ—§ç‰ˆæœ¬
                    res = client.user_info()

                # æ£€æŸ¥APIè¿”å›ç»“æœæ˜¯å¦è¡¨ç¤ºtokenè¿‡æœŸ
                if res.get('code') != 0 or res.get('message') != "ok":
                    reply_thread_pool.submit(send_message, "123 tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    logger.info("æ£€æµ‹åˆ°tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    if os.path.exists(token_path):
                        os.remove(token_path)
                    break
                else:
                    logger.info("123å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æŒä¹…åŒ–tokenï¼‰")
                    return client
            except Exception as e:
                if "token is expired" in str(e).lower() or (
                        hasattr(e, 'args') and "token is expired" in str(e.args).lower()):
                    logger.info("æ£€æµ‹åˆ°tokenè¿‡æœŸï¼Œå°†é‡æ–°è·å–")
                    if os.path.exists(token_path):
                        os.remove(token_path)
                    break
                else:
                    logger.warning(f"tokenå¥åº·æ£€æŸ¥å¼‚å¸¸ï¼Œç¨åé‡è¯•ï¼š{e}")
                    time.sleep(RETRY_DELAY)
                    # å¦‚æœæ˜¯ç½‘ç»œé”™è¯¯ç­‰éè¿‡æœŸé”™è¯¯ï¼Œè·³å‡ºå¾ªç¯è®©å…¶å°è¯•é‡æ–°ç™»å½•æˆ–é‡è¯•
                    break 

    # é€šè¿‡APIæ¥å£è·å–æ–°token
    try:
        # --- ä¿®æ­£ï¼šç§»é™¤ app='web' å‚æ•° ---
        client = P123Client(CLIENT_ID, CLIENT_SECRET)
        
        with open(token_path, "w", encoding="utf-8") as f:
            f.write(client.token)

        logger.info("123å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æ–°è·å–çš„tokenï¼‰")
        return client
    except Exception as e:
        if not retry:
            logger.error(f"è·å–tokenå¤±è´¥ï¼š{e}ï¼Œå°è¯•é‡è¯•...")
            return init_123_client(retry=True)
        logger.error(f"è·å–tokenå¤±è´¥ï¼ˆå·²é‡è¯•ï¼‰ï¼š{e}")
        raise

# æ•°æ®åº“ç›¸å…³å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼‰
def init_database():
    conn = sqlite3.connect(DATABASE_FILE)
    conn.execute('''CREATE TABLE IF NOT EXISTS messages
                  (msg_id INTEGER PRIMARY KEY AUTOINCREMENT, id TEXT, date TEXT, message_url TEXT, target_url TEXT, 
                   transfer_status TEXT, transfer_time TEXT, transfer_result TEXT)''')
    conn.commit()
    conn.close()


def is_message_processed(message_url):
    """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å¤„ç†ï¼ˆæ— è®ºè½¬å­˜æ˜¯å¦æˆåŠŸï¼‰"""
    conn = sqlite3.connect(DATABASE_FILE)
    result = conn.execute("SELECT 1 FROM messages WHERE message_url = ?",
                          (message_url,)).fetchone()
    conn.close()
    return result is not None


def save_message(message_id, date, message_url, target_url,
                 status="å¾…è½¬å­˜", result="", transfer_time=None):
    conn = sqlite3.connect(DATABASE_FILE)
    try:
        conn.execute("INSERT INTO messages (id, date, message_url, target_url, transfer_status, transfer_time, transfer_result) VALUES (?, ?, ?, ?, ?, ?, ?)",
                     (message_id, date, message_url, target_url,
                      status, transfer_time or datetime.now().isoformat(), result))
        conn.commit()
        logger.info(f"å·²è®°å½•: {message_id} | {target_url} | çŠ¶æ€: {status}")
    except sqlite3.IntegrityError:
        conn.execute("UPDATE messages SET transfer_status=?, transfer_result=?, transfer_time=? WHERE id=?",
                     (status, result, transfer_time or datetime.now().isoformat(), message_id))
        conn.commit()
    finally:
        conn.close()


# è·å–æœ€æ–°æ¶ˆæ¯ï¼ˆä¿æŒä¸å˜ï¼‰
def get_latest_messages():
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–å¤šä¸ªé¢‘é“é“¾æ¥
        channel_urls = os.getenv("ENV_TG_CHANNEL", "").split('|')
        if not channel_urls or channel_urls == ['']:
            logger.warning("æœªè®¾ç½®ENV_TG_CHANNELç¯å¢ƒå˜é‡")
            return []
            
        all_new_messages = []
        
        # å¯¹æ¯ä¸ªé¢‘é“é“¾æ¥æ‰§è¡Œè·å–æ¶ˆæ¯é€»è¾‘
        for channel_url in channel_urls:
            channel_url = channel_url.strip()
            if not channel_url:
                continue
                
            # é¢„å¤„ç†channel_urlï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            if channel_url.startswith('https://t.me/') and '/s/' not in channel_url:
                # æå–é¢‘é“åç§°éƒ¨åˆ†
                channel_name = channel_url.split('https://t.me/')[-1]
                # é‡æ„URLï¼Œæ·»åŠ /s/
                channel_url = f'https://t.me/s/{channel_name}'
            logger.info(f"===== å¤„ç†é¢‘é“: {channel_url} =====")
            
            session = requests.Session()
            retry = Retry(total=RETRY_TIMES, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
            session.mount("https://", HTTPAdapter(max_retries=retry))
            headers = {"User-Agent": USER_AGENTS[int(time.time()) % len(USER_AGENTS)]}
            response = session.get(channel_url, headers=headers, timeout=TIMEOUT)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            message_divs = soup.find_all('div', class_='tgme_widget_message')
            total = len(message_divs)
            logger.info(f"å…±è§£æåˆ°{total}æ¡æ¶ˆæ¯ï¼ˆæœ€æ–°çš„åœ¨æœ€åï¼‰")
            new_messages = []
            for i in range(total):
                msg_index = total - 1 - i
                msg = message_divs[msg_index]
                data_post = msg.get('data-post', '')
                message_id = data_post.split('/')[-1] if data_post else f"æœªçŸ¥ID_{msg_index}"
                logger.info(f"æ£€æŸ¥ç¬¬{i + 1}æ–°æ¶ˆæ¯ï¼ˆå€’æ•°ç¬¬{i + 1}æ¡ï¼ŒID: {message_id}ï¼‰")
                time_elem = msg.find('time')
                date_str = time_elem.get('datetime') if time_elem else datetime.now().isoformat()
                link_elem = msg.find('a', class_='tgme_widget_message_date')
                message_url = f"{link_elem.get('href').lstrip('/')}" if link_elem else ''
                text_elem = msg.find('div', class_='tgme_widget_message_text')
                #print(str(text_elem))
                if text_elem:
                    message_text = text_elem.get_text(separator='\\n', strip=True)
                    target_urls = extract_target_url(f"{msg}")
                    if target_urls:
                        for url in target_urls:
                            # æ£€æŸ¥æ˜¯å¦æœ‰æå–ç ä½†URLä¸­æ²¡æœ‰pwdå‚æ•°
                            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', str(text_elem), re.IGNORECASE)
                            if pwd_match and 'pwd=' not in url:
                                pwd = pwd_match.group(1)
                                # ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼Œæ·»åŠ pwdå‚æ•°
                                if '?' in url:
                                    url = f"{url}&pwd={pwd}"
                                else:
                                    url = f"{url}?pwd={pwd}"
                                logger.info(f"å·²ä¸ºURLæ·»åŠ æå–ç : {url}")
                            if not is_message_processed(message_url):
                                new_messages.append((message_id, date_str, message_url, url, message_text))                               
                            else:
                                logger.info(f"ç¬¬{i + 1}æ–°æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡")
                            #print(f"tgæ¶ˆæ¯é“¾æ¥ï¼š{message_url}")
                            #print(f"123é“¾æ¥ï¼š{url}")
                    else:
                        if not is_message_processed(message_url):
                            new_messages.append((message_id, date_str, message_url, "", message_text))
                        else:
                            logger.info(f"ç¬¬{i + 1}æ–°æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡")                       
                        #print("æœªå‘ç°ç›®æ ‡123é“¾æ¥")
            new_messages.reverse()
            logger.info(f"å‘ç°{len(new_messages)}æ¡æ–°çš„123åˆ†äº«é“¾æ¥")
            all_new_messages.extend(new_messages)
        
        # æŒ‰æ—¶é—´æ’åºæ‰€æœ‰æ¶ˆæ¯
        all_new_messages.sort(key=lambda x: x[1])
        logger.info(f"===== æ‰€æœ‰é¢‘é“å…±å‘ç°{len(all_new_messages)}æ¡æ–°çš„123åˆ†äº«é“¾æ¥ =====")
        return all_new_messages
    except requests.exceptions.RequestException as e:
        logger.error(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)[:100]}")
        return []


def extract_target_url(text):
    pattern = r'https?:\/\/(?:www\.)?123(?:\d+|pan)\.\w+\/s\/[\w-]+(?:\?pwd=\w+|(?:\s*æå–ç \s*[:ï¼š]\s*\w+))?'
    matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
    if matches:
        # å»é™¤é‡å¤é“¾æ¥
        unique_matches = list(set([match.strip() for match in matches]))
        return unique_matches
    return []


# è½¬å­˜åˆ†äº«é“¾æ¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
from collections import defaultdict, deque
def transfer_shared_link_optimize(client: P123Client, target_url: str, UPLOAD_TARGET_PID: int | str) -> bool:
    parsed_url = urlsplit(target_url)
    if '/s/' in parsed_url.path:
        after_s = parsed_url.path.split('/s/')[-1]
        temp_key = after_s.split('/')[0]
        pwd_sep_index = re.search(r'æå–ç [:ï¼š]', temp_key)
        share_key = temp_key[:pwd_sep_index.start()].strip() if pwd_sep_index else temp_key
    else:
        share_key = None
    if not share_key:
        logger.error(f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        return False

    # è§£æå¯†ç 
    query_params = parse_qs(parsed_url.query)
    share_pwd = query_params.get('pwd', [None])[0]
    if not share_pwd:
        pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', parsed_url.path, re.IGNORECASE)
        if not pwd_match:
            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', target_url, re.IGNORECASE)
        share_pwd = pwd_match.group(1) if pwd_match else ""

    all_items = []

    def recursive_fetch(parent_file_id: int = 0) -> None:
        """é€’å½’è·å–åˆ†äº«ä¸­çš„æ–‡ä»¶å’Œç›®å½•"""
        try:
            page = 1
            while True:
                # --- ä¿®å¤ï¼šå›é€€åˆ°å…¼å®¹æ€§æ›´å¥½çš„ share_fs_list ---
                resp = client.share_fs_list({
                    "ShareKey": share_key,
                    "SharePwd": share_pwd,
                    "parentFileId": parent_file_id,
                    "limit": 100,
                    "Page": page
                })
                check_response(resp)
                data = resp["data"]
                
                if data and "InfoList" in data:
                    for item in data["InfoList"]:
                        all_items.append({
                            "file_id": item["FileId"],
                            "name": item["FileName"],
                            "etag": item.get("Etag", ""),
                            "parent_dir_id": parent_file_id,
                            "size": item.get("Size", 0),
                            "Type": item["Type"]
                        })
                if not data or len(data.get("InfoList", [])) < 100:
                    break
                page += 1
        except Exception as e:
            logger.error(f"è·å–åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
            raise
    try:
        recursive_fetch()
        file_count = sum(1 for item in all_items if item["Type"] != 1)
        dir_count = sum(1 for item in all_items if item["Type"] == 1)
        logger.info(f"å…±å‘ç°{file_count}ä¸ªæ–‡ä»¶å’Œ{dir_count}ä¸ªç›®å½•ï¼Œå‡†å¤‡è½¬å­˜")
    except Exception as e:
        logger.error(f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        return False
    
    fileList = []
    for item in all_items:
        # å¦‚æœæ˜¯æ–‡ä»¶ä¸”åç¼€åœ¨é»‘åå•ä¸­ï¼Œåˆ™è·³è¿‡
        if item["Type"] != 1 and check_ext_filter(item["name"]):
            logger.info(f"ğŸš« æ ¹æ®é…ç½®è·³è¿‡æ–‡ä»¶: {item['name']}")
            continue
            
        fileList.append({
            "fileID": item["file_id"],
            "size": item["size"],
            "etag": item["etag"],
            "type": item["Type"],
            "parentFileID": UPLOAD_TARGET_PID,
            "fileName": item["name"],
            "driveID": 0
        })

    if not fileList:
        logger.warning("è¿‡æ»¤åæ²¡æœ‰æ–‡ä»¶éœ€è¦è½¬å­˜")
        return False

    logger.info(f"å‡†å¤‡è½¬å­˜æ–‡ä»¶åˆ—è¡¨åˆ°ç›®å½•: {UPLOAD_TARGET_PID}")

    try:
        # ä¿æŒåŸç”Ÿ requests è°ƒç”¨ï¼Œè¿™æ˜¯æœ€ç¨³å¦¥çš„æ‰¹é‡è½¬å­˜æ–¹å¼
        url = "https://www.123pan.com/b/api/restful/goapi/v1/file/copy/save"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {client.token}"
        }
        payload = {
            "fileList": fileList,
            "shareKey": share_key,
            "sharePwd": share_pwd,
            "currentLevel": 0
        }
        response = requests.post(url, json=payload, headers=headers)
        if response.status_code == 200:
            response_json = response.json()
            if response_json.get("message") == "ok":
                logger.info(f"{target_url} è½¬å­˜æˆåŠŸ")
                return True
            else:
                logger.error(f"è½¬å­˜å¤±è´¥: {response_json.get('message')}")
                return False
        else:
            logger.error(f"è¯·æ±‚å¤±è´¥: {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"è½¬å­˜é”™è¯¯: {str(e)}")
        return False

def transfer_shared_link(client: P123Client, target_url: str, UPLOAD_TARGET_PID: int | str) -> bool:
    parsed_url = urlsplit(target_url)
    if '/s/' in parsed_url.path:
        after_s = parsed_url.path.split('/s/')[-1]
        temp_key = after_s.split('/')[0]
        pwd_sep_index = re.search(r'æå–ç [:ï¼š]', temp_key)
        share_key = temp_key[:pwd_sep_index.start()].strip() if pwd_sep_index else temp_key
    else:
        share_key = None
    if not share_key:
        logger.error(f"æ— æ•ˆçš„åˆ†äº«é“¾æ¥: {target_url}")
        return False

    query_params = parse_qs(parsed_url.query)
    share_pwd = query_params.get('pwd', [None])[0]
    if not share_pwd:
        pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', parsed_url.path, re.IGNORECASE)
        if not pwd_match:
            pwd_match = re.search(r'æå–ç \s*[:ï¼š]\s*(\w+)', target_url, re.IGNORECASE)
        share_pwd = pwd_match.group(1) if pwd_match else ""

    all_dirs = []
    all_files = []

    def recursive_fetch(parent_file_id: int = 0) -> None:
        try:
            page = 1
            while True:
                # --- ä¿®å¤ï¼šå›é€€åˆ° share_fs_list ---
                resp = client.share_fs_list({
                    "ShareKey": share_key,
                    "SharePwd": share_pwd,
                    "parentFileId": parent_file_id,
                    "limit": 100,
                    "Page": page
                })
                check_response(resp)
                data = resp["data"]
                
                if data and "InfoList" in data:
                    for item in data["InfoList"]:
                        if item["Type"] == 1:
                            all_dirs.append({
                                "dir_id": item["FileId"],
                                "name": item["FileName"],
                                "parent_dir_id": parent_file_id
                            })
                            recursive_fetch(item["FileId"])
                        else:
                            all_files.append({
                                "file_id": item["FileId"],
                                "name": item["FileName"],
                                "etag": item["Etag"],
                                "parent_dir_id": parent_file_id,
                                "size": item["Size"]
                            })
                
                if not data or len(data.get("InfoList", [])) < 100:
                    break
                page += 1
        except Exception as e:
            logger.error(f"è·å–åˆ—è¡¨å¤±è´¥ï¼ˆçˆ¶ID: {parent_file_id}ï¼‰: {str(e)}")
            raise

    try:
        recursive_fetch()
        logger.info(f"å…±å‘ç°{len(all_dirs)}ä¸ªç›®å½•å’Œ{len(all_files)}ä¸ªæ–‡ä»¶")
    except Exception as e:
        logger.error(f"è·å–èµ„æºç»“æ„å¤±è´¥: {str(e)}")
        return False

    # 1. ç›®å½•æ„å»ºé€»è¾‘
    dir_children = defaultdict(list)
    all_dir_ids = {d["dir_id"] for d in all_dirs}
    share_top_dirs = []
    for dir_info in all_dirs:
        parent_id = dir_info["parent_dir_id"]
        if parent_id not in all_dir_ids:
            share_top_dirs.append(dir_info)
        else:
            dir_children[parent_id].append(dir_info)
            
    dir_queue = deque(share_top_dirs)
    dir_id_mapping = {}
    for dir_info in share_top_dirs:
        dir_id_mapping[dir_info["dir_id"]] = None

    while dir_queue:
        dir_info = dir_queue.popleft()
        original_dir_id = dir_info["dir_id"]
        dir_name = dir_info["name"]
        original_parent_id = dir_info["parent_dir_id"]

        if original_dir_id in [d["dir_id"] for d in share_top_dirs]:
            new_parent_id = UPLOAD_TARGET_PID
        else:
            new_parent_id = dir_id_mapping.get(original_parent_id)

        if not new_parent_id:
            continue

        try:
            # ç›®å½•åˆ›å»ºé€šå¸¸ä¸å˜ï¼šfs_mkdir(name, parent_id)
            create_resp = client.fs_mkdir(
                name=dir_name,
                parent_id=new_parent_id,
                duplicate=1
            )
            check_response(create_resp)
            new_dir_id = create_resp["data"]["Info"]["FileId"]
            dir_id_mapping[original_dir_id] = new_dir_id
            
            child_dirs = dir_children.get(original_dir_id, [])
            dir_queue.extend(child_dirs)
        except Exception as e:
            logger.error(f"åˆ›å»ºç›®å½• {dir_name} å¤±è´¥: {str(e)}")
            return False

    # 2. æ–‡ä»¶è½¬å­˜é€»è¾‘
    MAX_BATCH_SIZE = 100
    file_batches = defaultdict(list)
    
    for file_info in all_files:
        file_id = file_info["file_id"]
        original_parent_id = file_info["parent_dir_id"]
        target_parent_id = dir_id_mapping.get(original_parent_id, UPLOAD_TARGET_PID)
        
        file_data = {
            "file_id": file_id,
            "file_name": file_info["name"],
            "etag": file_info["etag"],
            "parent_file_id": original_parent_id,
            "size": file_info["size"]
        }
        file_batches[target_parent_id].append(file_data)
    
    all_batches = []
    for target_parent_id, files_in_dir in file_batches.items():
        for i in range(0, len(files_in_dir), MAX_BATCH_SIZE):
            batch_files = files_in_dir[i:i + MAX_BATCH_SIZE]
            all_batches.append((target_parent_id, batch_files))
    
    for batch_index, (target_parent_id, batch_files) in enumerate(all_batches, 1):
        try:
            # --- ä¿®å¤ï¼šå›é€€åˆ° share_fs_copy ---
            copy_resp = client.share_fs_copy({
                "share_key": share_key,
                "share_pwd": share_pwd,
                "file_list": batch_files,
                "current_level": 1,
                "event": "transfer"
            }, parent_id=target_parent_id)
            
            check_response(copy_resp)
            logger.info(f"æ‰¹æ¬¡ {batch_index} è½¬å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_index} è½¬å­˜å¤±è´¥: {str(e)}")
            return False
            
    return True



class UserStateManager:
    def __init__(self, db_file):
        self.db_file = db_file
        self.init_db()

    def init_db(self):
        conn = sqlite3.connect(self.db_file)
        # åˆ›å»ºè¡¨ï¼Œå¦‚æœä¸å­˜åœ¨
        conn.execute('''CREATE TABLE IF NOT EXISTS user_states
                     (user_id INTEGER PRIMARY KEY, state TEXT, data TEXT)''')
        
        # [æ–°å¢] æ£€æŸ¥åˆ—æ•°ï¼Œå¦‚æœåˆ—æ•°ä¸å¯¹ï¼ˆæ¯”å¦‚æ—§ç‰ˆç•™ä¸‹çš„4åˆ—ï¼‰ï¼Œåˆ™é‡å»ºè¡¨
        try:
            cursor = conn.execute("PRAGMA table_info(user_states)")
            columns = cursor.fetchall()
            if len(columns) != 3:
                logger.warning(f"æ£€æµ‹åˆ° user_states è¡¨ç»“æ„ä¸åŒ¹é…ï¼ˆå½“å‰{len(columns)}åˆ—ï¼‰ï¼Œæ­£åœ¨é‡å»º...")
                conn.execute("DROP TABLE user_states")
                conn.execute('''CREATE TABLE user_states
                             (user_id INTEGER PRIMARY KEY, state TEXT, data TEXT)''')
        except Exception as e:
            logger.error(f"æ£€æŸ¥æ•°æ®åº“ç»“æ„å¤±è´¥: {e}")
            
        conn.commit()
        conn.close()

    def set_state(self, user_id, state, data=None):
        conn = sqlite3.connect(self.db_file)
        # [ä¿®æ”¹] æ˜¾å¼æŒ‡å®šåˆ—å (user_id, state, data)ï¼Œé˜²æ­¢å› æ•°æ®åº“åˆ—æ•°ä¸åŒ¹é…å¯¼è‡´çš„ "4 columns but 3 values" é”™è¯¯
        conn.execute("INSERT OR REPLACE INTO user_states (user_id, state, data) VALUES (?, ?, ?)",
                     (user_id, state, data))
        conn.commit()
        conn.close()

    def get_state(self, user_id):
        conn = sqlite3.connect(self.db_file)
        result = conn.execute("SELECT state, data FROM user_states WHERE user_id = ?",
                              (user_id,)).fetchone()
        conn.close()
        return result if result else (None, None)

    def clear_state(self, user_id):
        conn = sqlite3.connect(self.db_file)
        conn.execute("DELETE FROM user_states WHERE user_id = ?", (user_id,))
        conn.commit()
        conn.close()


# åˆå§‹åŒ–ç”¨æˆ·çŠ¶æ€ç®¡ç†å™¨
user_state_manager = UserStateManager(USER_STATE_DB)


# æœç´¢123ç½‘ç›˜æ–‡ä»¶å¤¹ï¼ˆä¿®æ”¹ç»“æœæ•°é‡ä¸º15ï¼‰
async def search_123_files(client: P123Client, keyword: str) -> list:
    """æœç´¢123ç½‘ç›˜ä¸­çš„æ–‡ä»¶å¤¹ï¼ˆè¿”å›æœ€å¤š15ä¸ªç»“æœï¼‰"""
    all_items = []
    last_file_id = 0
    try:
        for i in range(5):  # æœ€å¤š3é¡µ
            response = requests.get(
                f"https://open-api.123pan.com/api/v2/file/list?parentFileId=0&searchData={encodeURIComponent(keyword)}&searchMode=1&limit=100&lastFileId={last_file_id}",
                headers={
                    'Authorization': f'Bearer {client.token}',
                    'Platform': 'open_platform'
                },
                timeout=TIMEOUT
            )
            data = response.json()
            if data.get('code') == 401 or 'expired' in str(data.get('message', '')).lower():
                raise Exception("token expired")
            if data.get('code') != 0:
                raise Exception(f"æœç´¢å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            items = data.get('data', {}).get('fileList', [])
            # ä»…ç­›é€‰æ–‡ä»¶å¤¹ï¼ˆtype=1ï¼‰
            folder_items = [item for item in items if item.get('type') == 1]
            all_items.extend(folder_items)
            last_file_id = data.get('data', {}).get('lastFileId', -1)
            if last_file_id == -1:
                break

        # é™åˆ¶æœ€å¤šè¿”å›15ä¸ªç»“æœ
        results = []
        # æ‰¹é‡å¤„ç†15ä¸ªç»“æœï¼Œè·å–å®Œæ•´è·¯å¾„
        items_to_process = all_items[:20]  # é™åˆ¶ä¸º15ä¸ªç»“æœ
        logger.info(f"å‡†å¤‡æ‰¹é‡å¤„ç†{len(items_to_process)}ä¸ªæ–‡ä»¶å¤¹ç»“æœ")
        
        # ä½¿ç”¨æ‰¹é‡æ„å»ºè·¯å¾„å‡½æ•°
        # æ³¨æ„ï¼šå³ä½¿åªæœ‰15ä¸ªæ–‡ä»¶å¤¹é¡¹ç›®ï¼Œç”±äºéœ€è¦è·å–å„çº§çˆ¶ç›®å½•ä¿¡æ¯ï¼Œæ‰€ä»¥å®é™…æŸ¥è¯¢çš„IDæ•°é‡ä¼šå¤šäº15ä¸ª
        # è¿™ç§è®¾è®¡å¯ä»¥æ˜¾è‘—å‡å°‘APIè°ƒç”¨æ¬¡æ•°ï¼Œæé«˜è·¯å¾„æ„å»ºæ•ˆç‡

        paths_map = await batch_build_full_paths(client, items_to_process)
        
        # åˆ›å»ºæ˜ å°„ï¼Œä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾itemä¿¡æ¯
        item_map = {str(item.get('fileId', '')): item for item in items_to_process if str(item.get('fileId', ''))}
        
        # éå†paths_mapçš„é”®å€¼å¯¹ï¼Œä½¿resultsçš„é¡ºåºä¸paths_mapçš„é¡ºåºä¿æŒä¸€è‡´
        for file_id, full_path in paths_map.items():
            item = item_map.get(file_id)
            if not item:
                continue
            
            results.append({
                "id": file_id,
                "name": item.get('filename'),
                "type": "æ–‡ä»¶å¤¹",
                "path": full_path,  # å®Œæ•´è·¯å¾„
                "create_time": item.get('createTime')
            })
        
        # å¦‚æœè¿˜æœ‰æœªåœ¨paths_mapä¸­çš„é¡¹ç›®ï¼Œä¹Ÿæ·»åŠ åˆ°resultsä¸­
        for item in items_to_process:
            file_id = str(item.get('fileId', ''))
            if not file_id or file_id in paths_map:
                continue
            
            full_path = item.get('filename', '')
            results.append({
                "id": file_id,
                "name": item.get('filename'),
                "type": "æ–‡ä»¶å¤¹",
                "path": full_path,  # å®Œæ•´è·¯å¾„
                "create_time": item.get('createTime')
            })
        return results
    except Exception as e:
        logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        raise


def get_folder_detail(client: P123Client, file_id: str) -> dict:
    """è·å–æ–‡ä»¶å¤¹è¯¦æƒ…"""
    if not file_id:
        logger.error("æ–‡ä»¶å¤¹IDä¸ºç©º")
        return {"filename": ""}
    try:
        response = requests.get(
            f"https://open-api.123pan.com/api/v1/file/detail?fileID={file_id}",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform'
            },
            timeout=TIMEOUT
        )
        data = response.json()
        if data.get('code') != 0:
            logger.error(f"è·å–æ–‡ä»¶å¤¹{file_id}è¯¦æƒ…å¤±è´¥: {data.get('message')}")
            return {"filename": ""}
        return data.get('data', {})
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤¹{file_id}è¯¦æƒ…å¼‚å¸¸: {str(e)}")
        return {"filename": ""}


def get_files_details(client: P123Client, file_ids: list) -> dict:
    """æ‰¹é‡è·å–æ–‡ä»¶/æ–‡ä»¶å¤¹è¯¦æƒ…"""
    if not file_ids:
        logger.error("æ–‡ä»¶IDåˆ—è¡¨ä¸ºç©º")
        return {}
    try:
        logger.info(f"è¯·æ±‚ä»¥ä¸‹çˆ¶ç›®å½•IDè¯¦æƒ…ï¼š{file_ids}")
        response = requests.post(
            "https://open-api.123pan.com/api/v1/file/infos",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform',
                'Content-Type': 'application/json'
            },
            json={"fileIds": file_ids},
            timeout=TIMEOUT
        )
        data = response.json()
        #logger.info(f"ä»¥ä¸‹çˆ¶ç›®å½•è¯¦æƒ…ï¼š{data}")
        if data.get('code') != 0:
            logger.error(f"æ‰¹é‡è·å–æ–‡ä»¶è¯¦æƒ…å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
            return {}
        details_map = {}
        # æ³¨æ„ï¼šAPIè¿”å›çš„å­—æ®µåæ˜¯fileListï¼Œä¸æ˜¯list
        for item in data.get('data', {}).get('fileList', []):
            file_id = str(item.get('fileId'))
            details_map[file_id] = item
        return details_map
    except Exception as e:
        logger.error(f"æ‰¹é‡è·å–æ–‡ä»¶è¯¦æƒ…å¼‚å¸¸: {str(e)}")
        return {}


async def build_full_path(client: P123Client, item: dict) -> str:
    """æ„å»ºæ–‡ä»¶å¤¹å®Œæ•´è·¯å¾„ï¼ˆç”¨äºæ˜¾ç¤ºï¼‰ - å•ä¸ªå¤„ç†ç‰ˆæœ¬ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
    # ç”±äºå·²ç»å®ç°äº†æ‰¹é‡æ„å»ºè·¯å¾„çš„åŠŸèƒ½ï¼Œè¿™é‡Œå¯ä»¥ä¿ç•™ä¸ºå‘åå…¼å®¹æˆ–ç®€å•è°ƒç”¨
    paths_map = await batch_build_full_paths(client, [item])
    file_id = str(item.get('fileId', ''))
    return paths_map.get(file_id, item.get('filename', ''))


async def batch_build_full_paths(client: P123Client, items: list) -> dict:
    """æ‰¹é‡æ„å»ºå¤šä¸ªæ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„ï¼ˆä¿®å¤å…¨å±€ç¼“å­˜é—®é¢˜ï¼Œç¡®ä¿çˆ¶IDè¯¦æƒ…ä¸ä¸¢å¤±ï¼‰"""
    path_map = {}
    if not items:
        return path_map
    
    query_level = 4  # ä¿æŒå›ºå®š4å±‚
    temp_path_map = {}
    queried_ids = set()  # å·²æŸ¥è¯¢è¿‡çš„IDï¼ˆé¿å…é‡å¤è¯·æ±‚ï¼‰
    current_query_ids = set()  # å½“å‰è½®éœ€æŸ¥è¯¢çš„ID
    global_details_cache = {}  # æ–°å¢ï¼šå…¨å±€ç¼“å­˜ï¼Œä¿å­˜æ‰€æœ‰å·²æŸ¥è¯¢çš„çˆ¶ç›®å½•è¯¦æƒ…ï¼ˆè·¨è½®å¤ç”¨ï¼‰
    
    # åˆå§‹åŒ–ï¼šæ”¶é›†æ¯ä¸ªæ–‡ä»¶çš„åˆå§‹ä¿¡æ¯
    logger.info(f"å¼€å§‹å¤„ç†{len(items)}ä¸ªæ–‡ä»¶å¤¹é¡¹ç›®ï¼Œquery_level={query_level}")
    for item in items:
        file_id = str(item.get('fileId', ''))
        if not file_id:
            continue
        
        temp_path_map[file_id] = {
            'path_parts': [item.get('filename', '')],
            'current_parent_id': item.get('parentFileId'),
            'remaining_levels': query_level
        }
        
        parent_id = item.get('parentFileId')
        if parent_id and parent_id != 0:
            current_query_ids.add(str(parent_id))
    
    logger.info(f"ç¬¬ä¸€è½®æŸ¥è¯¢ï¼ˆç¬¬1å±‚çˆ¶ç›®å½•ï¼‰ï¼š{len(current_query_ids)}ä¸ªIDï¼Œå¤„ç†{len(temp_path_map)}ä¸ªæ–‡ä»¶")
    
    # è¿­ä»£æŸ¥è¯¢çˆ¶ç›®å½•ï¼ˆ4è½®ï¼‰
    for level in range(query_level):
        if not current_query_ids:
            logger.info(f"ç¬¬{level+1}è½®æ— çˆ¶IDå¯æŸ¥ï¼Œæå‰ç»“æŸ")
            break
        
        logger.info(f"ç¬¬{level+1}è½®æŸ¥è¯¢ï¼ˆå‰©ä½™å±‚çº§ï¼š{query_level - level}ï¼‰ï¼š{len(current_query_ids)}ä¸ªID")
        
        # 1. æ–°å¢ï¼šæŸ¥è¯¢å½“å‰è½®IDï¼Œåˆå¹¶åˆ°å…¨å±€ç¼“å­˜
        current_details = get_files_details(client, list(current_query_ids))
        global_details_cache.update(current_details)  # å…³é”®ï¼šå°†å½“å‰è½®è¯¦æƒ…å­˜å…¥å…¨å±€ç¼“å­˜
        
        next_query_ids = set()
        
        # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„çˆ¶ç›®å½•é“¾ï¼šä»å…¨å±€ç¼“å­˜è·å–è¯¦æƒ…ï¼Œè€Œéå½“å‰è½®ç¼“å­˜
        for file_id, info in temp_path_map.items():
            if info['remaining_levels'] <= 0:
                continue
            
            current_parent_id = info['current_parent_id']
            if not current_parent_id or current_parent_id == 0:
                continue
            
            current_parent_id_str = str(current_parent_id)
            # å…³é”®ï¼šä»å…¨å±€ç¼“å­˜è·å–è¯¦æƒ…ï¼Œè€Œéå½“å‰è½®ç¼“å­˜
            parent_detail = global_details_cache.get(current_parent_id_str)
            
            if not parent_detail:
                logger.warning(f"ç¬¬{level+1}è½®ï¼šå…¨å±€ç¼“å­˜ä¸­æœªæ‰¾åˆ°ID[{current_parent_id_str}]çš„è¯¦æƒ…ï¼Œåœæ­¢è¯¥æ–‡ä»¶çš„ä¸Šå±‚æŸ¥è¯¢")
                info['remaining_levels'] = 0
                continue
            
            # æå–çˆ¶ç›®å½•åç§°ï¼Œæ›´æ–°è·¯å¾„
            parent_name = parent_detail.get('filename', '')
            if parent_name:
                # æ–°å¢ï¼šé¿å…é‡å¤æ·»åŠ åŒä¸€ç›®å½•ï¼ˆé˜²æ­¢å¼‚å¸¸æƒ…å†µä¸‹çš„é‡å¤ï¼‰
                if not info['path_parts'] or info['path_parts'][0] != parent_name:
                    info['path_parts'].insert(0, parent_name)
                logger.debug(f"æ–‡ä»¶[{file_id}]ç¬¬{level+1}å±‚çˆ¶ç›®å½•ï¼š{parent_name}ï¼Œå½“å‰è·¯å¾„ï¼š{'/'.join(info['path_parts'])}")
            
            # è·å–ä¸‹ä¸€å±‚çˆ¶IDï¼ŒåŠ å…¥ä¸‹è½®æŸ¥è¯¢ï¼ˆéœ€æœªæŸ¥è¯¢è¿‡ï¼‰
            next_parent_id = parent_detail.get('parentFileId')
            if next_parent_id and next_parent_id != 0:
                next_parent_id_str = str(next_parent_id)
                if (next_parent_id_str not in queried_ids and 
                    next_parent_id_str not in current_query_ids and 
                    next_parent_id_str not in next_query_ids):
                    next_query_ids.add(next_parent_id_str)
                info['current_parent_id'] = next_parent_id
            else:
                info['remaining_levels'] = 0
            
            # å‰©ä½™å±‚çº§-1
            info['remaining_levels'] -= 1
        
        # æ›´æ–°å·²æŸ¥è¯¢IDå’Œä¸‹è½®æŸ¥è¯¢ID
        queried_ids.update(current_query_ids)
        current_query_ids = next_query_ids
    
    # 4è½®æŸ¥è¯¢å®Œæˆåï¼Œä»å…¨å±€ç¼“å­˜ä¸­ç»§ç»­æ„å»ºè·¯å¾„ï¼ˆä¸å‘èµ·æ–°è¯·æ±‚ï¼‰
    logger.info("4è½®æŸ¥è¯¢å·²å®Œæˆï¼Œå¼€å§‹ä»å…¨å±€ç¼“å­˜ä¸­ç»§ç»­æ„å»ºè·¯å¾„ï¼ˆä¸å‘èµ·æ–°è¯·æ±‚ï¼‰")
    has_more_to_process = True
    while has_more_to_process:
        has_more_to_process = False
        for file_id, info in temp_path_map.items():
            current_parent_id = info['current_parent_id']
            if not current_parent_id or current_parent_id == 0:
                continue
            
            current_parent_id_str = str(current_parent_id)
            # åªä»å…¨å±€ç¼“å­˜ä¸­è·å–è¯¦æƒ…ï¼Œä¸å‘èµ·æ–°è¯·æ±‚
            parent_detail = global_details_cache.get(current_parent_id_str)
            
            if parent_detail:
                # æå–çˆ¶ç›®å½•åç§°ï¼Œæ›´æ–°è·¯å¾„
                parent_name = parent_detail.get('filename', '')
                if parent_name:
                    if not info['path_parts'] or info['path_parts'][0] != parent_name:
                        info['path_parts'].insert(0, parent_name)
                    logger.debug(f"ä»ç¼“å­˜ä¸­è¡¥å……è·¯å¾„ï¼šæ–‡ä»¶[{file_id}]æ–°å¢çˆ¶ç›®å½•ï¼š{parent_name}ï¼Œå½“å‰è·¯å¾„ï¼š{'/'.join(info['path_parts'])}")
                
                # æ›´æ–°ä¸‹ä¸€å±‚çˆ¶ID
                next_parent_id = parent_detail.get('parentFileId')
                if next_parent_id and next_parent_id != 0:
                    info['current_parent_id'] = next_parent_id
                    has_more_to_process = True  # è¿˜æœ‰æ›´å¤šçˆ¶IDå¯ä»¥ä»ç¼“å­˜ä¸­æŸ¥æ‰¾
                else:
                    info['current_parent_id'] = 0
            else:
                info['current_parent_id'] = 0  # ç¼“å­˜ä¸­æ²¡æœ‰ï¼Œåœæ­¢æŸ¥æ‰¾
    
    # æ„å»ºæœ€ç»ˆè·¯å¾„ - æŒ‰è·¯å¾„å­—ç¬¦ä¸²æ’åºï¼Œä½¿ç›¸åŒå…¬å…±å‰ç¼€çš„æ–‡ä»¶å¤¹ä¼˜å…ˆæ”¾åœ¨ä¸€èµ·
    # é¦–å…ˆè·å–æ‰€æœ‰é¡¹ï¼Œç„¶åæŒ‰è·¯å¾„å­—ç¬¦ä¸²æ’åº
    sorted_items = sorted(temp_path_map.items(), key=lambda x: '/'.join(x[1]['path_parts']))

    for file_id, info in sorted_items:
        full_path = '/'.join(info['path_parts'])
        path_map[file_id] = full_path
        logger.debug(f"æ–‡ä»¶[{file_id}]æœ€ç»ˆè·¯å¾„ï¼š{full_path}")
    logger.info(f"æ‰¹é‡è·¯å¾„æ„å»ºå®Œæˆï¼Œç”Ÿæˆ{len(path_map)}ä¸ªæ–‡ä»¶è·¯å¾„ï¼ˆquery_level=4ï¼Œç¼“å­˜è¡¥å……å®Œæˆï¼‰")
    return path_map


def encodeURIComponent(s: str) -> str:
    import urllib.parse
    return urllib.parse.quote(s, safe='~()*!.\'')


def create_share_link(client: P123Client, file_id: str, expiry_days: int = 0, password: str = None) -> dict:
    """åˆ›å»ºåˆ†äº«é“¾æ¥"""
    if not file_id or not str(file_id).strip():
        raise ValueError("æ–‡ä»¶å¤¹IDä¸ºç©ºæˆ–æ— æ•ˆ")

    valid_expire_days = {0, 1, 7, 30}
    if expiry_days not in valid_expire_days:
        logger.warning(f"è¿‡æœŸå¤©æ•°{expiry_days}æ— æ•ˆï¼Œè‡ªåŠ¨ä½¿ç”¨7å¤©")
        expiry_days = 7

    try:
        folder_detail = get_folder_detail(client, file_id)
        folder_name = folder_detail.get('filename', f"åˆ†äº«æ–‡ä»¶å¤¹_{file_id}")
        if not folder_name:
            logger.warning(f"æ–‡ä»¶å¤¹ID{file_id}ä¸å­˜åœ¨ï¼Œå¯èƒ½å·²è¢«åˆ é™¤")

        response = requests.post(
            "https://open-api.123pan.com/api/v1/share/create",
            headers={
                'Authorization': f'Bearer {client.token}',
                'Platform': 'open_platform',
                'Content-Type': 'application/json'
            },
            json={
                "shareName": folder_name,
                "shareExpire": expiry_days,
                "fileIDList": file_id,
                "sharePwd": DIY_LINK_PWD
            },
            timeout=TIMEOUT
        )
        data = response.json()
        if data.get('code') != 0:
            raise Exception(f"åˆ›å»ºåˆ†äº«å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}ï¼ˆID: {file_id}ï¼‰")
        share_info = data.get('data', {})
        if expiry_days == 0:
            expiry_str = "æ°¸ä¹…æœ‰æ•ˆ"
        else:
            expiry_time = int(time.time()) + expiry_days * 86400
            expiry_str = datetime.fromtimestamp(expiry_time).strftime('%Y-%m-%d %H:%M:%S')
        return {
            "url": f"https://www.123pan.com/s/{share_info.get('shareKey')}{'?pwd=' + DIY_LINK_PWD if DIY_LINK_PWD else ''}",
            "password": share_info.get('sharePwd'),
            "expiry": expiry_str
        }
    except Exception as e:
        logger.error(f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
        raise


def get_first_video_file(client: P123Client, file_id: str) -> str:
    """è·å–æ–‡ä»¶å¤¹æˆ–å­æ–‡ä»¶å¤¹ä¸­ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶çš„åç§°"""
    video_extensions = {'.mkv', '.ts', '.mp4', '.avi', '.rmvb', '.wmv', '.m2ts', '.mpg', '.flv', '.rm', '.mov', '.iso'}

    def recursive_search(folder_id: str) -> str:
        try:
            # è°ƒç”¨123ç½‘ç›˜APIåˆ—å‡ºæ–‡ä»¶å¤¹å†…å®¹
            resp = client.fs_list(folder_id)
            check_response(resp)
            items = resp["data"]["InfoList"]

            # ä¼˜å…ˆæ£€æŸ¥å½“å‰æ–‡ä»¶å¤¹çš„æ–‡ä»¶
            for item in items:
                if item["Type"] == 0:  # ç±»å‹ä¸ºæ–‡ä»¶
                    filename = item["FileName"]
                    ext = os.path.splitext(filename)[1].lower()
                    if ext in video_extensions:
                        return filename

            # é€’å½’æ£€æŸ¥å­æ–‡ä»¶å¤¹
            for item in items:
                if item["Type"] == 1:  # ç±»å‹ä¸ºæ–‡ä»¶å¤¹
                    sub_result = recursive_search(item["FileId"])
                    if sub_result:
                        return sub_result
            return None
        except Exception as e:
            logger.error(f"æœç´¢è§†é¢‘æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None

    return recursive_search(file_id)

@bot.message_handler(commands=['info'])
def handle_info(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    client = init_123_client()
    response = client.user_info()  # éªŒè¯tokenæœ‰æ•ˆæ€§
    def mask_uid(uid):
        """è´¦æˆ·IDè„±æ•ï¼š1846764956 â†’ 184****956"""
        uid_str = str(uid)
        return f"{uid_str[:3]}****{uid_str[-3:]}" if len(uid_str)>=6 else uid_str

    def mask_mobile(mobile):
        """æ‰‹æœºå·è„±æ•ï¼š18221643386 â†’ 182****3386"""
        mobile_str = str(mobile)
        return f"{mobile_str[:3]}****{mobile_str[-4:]}" if len(mobile_str)==11 else mobile_str

    def format_size(size):
        """å­—èŠ‚è½¬TB/GBï¼ˆè‡ªåŠ¨é€‚é…å•ä½ï¼‰"""
        if size <= 0:
            return "0.00 GB"
        tb = size / (1024 **4)
        return f"{tb:.2f} TB" if tb >= 1 else f"{size / (1024** 3):.2f} GB"

    def space_progress(used, total, bar_len=10):
        """ç”Ÿæˆè¿›åº¦æ¡ï¼šâ–“=å·²ç”¨ï¼Œâ–‘=å‰©ä½™"""
        if total == 0:
            return "â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡â–¡ (0%)"
        ratio = used / total
        filled = int(ratio * bar_len)
        bar = "â–“" * filled + "â–‘" * (bar_len - filled)
        percent = f"{ratio*100:.1f}%"
        return f"{bar} ({percent})"

    # å‡è®¾å“åº”æ•°æ®ä¸º `response`
    data = response["data"]

    # 1. æ ‡é¢˜ä¸è´¦æˆ·ä¿¡æ¯
    base_title = "ğŸš€ 123äº‘ç›˜ä¿¡æ¯"

    account_info = f"""ğŸ‘¤ è´¦æˆ·ä¿¡æ¯
    â”œâ”€ æ˜µç§°ï¼š{data['Nickname']} {'ğŸ–ï¸VIP' if data['Vip'] else ''}
    â”œâ”€ è´¦æˆ·IDï¼š{mask_uid(data['UID'])}
    â”œâ”€ æ‰‹æœºå·ï¼š{mask_mobile(data['Passport'])}
    â””â”€ å¾®ä¿¡ç»‘å®šï¼š{"âœ…å·²ç»‘" if data['BindWechat'] else "âŒæœªç»‘"}"""

    # 2. å­˜å‚¨ç©ºé—´ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
    used = data['SpaceUsed']
    total = data['SpacePermanent']
    storage_progress = space_progress(used, total)

    storage_info = f"""ğŸ’¾ å­˜å‚¨ç©ºé—´ {storage_progress}
    â”œâ”€ å·²ç”¨ï¼š{format_size(used)}
    â”œâ”€ æ°¸ä¹…ï¼š{format_size(total)}
    â””â”€ æ–‡ä»¶æ€»æ•°ï¼š{data['FileCount']:,} ä¸ª"""

    # 3. VIPè¯¦æƒ…ï¼ˆæ‹†åˆ†å¤šä¸ªæƒç›Šï¼‰
    vip_details = []
    # æ·»åŠ åŸºç¡€VIPä¿¡æ¯
    #vip_details.append(f"â”œâ”€ ç­‰çº§ï¼š{data['VipLevel']} | ç±»å‹ï¼š{data['VipExplain']}")
    #vip_details.append(f"â”œâ”€ åˆ°æœŸæ—¶é—´ï¼š{data['VipExpire']}")
    #vip_details.append(f"â””â”€ æƒç›Šåˆ—è¡¨ï¼š")

    # é€ä¸ªæ·»åŠ VIPæƒç›Šï¼ˆå•ç‹¬æˆé¡¹ï¼‰
    for i, vip in enumerate(data['VipInfo'], 1):
        # æœ€åä¸€ä¸ªæƒç›Šç”¨ç‰¹æ®Šç¬¦å·
        symbol = "    â””â”€" if i == len(data['VipInfo']) else "    â”œâ”€"
        vip_details.append(f"{symbol} {vip['vip_label']}ï¼š{vip['start_time']} â†’ {vip['end_time']}")

    vip_info = "ğŸ’ VIPä¼šå‘˜\n" + "\n".join(vip_details)

    # 4. æµé‡ä¸åŠŸèƒ½çŠ¶æ€
    traffic_info = f"""ğŸš€ æµé‡ä¸åŠŸèƒ½
    â”œâ”€ ç›´è¿æµé‡ï¼š{format_size(data['DirectTraffic'])}
    â”œâ”€ åˆ†äº«æµé‡ï¼š{format_size(data['ShareTraffic'])}
    â””â”€ ç›´é“¾åŠŸèƒ½ï¼š{"âœ…å¼€å¯" if data['StraightLink'] else "âŒå…³é—­"}"""

    # 5. å¤‡ä»½ä¿¡æ¯
    backup_info = f"""ğŸ“¦ å¤‡ä»½é…ç½®
    â”œâ”€ ç§»åŠ¨ç«¯ï¼š{data['BackupFileInfo']['MobileTerminalBackupFileName']}
    â””â”€ æ¡Œé¢ç«¯ï¼š{data['BackupFileInfo']['DesktopTerminalBackupFileName']}"""

    # æ‹¼æ¥æœ€ç»ˆæ¶ˆæ¯
    tg_message = "\n\n".join([
        base_title,
        account_info,
        storage_info,
        vip_info,
        traffic_info,
        backup_info
    ])
    # æœ€åä¸€æ¬¡æ€§æ‰“å°å®Œæ•´æ¶ˆæ¯
    reply_thread_pool.submit(send_reply, message, tg_message)

@bot.callback_query_handler(func=lambda call: True)
def callback_handler(call):
    user_id = call.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        bot.answer_callback_query(call.id, "æ— æƒæ“ä½œ", show_alert=True)
        return

    if call.data == "show_usage":
        bot.answer_callback_query(call.id)
        bot.send_message(call.message.chat.id, USE_METHOD)
    elif call.data == "show_disclaimer":
        bot.answer_callback_query(call.id)
        bot.send_message(call.message.chat.id, DISCLAIMER_TEXT)
    elif call.data == "show_userbot_help":
        bot.answer_callback_query(call.id)
        bot.send_message(call.message.chat.id, USERBOT_HELP)   

# Telegramæœºå™¨äººæ¶ˆæ¯å¤„ç†
@bot.message_handler(commands=['start'])
def handle_start(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return

    # æ„é€ æŒ‰é’®é”®ç›˜
    markup = InlineKeyboardMarkup()
    # ç¬¬ä¸€è¡Œï¼šä½¿ç”¨è¯´æ˜ | å…è´£å£°æ˜
    markup.row(InlineKeyboardButton("ğŸ“– ä½¿ç”¨è¯´æ˜", callback_data="show_usage"),
               InlineKeyboardButton("âš ï¸ å…è´£å£°æ˜", callback_data="show_disclaimer"))
    # ç¬¬äºŒè¡Œï¼šäººå½¢å‘½ä»¤ | é¡¹ç›®åœ°å€
    markup.row(InlineKeyboardButton("ğŸ¤– äººå½¢å‘½ä»¤", callback_data="show_userbot_help"),
               InlineKeyboardButton("ğŸŒŸ é¡¹ç›®åœ°å€", url="https://github.com/dydydd/123bot"))
    
    # å‘é€ç®€æ´çš„å¯åŠ¨æ¶ˆæ¯
    bot.send_message(
        message.chat.id, 
        f"å®å’šï¼Œæˆ‘å·²æˆåŠŸå¯åŠ¨ï¼Œæ¬¢è¿ä½¿ç”¨123botï¼\n\n â•â•â•â•â•å½“å‰ç‰ˆæœ¬â€{version}â•â•â•â•â•\n\n", 
        parse_mode='HTML', 
        reply_markup=markup
    )

def save_env_filter(new_filter_value):
    """æŒä¹…åŒ–ä¿å­˜è¿‡æ»¤è¯åˆ°db/user.envæ–‡ä»¶"""
    env_file_path = os.path.join('db', 'user.env')
    
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(env_file_path):
        logger.warning(f"{env_file_path} æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(env_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # æŸ¥æ‰¾å¹¶æ›¿æ¢ENV_FILTERè¡Œ
        updated_lines = []
        found = False
        for line in lines:
            if line.startswith('ENV_FILTER='):
                updated_lines.append(f'ENV_FILTER={new_filter_value}\n')
                found = True
            else:
                updated_lines.append(line)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ENV_FILTERè¡Œï¼Œåˆ™æ·»åŠ 
        if not found:
            # æ‰¾åˆ°é¢‘é“ç›‘æ§é…ç½®éƒ¨åˆ†ï¼Œåœ¨åˆé€‚çš„ä½ç½®æ·»åŠ 
            insert_index = -1
            for i, line in enumerate(lines):
                if '# æ£€æŸ¥æ–°æ¶ˆæ¯çš„æ—¶é—´é—´éš”ï¼ˆåˆ†é’Ÿï¼‰' in line:
                    insert_index = i + 2
                    break
            if insert_index != -1:
                updated_lines.insert(insert_index, f'ENV_FILTER={new_filter_value}\n')
            else:
                # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚ä½ç½®ï¼Œå°±æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾
                updated_lines.append(f'\nENV_FILTER={new_filter_value}\n')
        
        # å†™å›æ–‡ä»¶
        with open(env_file_path, 'w', encoding='utf-8') as f:
            f.writelines(updated_lines)
        
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜ç¯å¢ƒå˜é‡å¤±è´¥ï¼š{str(e)}")
        return False

@bot.message_handler(commands=['add'])
def add_filter(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    global FILTER, filter_pattern
    try:
        # å±•ç¤ºå½“å‰è¿‡æ»¤è¯å’Œç”¨æ³•
        current_filters_text = FILTER if FILTER else "æ— ï¼ˆæœªè®¾ç½®ä»»ä½•è¿‡æ»¤è¯ï¼‰"
        usage_text = "â„¹ï¸ ç”¨æ³•ï¼š\n- æ·»åŠ è¿‡æ»¤è¯ï¼š/add å…³é”®è¯\nï¼ˆä¾‹ï¼š/add WALK   /add WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n- åˆ é™¤è¿‡æ»¤è¯ï¼š/remove å…³é”®è¯\nï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆ   /remove WALK|æƒåŠ›çš„æ¸¸æˆï¼‰"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°
        if len(message.text.split()) < 2:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦æ·»åŠ çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/add WALKï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå¤±è´¥ï¼šæ— è¾“å…¥å‚æ•°")
            return
        
        # è·å–ç”¨æˆ·è¾“å…¥çš„è¿‡æ»¤è¯å¹¶æ¸…ç†
        new_filters_text = message.text.split(maxsplit=1)[1].strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
        if not new_filters_text:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦æ·»åŠ çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/add WALK æˆ– /add WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå¤±è´¥ï¼šå‚æ•°ä¸ºç©º")
            return
        
        # æ‹†åˆ†ç”¨æˆ·è¾“å…¥çš„å¤šä¸ªè¿‡æ»¤è¯
        new_filters_list = [f.strip() for f in new_filters_text.split("|") if f.strip()]
        
        # æ‹†åˆ†ç°æœ‰è¿‡æ»¤è¯
        current_filters = FILTER.split("|") if FILTER else []
        
        # è®°å½•æ·»åŠ ç»“æœ
        added_filters = []
        existing_filters = []
        
        # æ£€æŸ¥æ¯ä¸ªè¿‡æ»¤è¯æ˜¯å¦å·²å­˜åœ¨å¹¶æ·»åŠ 
        for new_filter in new_filters_list:
            if new_filter not in current_filters:
                added_filters.append(new_filter)
                current_filters.append(new_filter)
            else:
                existing_filters.append(new_filter)
        
        # å¦‚æœæ²¡æœ‰æ·»åŠ ä»»ä½•æ–°è¿‡æ»¤è¯
        if not added_filters:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ æ‰€æœ‰è¿‡æ»¤è¯ã€Œ{', '.join(existing_filters)}ã€å·²å­˜åœ¨ï¼Œæ— éœ€é‡å¤æ·»åŠ \n\n{usage_text}")
            return
        
        # æ„å»ºæ–°çš„è¿‡æ»¤è¯å­—ç¬¦ä¸²
        FILTER = "|".join(current_filters)
        
        # æŒä¹…åŒ–ä¿å­˜åˆ°æ–‡ä»¶
        if not save_env_filter(FILTER):
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ è¿‡æ»¤è¯æ·»åŠ æˆåŠŸï¼Œä½†ä¿å­˜åˆ°æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åœ¨é…ç½®é¡µé¢æ›´æ–°\n\n{usage_text}")
        
        # é‡å»ºæ­£åˆ™å¯¹è±¡
        filter_pattern = re.compile(FILTER, re.IGNORECASE)
        
        # æ„å»ºåé¦ˆæ¶ˆæ¯
        feedback_msg = f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\n"
        
        if added_filters:
            feedback_msg += f"âœ… å·²æ·»åŠ è¿‡æ»¤è¯ï¼šã€Œ{', '.join(added_filters)}ã€\n"
        
        if existing_filters:
            feedback_msg += f"âš ï¸ å·²å­˜åœ¨çš„è¿‡æ»¤è¯ï¼šã€Œ{', '.join(existing_filters)}ã€\n"
        
        feedback_msg += f"ğŸ“Œ æ›´æ–°åè¿‡æ»¤è¯ï¼š{FILTER}\n\n{usage_text}"
        
        # å‘é€æˆåŠŸåé¦ˆ
        reply_thread_pool.submit(send_reply, message, feedback_msg)
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addï¼Œæ·»åŠ è¿‡æ»¤è¯ï¼š{', '.join(added_filters)}ï¼Œå·²å­˜åœ¨ï¼š{', '.join(existing_filters)}ï¼Œæ›´æ–°åï¼š{FILTER}")
        
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥ï¼š{str(e)}")
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/addå‡ºé”™ï¼š{str(e)}")

@bot.message_handler(commands=['remove'])
def remove_filter(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return

    global FILTER, filter_pattern
    try:
        # å±•ç¤ºå½“å‰è¿‡æ»¤è¯å’Œç”¨æ³•
        current_filters_text = FILTER if FILTER else "æ— ï¼ˆæœªè®¾ç½®ä»»ä½•è¿‡æ»¤è¯ï¼‰"
        usage_text = "â„¹ï¸ ç”¨æ³•ï¼š\n- æ·»åŠ è¿‡æ»¤è¯ï¼š/add å…³é”®è¯ï¼ˆä¾‹ï¼š/add WALKï¼‰\n- åˆ é™¤è¿‡æ»¤è¯ï¼š/remove å…³é”®è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆï¼‰"
        
        # æ£€æŸ¥å½“å‰æ˜¯å¦æœ‰è¿‡æ»¤è¯
        if not FILTER:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ å½“å‰æ— ä»»ä½•è¿‡æ»¤è¯ï¼Œæ— éœ€åˆ é™¤\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šå½“å‰æ— è¿‡æ»¤è¯")
            return
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å‚æ•°
        if len(message.text.split()) < 2:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦åˆ é™¤çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šæ— è¾“å…¥å‚æ•°")
            return
        
        # è·å–ç”¨æˆ·è¾“å…¥çš„è¿‡æ»¤è¯å¹¶æ¸…ç†
        del_filters_text = message.text.split(maxsplit=1)[1].strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºå­—ç¬¦ä¸²
        if not del_filters_text:
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâŒ è¯·è¾“å…¥è¦åˆ é™¤çš„è¿‡æ»¤è¯ï¼ˆä¾‹ï¼š/remove æƒåŠ›çš„æ¸¸æˆ æˆ– /remove WALK|æƒåŠ›çš„æ¸¸æˆï¼‰\n\n{usage_text}")
            logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå¤±è´¥ï¼šå‚æ•°ä¸ºç©º")
            return
        
        # æ‹†åˆ†ç”¨æˆ·è¾“å…¥çš„å¤šä¸ªè¿‡æ»¤è¯
        del_filters = [f.strip() for f in del_filters_text.split("|") if f.strip()]
        
        # æ‹†åˆ†ç°æœ‰è¿‡æ»¤è¯
        current_filters = FILTER.split("|") if FILTER else []
        
        # è®°å½•åˆ é™¤ç»“æœ
        deleted_filters = []
        not_found_filters = []
        
        # æ£€æŸ¥æ¯ä¸ªè¿‡æ»¤è¯æ˜¯å¦å­˜åœ¨å¹¶åˆ é™¤
        for del_filter in del_filters:
            if del_filter in current_filters:
                deleted_filters.append(del_filter)
            else:
                not_found_filters.append(del_filter)
        
        # åˆ é™¤å­˜åœ¨çš„è¿‡æ»¤è¯
        new_filters = [f for f in current_filters if f not in deleted_filters]
        FILTER = "|".join(new_filters) if new_filters else ""
        
        # æŒä¹…åŒ–ä¿å­˜åˆ°æ–‡ä»¶
        if not save_env_filter(FILTER):
            reply_thread_pool.submit(send_reply, message, f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\nâš ï¸ è¿‡æ»¤è¯åˆ é™¤æˆåŠŸï¼Œä½†ä¿å­˜åˆ°æ–‡ä»¶å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨åœ¨é…ç½®é¡µé¢æ›´æ–°\n\n{usage_text}")

        # é‡å»ºæ­£åˆ™å¯¹è±¡
        filter_pattern = re.compile(FILTER, re.IGNORECASE)
        
        # æ„å»ºåé¦ˆæ¶ˆæ¯
        updated_filters_text = FILTER if FILTER else "æ— "
        feedback_msg = f"ğŸ“Œ å½“å‰è¿‡æ»¤è¯ï¼š{current_filters_text} ï¼ˆå¤šä¸ªç”¨|åˆ†éš”ï¼Œå‘½ä¸­çš„å†…å®¹ä¼šè¢«è½¬å­˜ï¼Œä¸ºç©ºåˆ™ä¼šè½¬å­˜æ‰€æœ‰èµ„æºï¼‰\n"
        
        if deleted_filters:
            feedback_msg += f"âœ… å·²åˆ é™¤è¿‡æ»¤è¯ï¼šã€Œ{', '.join(deleted_filters)}ã€\n"
        
        if not_found_filters:
            feedback_msg += f"âš ï¸ æœªæ‰¾åˆ°çš„è¿‡æ»¤è¯ï¼šã€Œ{', '.join(not_found_filters)}ã€\n"
        
        feedback_msg += f"ğŸ“Œ æ›´æ–°åè¿‡æ»¤è¯ï¼š{updated_filters_text}\n\n{usage_text}"
        
        # å‘é€æˆåŠŸåé¦ˆ
        reply_thread_pool.submit(send_reply, message, feedback_msg)
        logger.info(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeï¼Œåˆ é™¤è¿‡æ»¤è¯ï¼š{', '.join(deleted_filters)}ï¼Œæœªæ‰¾åˆ°ï¼š{', '.join(not_found_filters)}ï¼Œæ›´æ–°åï¼š{FILTER}")
        
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥ï¼š{str(e)}")
        logger.error(f"ç”¨æˆ· {message.from_user.id} æ‰§è¡Œ/removeå‡ºé”™ï¼š{str(e)}")

@bot.message_handler(commands=['share', 's123'])
def handle_share_command(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤å‘½ä»¤ã€‚")
        return
    try:
        command_used = message.text.split()[0].replace('/', '')
        command_parts = message.text.split(' ', 1)
        
        if len(command_parts) < 2 or not command_parts[1].strip():
            reply_thread_pool.submit(send_reply, message, f"è¯·æä¾›æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š/{command_used} æƒåŠ›çš„æ¸¸æˆ")
            return
        
        keyword = command_parts[1].strip()
        
        # æ ¹æ®å‘½ä»¤ç¡®å®šæ¨¡å¼
        mode = "json" if "s123" in command_used else "link"
        mode_text = "JSONç”Ÿæˆ" if mode == "json" else "åˆ†äº«é“¾æ¥"
        
        reply_thread_pool.submit(send_reply, message, f"æ­£åœ¨æœç´¢åŒ…å« '{keyword}' çš„æ–‡ä»¶å¤¹ ({mode_text}æ¨¡å¼)...")
        client = init_123_client()
        import threading
        # ä¼ å…¥ mode
        threading.Thread(target=perform_search, args=(client, keyword, user_id, message.chat.id, mode)).start()
    except Exception as e:
        reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥: {str(e)}")
        logger.error(f"å¤„ç†å‘½ä»¤å¤±è´¥: {str(e)}")

def build_folder_message(results):
    """
    æ ¸å¿ƒè§„åˆ™ï¼š
    1. ç¼–å·é¡ºåºï¼š1-20ä¸¥æ ¼å¯¹åº”è¾“å…¥é¡ºåºï¼Œä¸æ‰“ä¹±ã€ä¸é‡æ’
    2. å¤§ç»„åˆ’åˆ†ï¼šæŒ‰â€œåŸå§‹ç¼–å·è¿ç»­+å‰ä¸¤å±‚ç›®å½•ç›¸åŒâ€åˆ’å¤§ç»„ï¼ˆéè¿ç»­/å‰ç¼€ä¸åŒåˆ™å•ç‹¬æˆç»„ï¼‰
    3. ç»„å†…åˆå¹¶ï¼šæ¯ä¸ªå¤§ç»„å†…è®¡ç®—æ‰€æœ‰è·¯å¾„çš„å…¬å…±å‰ç¼€ï¼ˆå«å‰ä¸¤å±‚å¤–çš„æ·±å±‚å‰ç¼€ï¼‰ï¼Œåˆå¹¶ä¸ºçˆ¶ç›®å½•
    4. å•ç‹¬ç»„å¤„ç†ï¼šç»„å†…ä»…1æ¡è·¯å¾„æ—¶ï¼Œè‡ªåŠ¨ä½œä¸ºå•ç‹¬ç»„ï¼Œä¸å¼ºåˆ¶åˆå¹¶å…¬å…±å‰ç¼€
    """
    # æ­¥éª¤1ï¼šé¢„å¤„ç†è·¯å¾„ï¼Œæå–å…³é”®ä¿¡æ¯ï¼ˆä¿ç•™åŸå§‹ç¼–å·ï¼‰
    path_info_list = []
    for orig_seq, item in enumerate(results, start=1):  # åŸå§‹ç¼–å·1-20
        raw_path = item.get("path", "").strip("/")
        dir_list = [p.strip() for p in raw_path.split("/") if p.strip()]  # æ‹†åˆ†ç›®å½•åˆ—è¡¨
        dir_len = len(dir_list)
        
        # æå–å‰ä¸¤å±‚ç›®å½•ä½œä¸ºåˆ†ç»„keyï¼ˆä¸è¶³ä¸¤å±‚åˆ™å–å®é™…å±‚æ•°ï¼Œå¦‚1å±‚ï¼‰
        if dir_len >= 2:
            group_key = tuple(dir_list[:2])  # å‰ä¸¤å±‚ç›®å½•ä½œä¸ºkeyï¼ˆå¦‚("Resource","å¤§åŒ…èµ„æº")ï¼‰
        else:
            group_key = tuple(dir_list)  # ä¸è¶³ä¸¤å±‚ï¼Œç”¨å…¨éƒ¨ç›®å½•ä½œä¸ºkeyï¼ˆå¦‚("Video",)ï¼‰
        
        path_info_list.append({
            "orig_seq": orig_seq,
            "raw_path": raw_path,
            "dir_list": dir_list,
            "dir_len": dir_len,
            "group_key": group_key,
            "is_root": dir_len == 1  # æ ¹ç›®å½•åˆ¤æ–­ï¼šä»…1å±‚ç›®å½•
        })
    if not path_info_list:
        return "æœªæ‰¾åˆ°åŒ¹é…æ–‡ä»¶å¤¹"

    # å·¥å…·å‡½æ•°1ï¼šè®¡ç®—ä¸€ç»„è·¯å¾„çš„å…¬å…±å‰ç¼€é•¿åº¦ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼ï¼‰
    def get_group_common_prefix(group_paths):
        if len(group_paths) == 1:
            # å•ç‹¬ç»„ï¼šå…¬å…±å‰ç¼€å–åˆ°â€œå€’æ•°ç¬¬äºŒå±‚â€ï¼Œç¡®ä¿å­è·¯å¾„æ˜¾ç¤ºæœ€å1å±‚
            single_path = group_paths[0]
            return max(0, single_path["dir_len"] - 1)
        # å¤šè·¯å¾„ç»„ï¼šå…³é”®ä¿®æ­£â€”â€”å…¬å…±å‰ç¼€é•¿åº¦ â‰¤ æœ€çŸ­è·¯å¾„çš„dir_len - 1
        min_dir_len = min(p["dir_len"] for p in group_paths)
        max_allowed_len = min_dir_len - 1  # ç¦æ­¢å…¬å…±å‰ç¼€åŒ…å«æœ€çŸ­è·¯å¾„çš„æœ€åä¸€å±‚
        base_dir = group_paths[0]["dir_list"]
        common_len = max_allowed_len  # åˆå§‹åŒ–ä¸ºæœ€å¤§å…è®¸é•¿åº¦
        # æ¯”è¾ƒæ‰€æœ‰è·¯å¾„ï¼Œæ‰¾åˆ°æœ€é•¿å…¬å…±å‰ç¼€ï¼ˆä¸è¶…è¿‡max_allowed_lenï¼‰
        for p in group_paths[1:]:
            curr_dir = p["dir_list"]
            curr_common = 0
            while curr_common < common_len and curr_dir[curr_common] == base_dir[curr_common]:
                curr_common += 1
            if curr_common < common_len:
                common_len = curr_common
            if common_len == 0:
                break
        return common_len

    # å·¥å…·å‡½æ•°2ï¼šç”Ÿæˆçˆ¶ç›®å½•å­—ç¬¦ä¸²å’Œå­è·¯å¾„å­—ç¬¦ä¸²
    def get_parent_subpath(path, common_len):
        dir_list = path["dir_list"]
        # çˆ¶ç›®å½•ï¼šå…¬å…±å‰ç¼€éƒ¨åˆ†
        parent_dir = dir_list[:common_len] if common_len > 0 else []
        parent_str = " / ".join(parent_dir) if parent_dir else ("æ ¹ç›®å½•" if path["is_root"] else "")
        # å­è·¯å¾„ï¼šå…¬å…±å‰ç¼€ä¹‹åçš„éƒ¨åˆ†ï¼ˆè‹¥ä¸ºç©ºï¼Œæ˜¾ç¤ºæœ€å1å±‚ç›®å½•ï¼‰
        sub_dir = dir_list[common_len:] if common_len < path["dir_len"] else [dir_list[-1]]
        sub_path_str = " / ".join(sub_dir)
        return parent_str, sub_path_str

    # æ­¥éª¤2ï¼šæŒ‰â€œç¼–å·è¿ç»­+group_keyç›¸åŒâ€åˆ’å¤§ç»„ï¼ˆæ ¸å¿ƒåˆ†ç»„é€»è¾‘ï¼‰
    groups = []
    if path_info_list:
        current_group = [path_info_list[0]]  # åˆå§‹åŒ–å½“å‰ç»„ï¼ˆç¬¬ä¸€ä¸ªè·¯å¾„ï¼‰
        for path in path_info_list[1:]:
            prev_path = current_group[-1]
            # åˆ¤æ–­ï¼šå½“å‰è·¯å¾„ä¸å‰ä¸€ä¸ªè·¯å¾„â€œç¼–å·è¿ç»­ï¼ˆå¿…ç„¶æ»¡è¶³ï¼ŒæŒ‰é¡ºåºéå†ï¼‰ä¸”group_keyç›¸åŒâ€
            if path["group_key"] == prev_path["group_key"]:
                current_group.append(path)
            else:
                # ä¸åŒgroup_keyï¼Œä¿å­˜å½“å‰ç»„ï¼Œæ–°å»ºç»„
                groups.append(current_group)
                current_group = [path]
        groups.append(current_group)  # åŠ å…¥æœ€åä¸€ä¸ªç»„

    # æ­¥éª¤3ï¼šå¤„ç†æ¯ä¸ªå¤§ç»„ï¼Œåˆå¹¶ç»„å†…å…¬å…±å‰ç¼€
    processed_groups = []
    for group in groups:
        common_len = get_group_common_prefix(group)  # ç»„å†…å…¬å…±å‰ç¼€é•¿åº¦
        group_parent = ""  # ç»„çš„ç»Ÿä¸€çˆ¶ç›®å½•ï¼ˆå–ç¬¬ä¸€æ¡è·¯å¾„çš„çˆ¶ç›®å½•ï¼Œç»„å†…æ‰€æœ‰è·¯å¾„çˆ¶ç›®å½•ç›¸åŒï¼‰
        group_paths = []
        
        for path in group:
            parent_str, sub_path_str = get_parent_subpath(path, common_len)
            # ç»Ÿä¸€ç»„çš„çˆ¶ç›®å½•ï¼ˆç»„å†…æ‰€æœ‰è·¯å¾„çˆ¶ç›®å½•ä¸€è‡´ï¼Œå–ç¬¬ä¸€æ¡çš„å³å¯ï¼‰
            if not group_parent:
                group_parent = parent_str
            # æ”¶é›†ç»„å†…è·¯å¾„ï¼ˆå«åŸå§‹ç¼–å·å’Œå­è·¯å¾„ï¼‰
            group_paths.append({
                "orig_seq": path["orig_seq"],
                "sub_path": sub_path_str
            })
        
        processed_groups.append({
            "parent_str": group_parent,
            "paths": group_paths  # ç»„å†…è·¯å¾„æŒ‰åŸå§‹ç¼–å·é¡ºåº
        })

    # æ­¥éª¤4ï¼šæŒ‰åŸå§‹ç¼–å·1-20æ‹¼æ¥æœ€ç»ˆæ¶ˆæ¯ï¼ˆç¡®ä¿é¡ºåºä¸å˜ï¼‰
    msg = "æ‰¾åˆ°ä»¥ä¸‹åŒ¹é…çš„æ–‡ä»¶å¤¹ï¼Œè¯·è¾“å…¥åºå·é€‰æ‹©ï¼š\n\n"
    # ç”¨å­—å…¸æš‚å­˜æ‰€æœ‰è·¯å¾„ï¼ˆkey=åŸå§‹ç¼–å·ï¼Œvalue=ï¼ˆçˆ¶ç›®å½•ï¼Œå­è·¯å¾„ï¼‰ï¼‰
    seq_path_dict = {}
    for group in processed_groups:
        parent = group["parent_str"]
        for path in group["paths"]:
            seq_path_dict[path["orig_seq"]] = (parent, path["sub_path"])

    # æŒ‰ç¼–å·1-20ä¾æ¬¡éå†ï¼Œæ˜¾ç¤ºç»“æœ
    last_parent = None  # é¿å…é‡å¤æ˜¾ç¤ºçˆ¶ç›®å½•
    for orig_seq in range(1, len(seq_path_dict) + 1):
        parent, sub_path = seq_path_dict[orig_seq]
        
        # çˆ¶ç›®å½•å˜åŒ–æ—¶ï¼Œæ˜¾ç¤ºæ–°çˆ¶ç›®å½•
        if parent != last_parent:
            msg += f"ğŸ“ {parent}\n"
            last_parent = parent
        
        # æ˜¾ç¤ºç¼–å·å’Œå­è·¯å¾„
        msg += f"      {orig_seq}ï¼š{sub_path}\n"

        # ç»„é—´ç©ºè¡Œï¼ˆåˆ¤æ–­ä¸‹ä¸€ä¸ªç¼–å·çš„çˆ¶ç›®å½•æ˜¯å¦å˜åŒ–ï¼‰
        next_seq = orig_seq + 1
        if next_seq in seq_path_dict:
            next_parent = seq_path_dict[next_seq][0]
            if next_parent != parent:
                msg += "\n"

    msg += "\nè¯·è¾“å…¥åºå·ï¼ˆä¾‹ï¼š1ï¼‰é€‰æ‹©ï¼Œå¤šé€‰ç”¨ç©ºæ ¼åˆ†éš”ï¼ˆä¾‹ï¼š1 2 3ï¼‰"
    return msg




def perform_search(client, keyword, user_id, chat_id, mode="link"):
    try:
        import asyncio
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        results = loop.run_until_complete(search_123_files(client, keyword))
        if not results:
            reply_thread_pool.submit(send_message_with_id, chat_id, "æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶å¤¹")
            return
        
        # å°†ç»“æœå’Œæ¨¡å¼ä¸€èµ·å­˜å…¥çŠ¶æ€
        state_data = {
            "results": results,
            "mode": mode
        }
        user_state_manager.set_state(user_id, "SELECTING_FILE", json.dumps(state_data))
        
        folder_message = build_folder_message(results)
        reply_thread_pool.submit(send_message_with_id, chat_id, folder_message)
    except Exception as e:
        reply_thread_pool.submit(send_message_with_id, chat_id, f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        logger.error(f"æœç´¢æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")

from add_mag import submit_magnet_video_download
def add_magnet_links(client: P123Client, text, upload_dir=None, message=None):
    """è¯†åˆ«æ–‡æœ¬ä¸­çš„å¤šä¸ªç£åŠ›é“¾æ¥å¹¶æ·»åŠ åˆ°ç¦»çº¿ä¸‹è½½"""
    import re
    magnet_pattern = r'magnet:\?xt=urn:btih:(?:[A-Fa-f0-9]{40}(?![A-Fa-f0-9])|[A-Za-z0-9]{32}(?![A-Za-z0-9]))(?:&.*?)?'
    magnet_links = re.findall(magnet_pattern, text)
    magnet_links = list(set(magnet_links))
    if not magnet_links:
        return {'status': 'error', 'message': 'æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥', 'added_count': 0}
    
    logger.debug(f"æ‰¾åˆ°ç£åŠ›é“¾æ¥:{magnet_links}")
    if message:
        reply_thread_pool.submit(send_reply, message, f"æ‰¾åˆ°{len(magnet_links)}æ¡ç£åŠ›é“¾\næ­£åœ¨æ·»åŠ ...")
    
    added_count = 0
    responses = []
    try:
        for link in magnet_links:
            # æ–°ç‰ˆåº“é€šå¸¸ä½¿ç”¨ offline_download_add_url
            # å‚æ•°å¯èƒ½æ˜¯ url, save_path=None, ...
            # å¦‚æœæ–°ç‰ˆåº“ç§»é™¤äº† offline_addï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„æ ‡å‡†æ–¹æ³•
            try:
                # å°è¯•ä½¿ç”¨æ–°ç‰ˆæ–¹æ³•
                response = client.offline_download_add_url(
                    url=link,
                    parent_id=upload_dir
                )
            except AttributeError:
                # å›é€€åˆ° helper å‡½æ•°
                response = submit_magnet_video_download(link, client.token, upload_dir)
                
            time.sleep(0.5)
            responses.append({'link': link, 'response': response})
            added_count += 1
        return {'status': 'success', 'data': responses, 'added_count': added_count}
    except Exception as e:
        return {'status': 'error', 'message': f'æ·»åŠ ç£åŠ›é“¾æ¥å¤±è´¥: {str(e)}', 'added_count': added_count}

import base64
import binascii
import re

def robust_normalize_md5(input_str):
    """
    è‡ªåŠ¨è¯†åˆ«MD5æ ¼å¼å¹¶è½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼ï¼Œå¼‚å¸¸æ—¶è¿”å›åŸå§‹è¾“å…¥
    
    å‚æ•°:
        input_str: å¾…å¤„ç†çš„è¾“å…¥ï¼ˆå¯ä»¥æ˜¯ä»»ä½•ç±»å‹ï¼‰
    
    è¿”å›:
        è½¬æ¢åçš„åå…­è¿›åˆ¶MD5ï¼ˆå°å†™ï¼‰ï¼Œæˆ–åŸå§‹è¾“å…¥ï¼ˆå¤„ç†å¤±è´¥æ—¶ï¼‰
    """
    # å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºå­—ç¬¦ä¸²ç±»å‹ï¼Œéå­—ç¬¦ä¸²ç›´æ¥è¿”å›åŸå§‹å€¼
    if not isinstance(input_str, str):
        return input_str
    
    # å¤„ç†ç©ºå­—ç¬¦ä¸²
    if not input_str:
        return input_str
    
    # å»é™¤é¦–å°¾ç©ºæ ¼
    processed_str = input_str.strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºåå…­è¿›åˆ¶MD5ï¼ˆ32ä½ï¼Œä»…å«0-9ã€a-fã€A-Fï¼‰
    hex_pattern = re.compile(r'^[0-9a-fA-F]{32}$')
    if hex_pattern.match(processed_str):
        return processed_str.lower()
    
    # å°è¯•Base64è§£ç å¤„ç†
    try:
        # å°è¯•Base64è§£ç ï¼ˆå¤„ç†æ ‡å‡†Base64å’ŒURLå®‰å…¨çš„Base64ï¼‰
        binary_data = base64.b64decode(processed_str, validate=True)
        
        # éªŒè¯MD5å›ºå®šé•¿åº¦ï¼ˆ16å­—èŠ‚ï¼‰
        if len(binary_data) == 16:
            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆå°å†™ï¼‰
            return binascii.hexlify(binary_data).decode('utf-8').lower()
    
    # æ•æ‰Base64è§£ç ç›¸å…³å¼‚å¸¸
    except binascii.Error:
        pass
    # æ•æ‰å…¶ä»–å¯èƒ½çš„å¼‚å¸¸
    except Exception:
        pass
    
    # æ‰€æœ‰å¤„ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹è¾“å…¥
    return input_str

def parse_share_link(message, share_link, up_load_pid=UPLOAD_JSON_TARGET_PID, send_messages=True):
    """
    è§£æç§’ä¼ é“¾æ¥å¹¶è½¬å­˜ (é€‚é…æ–°ç‰ˆ p123client + æ—¥å¿—å¢å¼ºç‰ˆ)
    """
    # ================= é“¾æ¥è§£æéƒ¨åˆ† (ä¿æŒåŸé€»è¾‘) =================
    if '#' in share_link and '$' in share_link:
        pass
    else:
        return False
        
    logger.info("æ­£åœ¨è§£æç§’ä¼ é“¾æ¥...")
    
    common_base_path = ""
    is_common_path_format = False
    is_v2_etag_format = False
    
    # å®šä¹‰å‰ç¼€å¸¸é‡
    LEGACY_FOLDER_LINK_PREFIX_V1 = "123FSLinkV1$"
    LEGACY_FOLDER_LINK_PREFIX_V2 = "123FSLinkV2$"
    COMMON_PATH_LINK_PREFIX_V1 = "123FLCPV1$"
    COMMON_PATH_LINK_PREFIX_V2 = "123FLCPV2$"
    COMMON_PATH_DELIMITER = "%"
    
    # åˆ¤æ–­é“¾æ¥ç‰ˆæœ¬å¹¶å‰¥ç¦»å‰ç¼€
    if share_link.startswith(COMMON_PATH_LINK_PREFIX_V2):
        is_common_path_format = True
        is_v2_etag_format = True
        share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V2):]
    elif share_link.startswith(COMMON_PATH_LINK_PREFIX_V1):
        is_common_path_format = True
        share_link = share_link[len(COMMON_PATH_LINK_PREFIX_V1):]
    elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V2):
        is_v2_etag_format = True
        share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V2):]
    elif share_link.startswith(LEGACY_FOLDER_LINK_PREFIX_V1):
        share_link = share_link[len(LEGACY_FOLDER_LINK_PREFIX_V1):]
        
    if is_common_path_format:
        delimiter_pos = share_link.find(COMMON_PATH_DELIMITER)
        if delimiter_pos > -1:
            common_base_path = share_link[:delimiter_pos]
            share_link = share_link[delimiter_pos + 1:]
            
    # è§£ææ–‡ä»¶åˆ—è¡¨
    files = []
    for s_link in share_link.split('$'):
        if not s_link: continue
        parts = s_link.split('#')
        if len(parts) < 3: continue
        
        etag = parts[0]
        size = parts[1]
        file_path = '#'.join(parts[2:])
        
        if is_common_path_format and common_base_path:
            file_path = common_base_path + file_path

        # æ³¨æ„ï¼šè¿™é‡Œ file_path å¯èƒ½æ˜¯å®Œæ•´è·¯å¾„ "folder/file.jpg"
        if check_ext_filter(file_path):
             # å¯ä»¥åœ¨è¿™é‡Œè®°å½•æ—¥å¿—ï¼Œä½†ä¸ºäº†é¿å…åˆ·å±ï¼Œå¯ä»¥é€‰æ‹©ä¸è®°å½•æˆ–debugè®°å½•
             continue 
        # =========================
            
        files.append({
            "etag": etag,
            "size": int(size),
            "file_name": file_path,
            "is_v2_etag": is_v2_etag_format
        })
    
    logger.info(f"è§£æå®Œæˆ: å…± {len(files)} ä¸ªæ–‡ä»¶ (å·²è¿‡æ»¤åç¼€)")
    
    if not files:
        return False

    status = True
    # å‘é€åˆå§‹é€šçŸ¥
    if send_messages:
        reply_thread_pool.submit(send_reply_delete, message, f"ğŸš€ å¼€å§‹è½¬å­˜ {len(files)} ä¸ªæ–‡ä»¶...")
    
    try:
        # å¼€å§‹è®¡æ—¶
        start_time = time.time()
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        client = init_123_client()
        
        # ç»Ÿè®¡å˜é‡
        results = []
        message_batch = []  # æ¶ˆæ¯æ‰¹æ¬¡ç¼“å­˜
        batch_size = 0      # å½“å‰æ‰¹æ¬¡è®¡æ•°
        total_size = 0      # æˆåŠŸè½¬å­˜ä½“ç§¯
        skip_count = 0      # è·³è¿‡è®¡æ•°
        last_etag = None    # ä¸Šä¸€ä¸ªæ–‡ä»¶çš„ETag (ç”¨äºå»é‡)
        
        # æ–‡ä»¶å¤¹IDç¼“å­˜ { "çˆ¶ID/æ–‡ä»¶å¤¹å": æ–°ID }
        folder_cache = {}
        target_dir_id = up_load_pid
        
        total_files = len(files)

        for i, file_info in enumerate(files):
            file_path = file_info.get('file_name', '')
            etag = file_info.get('etag', '')
            size = int(file_info.get('size', 0))
            is_v2_etag = file_info.get('is_v2_etag', False)
            
            # æ•°æ®å®Œæ•´æ€§æ ¡éªŒ
            if not all([file_path, etag, size]):
                error_msg = "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´"
                results.append({"success": False, "file_name": file_path, "error": error_msg})
                logger.error(f"âŒ {file_path}: {error_msg}")
                continue
            
            try:
                # ---------------- 1. ç›®å½•ç»“æ„åˆ›å»º ----------------
                path_parts = file_path.split('/')
                file_name = path_parts.pop() # å–å‡ºæ–‡ä»¶åï¼Œå‰©ä¸‹çš„å°±æ˜¯ç›®å½•
                current_parent_id = target_dir_id
                
                # é€çº§æ£€æŸ¥/åˆ›å»ºç›®å½•
                temp_path_str = ""
                for part in path_parts:
                    if not part: continue
                    temp_path_str = f"{temp_path_str}/{part}" if temp_path_str else part
                    cache_key = f"{current_parent_id}/{part}"
                    
                    if cache_key in folder_cache:
                        current_parent_id = folder_cache[cache_key]
                        continue
                    
                    # åˆ›å»ºç›®å½• (å¸¦é‡è¯•)
                    mk_retry = 2
                    folder_id = None
                    while mk_retry > 0:
                        try:
                            # client.fs_mkdir æ˜¯æ ‡å‡†API
                            resp = client.fs_mkdir(name=part, parent_id=current_parent_id)
                            check_response(resp)
                            folder_id = resp["data"]["Info"]["FileId"]
                            break
                        except Exception as e:
                            mk_retry -= 1
                            if mk_retry == 0:
                                logger.warning(f"åˆ›å»ºæ–‡ä»¶å¤¹å¤±è´¥ '{part}': {e}")
                            time.sleep(0.5)
                    
                    if folder_id:
                        folder_cache[cache_key] = folder_id
                        current_parent_id = folder_id
                    else:
                        # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œå°è¯•æ²¿ç”¨ä¸Šçº§IDï¼Œé˜²æ­¢ç¨‹åºå®Œå…¨å´©æºƒ
                        pass

                # ---------------- 2. ETag å¤„ç† ----------------
                # å¦‚æœæ˜¯ V2 æ ¼å¼ (Base62)ï¼Œè½¬ä¸º Hex MD5
                if is_v2_etag:
                    etag = optimized_etag_to_hex(etag, True)
                
                # æ ‡å‡†åŒ– MD5 (ç¡®ä¿å°å†™ä¸”åˆæ³•)
                final_md5 = robust_normalize_md5(etag)

                # ---------------- 3. ç§’ä¼ æ ¸å¿ƒé€»è¾‘ ----------------
                retry_count = 3
                rapid_resp = None
                is_skipped = False
                
                while retry_count > 0:
                    # 3.1 è¿ç»­é‡å¤æ–‡ä»¶æ£€æµ‹ (Simple Deduplication)
                    if last_etag == final_md5:
                        skip_count += 1
                        is_skipped = True
                        # æ„é€ ä¸€ä¸ªä¼ªé€ çš„æˆåŠŸå“åº”
                        rapid_resp = {"code": 0, "data": {"reuse": True, "skip": True}}
                        logger.info(f"ğŸ”„ è·³è¿‡é‡å¤æ–‡ä»¶: {file_name}")
                        break
                    
                    try:
                        # 3.2 è°ƒç”¨æ ‡å‡† API: upload_file_fast
                        rapid_resp = client.upload_file_fast(
                            file_name=file_name,
                            parent_id=current_parent_id,
                            file_md5=final_md5,
                            file_size=size,
                            duplicate=1
                        )
                        
                        # 3.3 åˆ¤æ–­ç»“æœ
                        is_reused = rapid_resp.get("data", {}).get("Reuse") or rapid_resp.get("data", {}).get("reuse")
                        if is_reused:
                            break # æˆåŠŸï¼Œè·³å‡ºé‡è¯•
                        else:

                            break 
                            
                    except Exception as e:
                        retry_count -= 1
                        logger.warning(f"ç§’ä¼ è¯·æ±‚å¼‚å¸¸ {file_name}: {e} (å‰©ä½™é‡è¯•: {retry_count})")
                        time.sleep(2)
                        if retry_count == 0:
                            rapid_resp = {"code": -1, "message": str(e)}

                # ---------------- 4. ç»“æœå¤„ç†ä¸æ—¥å¿—è®°å½• ----------------
                dir_path = os.path.dirname(file_path)
                
                # æˆåŠŸåˆ¤å®šï¼šcode=0 ä¸” reuse=True (å…¼å®¹å¤§å°å†™)
                is_success_response = rapid_resp and rapid_resp.get("code") == 0 and \
                                      (rapid_resp.get("data", {}).get("Reuse") or rapid_resp.get("data", {}).get("reuse"))

                if is_success_response:
                    
                    if is_skipped:
                        status_icon = 'ğŸ”„'
                        log_msg = f"{file_name} (é‡å¤è·³è¿‡)"
                    else:
                        status_icon = 'âœ…'
                        log_msg = file_name
                        total_size += size
                        last_etag = final_md5 # æ›´æ–° last_etag
                        results.append({"success": True, "file_name": file_path, "size": size})

                    logger.info(f"{status_icon} è½¬å­˜æˆåŠŸ: {dir_path}/{log_msg}")
                    
                    # æ·»åŠ åˆ°æ‰¹æ¬¡æ¶ˆæ¯
                    message_batch.append({
                        'status': status_icon,
                        'dir': dir_path,
                        'file': log_msg
                    })
                    
                else:
                    # å¤±è´¥å¤„ç†
                    status_icon = 'âŒ'
                    # åˆ¤æ–­æ˜¯å¦å› ä¸ºæ–‡ä»¶ä¸åœ¨äº‘ç«¯å¯¼è‡´å¤±è´¥ (å…¼å®¹å¤§å°å†™)
                    is_not_reused = rapid_resp and rapid_resp.get("code") == 0 and \
                                    not (rapid_resp.get("data", {}).get("Reuse") or rapid_resp.get("data", {}).get("reuse"))

                    if is_not_reused:
                        err_reason = "æ–‡ä»¶æœªåœ¨äº‘ç«¯ï¼Œæ— æ³•ç§’ä¼ "
                    else:
                        err_reason = rapid_resp.get("message", "æœªçŸ¥é”™è¯¯") if rapid_resp else "è¯·æ±‚æ— å“åº”"
                    
                    logger.warning(f"{status_icon} è½¬å­˜å¤±è´¥: {dir_path}/{file_name} ({err_reason})")
                    
                    results.append({"success": False, "file_name": file_path, "error": err_reason})
                    
                    message_batch.append({
                        'status': status_icon,
                        'dir': dir_path,
                        'file': f"{file_name} ({err_reason})"
                    })

                batch_size += 1

                # ---------------- 5. æ‰¹æ¬¡é€šçŸ¥ (æ¯10æ¡) ----------------
                if batch_size % 10 == 0:
                    tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                    for entry in message_batch:
                        tree_messages[entry['dir']][entry['status']].append(entry['file'])
                    
                    batch_msg_lines = []
                    for d_path, status_files in tree_messages.items():
                        for stat, f_list in status_files.items():
                            if f_list:
                                batch_msg_lines.append(f"--- {stat} {d_path}")
                                for idx, fname in enumerate(f_list):
                                    prefix = '      â””â”€â”€' if idx == len(f_list)-1 else '      â”œâ”€â”€'
                                    batch_msg_lines.append(f"{prefix} {fname}")
                    
                    full_batch_msg = "\n".join(batch_msg_lines)
                    progress_text = f"ğŸ“Š è¿›åº¦: {batch_size}/{total_files} ({int(batch_size/total_files*100)}%)\n\n{full_batch_msg}"
                    
                    if send_messages:
                        reply_thread_pool.submit(send_reply_delete, message, progress_text)
                    
                    # æ¸…ç©ºæ‰¹æ¬¡ç¼“å­˜
                    message_batch = []
                
                # é€Ÿç‡æ§åˆ¶
                time.sleep(1.0 / get_int_env("ENV_FILE_PER_SECOND", 5))
                
            except Exception as e:
                # æ•è·å•ä¸ªæ–‡ä»¶å¤„ç†ä¸­çš„ä¸¥é‡é”™è¯¯
                logger.error(f"å¤„ç†æ–‡ä»¶å¼‚å¸¸ {file_path}: {e}")
                results.append({"success": False, "file_name": file_path, "error": str(e)})
                
                dir_path, fname = os.path.split(file_path)
                message_batch.append({
                    'status': 'âŒ', 
                    'dir': dir_path, 
                    'file': f"{fname} (ç³»ç»Ÿå¼‚å¸¸)"
                })
                batch_size += 1

        # ---------------- 6. å‘é€å‰©ä½™æ¶ˆæ¯ ----------------
        if message_batch and send_messages:
            tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
            for entry in message_batch:
                tree_messages[entry['dir']][entry['status']].append(entry['file'])
            
            batch_msg_lines = []
            for d_path, status_files in tree_messages.items():
                for stat, f_list in status_files.items():
                    if f_list:
                        batch_msg_lines.append(f"--- {stat} {d_path}")
                        for idx, fname in enumerate(f_list):
                            prefix = '      â””â”€â”€' if idx == len(f_list)-1 else '      â”œâ”€â”€'
                            batch_msg_lines.append(f"{prefix} {fname}")
            
            full_batch_msg = "\n".join(batch_msg_lines)
            reply_thread_pool.submit(send_reply_delete, message, f"ğŸ“Š è¿›åº¦: {batch_size}/{total_files} (100%)\n\n{full_batch_msg}")

        # ---------------- 7. æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š ----------------
        end_time = time.time()
        elapsed_time = round(end_time - start_time, 2)
        
        success_count = sum(1 for r in results if r['success'])
        fail_count = len(results) - success_count
        
        # æ ¼å¼åŒ–ä½“ç§¯
        total_size_gb = total_size / (1024 ** 3)
        avg_size = total_size / success_count if success_count > 0 else 0
        avg_size_gb = avg_size / (1024 ** 3)
        
        # æ„é€ æ±‡æ€»æ¶ˆæ¯
        summary = (
            f"âœ… ç§’ä¼ ä»»åŠ¡å®Œæˆï¼\n"
            f"ğŸ“ æ–‡ä»¶æ€»æ•°: {total_files}\n"
            f"âœ… æˆåŠŸè½¬å­˜: {success_count}\n"
            f"ğŸ”„ é‡å¤è·³è¿‡: {skip_count}\n"
            f"âŒ è½¬å­˜å¤±è´¥: {fail_count}\n"
            f"ğŸ“¦ æ€»è®¡ä½“ç§¯: {total_size_gb:.2f} GB\n"
            f"ğŸ“Š å¹³å‡å¤§å°: {avg_size_gb:.2f} GB\n"
            f"â±ï¸ è€—æ—¶ç»Ÿè®¡: {elapsed_time} ç§’"
        )
        
        # æ„é€ å¤±è´¥è¯¦æƒ…
        error_details = ""
        if fail_count > 0:
            failed_list = [f"â€¢ {r['file_name']} ({r.get('error','æœªçŸ¥')})" for r in results if not r['success']]
            # æœ€å¤šæ˜¾ç¤º15æ¡é”™è¯¯ï¼Œé¿å…æ¶ˆæ¯è¿‡é•¿
            display_fails = failed_list[:15]
            error_details = "\n\nâŒ å¤±è´¥è¯¦æƒ… (å‰15æ¡):\n" + "\n".join(display_fails)
            if len(failed_list) > 15:
                error_details += f"\n... ä»¥åŠå…¶ä»– {len(failed_list)-15} ä¸ªé”™è¯¯"

        final_msg = summary + error_details
        
        logger.info(f"ä»»åŠ¡ç»“æŸ: {summary.replace(chr(10), ' | ')}") # chr(10) is \n
        
        if send_messages:
            reply_thread_pool.submit(send_reply, message, final_msg)
        
        # å¦‚æœå¤±è´¥å¤ªå¤šï¼Œè¿”å›False
        if fail_count == total_files:
            return False
            
    except Exception as e:
        logger.error(f"å¤„ç†ç§’ä¼ é“¾æ¥å…¨å±€å¼‚å¸¸: {str(e)}")
        if send_messages:
            reply_thread_pool.submit(send_reply, message, f"å¤„ç†å¼‚å¸¸: {str(e)}")
        status = False
    
    return status

def extract_123_links_from_full_text(message_str):
    """
    æå–ç¬¦åˆæ¡ä»¶çš„123ç³»åˆ—ç§’ä¼ é“¾æ¥
    ç‰¹å¾ï¼šä»¥123FSLinkV1/2ã€123FLCPV1/2å¼€å¤´ï¼Œä»¥æ–‡æœ¬å½¢å¼\nï¼ˆå­—ç¬¦ä¸²"\\n"ï¼‰æˆ–ğŸ”ä¸ºç»“æŸæ ‡å¿—
          è‹¥æœªåŒ¹é…åˆ°ç»“æŸæ ‡å¿—ï¼Œåˆ™è‡ªåŠ¨åŒ¹é…åˆ°æ–‡æœ¬æœ«å°¾
    :param message_str: å®Œæ•´çš„åŸå§‹å­—ç¬¦ä¸²
    :return: åŒ¹é…åˆ°çš„é“¾æ¥åˆ—è¡¨ï¼ˆå»é‡å¹¶ä¿ç•™åŸå§‹é¡ºåºï¼‰
    """
    # æ„å»ºæ­£åˆ™ï¼š
    # 1. åŒ¹é…æŒ‡å®šå¼€å¤´ (123FSLinkV1/2 æˆ– 123FLCPV1/2)
    # 2. .*? éè´ªå©ªåŒ¹é…ä»»æ„å­—ç¬¦ï¼ˆåŒ…æ‹¬å®é™…æ¢è¡Œï¼Œå› å¯ç”¨DOTALLï¼‰
    # 3. (?=\\n|ğŸ”|$) æ­£å‘é¢„æŸ¥ï¼šåŒ¹é…åˆ°æ–‡æœ¬"\\n"ã€"ğŸ”"æˆ–æ–‡æœ¬æœ«å°¾æ—¶åœæ­¢ï¼ˆä¸åŒ…å«ç»“æŸæ ‡å¿—æœ¬èº«ï¼‰
    # æ³¨æ„ï¼šæ­£åˆ™ä¸­ç”¨\\nè¡¨ç¤ºæ–‡æœ¬ä¸­çš„"\n"ï¼ˆéœ€è½¬ä¹‰åæ–œæ ï¼‰
    link_pattern = re.compile(
        r'(123FSLinkV[12]|123FLCPV[12]).*?(?=\\n|\'}|\',|$)',
        re.DOTALL  # è®©.åŒ¹é…å®é™…æ¢è¡Œç¬¦ï¼ˆè‹¥æ–‡æœ¬ä¸­å­˜åœ¨ï¼‰
    )

    # æå–æ‰€æœ‰åŒ¹é…çš„é“¾æ¥
    matched_links = [match.group(0) for match in link_pattern.finditer(message_str)]
    
    # å»é‡å¹¶ä¿ç•™åŸå§‹é¡ºåº
    return list(dict.fromkeys(matched_links))

def extract_kuake_target_url(text):
    # åŒ¹é…æ ‡å‡†å¤¸å…‹é“¾æ¥ï¼ˆhttp/httpså¼€å¤´ï¼Œæå–æ ¸å¿ƒshare_idï¼‰
    link_pattern = r'https?://pan\.quark\.cn/s/([\w-]+)(?:[#?].*)?'
    # åŒ¹é…é“¾æ¥è‡ªå¸¦çš„pwdå‚æ•°
    pwd_in_link_pattern = r'[?&]pwd=(\w+)'
    # åŒ¹é…æ–‡æœ¬ä¸­çš„æå–ç ï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
    pwd_text_pattern = r'æå–ç [ï¼š:]?\s*(\w+)'

    # å…³é”®ä¼˜åŒ–1ï¼šç”¨é›†åˆè®°å½•å·²å¤„ç†çš„share_idï¼Œé¿å…é‡å¤æ·»åŠ åŒä¸€é“¾æ¥
    processed_share_ids = set()
    link_info_list = []
    
    for match in re.finditer(link_pattern, text, re.IGNORECASE):
        share_id = match.group(1)
        if not share_id or share_id in processed_share_ids:  # é‡å¤share_idç›´æ¥è·³è¿‡
            continue
        
        original_link = match.group(0)
        built_in_pwd = re.search(pwd_in_link_pattern, original_link).group(1) if re.search(pwd_in_link_pattern, original_link) else None
        
        link_info_list.append({"share_id": share_id.strip(), "built_in_pwd": built_in_pwd})
        processed_share_ids.add(share_id)  # æ ‡è®°ä¸ºå·²å¤„ç†

    # æå–æ–‡æœ¬æå–ç ï¼ˆå»é‡ä¿åºï¼‰
    passwords = list(dict.fromkeys(re.findall(pwd_text_pattern, text, re.IGNORECASE)))

    # ç”Ÿæˆæ ‡å‡†åŒ–é“¾æ¥
    processed_links = []
    for idx, info in enumerate(link_info_list):
        base_url = f"https://pan.quark.cn/s/{info['share_id']}"
        # å…³é”®ä¼˜åŒ–2ï¼šç¡®ä¿pwdåŒ¹é…é€»è¾‘ä¸é”™ä½ï¼ˆä¼˜å…ˆè‡ªå¸¦pwdï¼Œæ— åˆ™æŒ‰ç´¢å¼•å–æ–‡æœ¬pwdï¼‰
        final_pwd = info['built_in_pwd']
        if not final_pwd and idx < len(passwords):
            final_pwd = passwords[idx]
        
        final_url = f"{base_url}?pwd={final_pwd}" if final_pwd else base_url
        processed_links.append(final_url)

    # æœ€ç»ˆå»é‡ï¼ˆä¿åºï¼‰
    return list(dict.fromkeys(processed_links))

# ================= [å¼€å§‹] æ–°å¢ sync189 é€»è¾‘ =================
def clean_filename(name):
    """
    æ¸…æ´—æ–‡ä»¶åï¼Œå»é™¤éæ³•å­—ç¬¦
    """
    if not name:
        return "Unknown_Folder"
    
    # 1. å»é™¤é¦–å°¾ç©ºæ ¼
    name = name.strip()
    
    # 2. æ›¿æ¢éæ³•å­—ç¬¦ (Windows/ç½‘ç›˜é€šç”¨é™åˆ¶: \ / : * ? " < > |)
    # å°†å®ƒä»¬æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ _
    name = re.sub(r'[\\/:*?"<>|]', '_', name)
    
    # 3. å»é™¤æ§åˆ¶å­—ç¬¦ (å¦‚æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰)
    name = re.sub(r'[\x00-\x1f\x7f]', '', name)
    
    # 4. å†æ¬¡å»é™¤å¯èƒ½çš„é¦–å°¾ç‚¹å·æˆ–ç©ºæ ¼
    name = name.strip('. ')
    
    return name

def find_child_folder_id(client, parent_id, folder_name):
    """
    åœ¨æŒ‡å®šçˆ¶ç›®å½•ä¸‹æŸ¥æ‰¾ç‰¹å®šåç§°çš„å­æ–‡ä»¶å¤¹ID
    ç”¨äºè§£å†³æ–‡ä»¶å¤¹å·²å­˜åœ¨å¯¼è‡´çš„åˆ›å»ºå¤±è´¥é—®é¢˜
    """
    try:
        # ä½¿ç”¨ v2 æ¥å£åˆ—å‡ºæ–‡ä»¶
        url = "https://open-api.123pan.com/api/v2/file/list"
        
        # éå†å‡ é¡µï¼Œé˜²æ­¢æ–‡ä»¶å¤¹å†…å®¹å¤ªå¤šå¯¼è‡´æ‰¾ä¸åˆ°ï¼ˆé€šå¸¸å‰100ä¸ªå°±èƒ½æ‰¾åˆ°ï¼‰
        last_file_id = 0
        
        # æœ€å¤šå¾€åç¿»3é¡µ(300ä¸ªæ–‡ä»¶)ï¼Œé€šå¸¸è¶³å¤Ÿäº†
        for _ in range(3): 
            params = {
                "parentFileId": parent_id,
                "limit": 100,
                "lastFileId": last_file_id,
                "trashed": 0,
                "orderBy": "fileId",
                "orderDirection": "desc"
            }
            headers = {
                "Authorization": f"Bearer {client.token}",
                "Platform": "open_platform"
            }
            
            resp = requests.get(url, params=params, headers=headers, timeout=10)
            res_json = resp.json()
            
            if res_json.get("code") != 0:
                # å¦‚æœæ¥å£æŠ¥é”™ï¼Œåœæ­¢æŸ¥æ‰¾
                break
                
            file_list = res_json.get("data", {}).get("fileList", [])
            if not file_list:
                break
                
            for item in file_list:
                # type=1 æ˜¯æ–‡ä»¶å¤¹ï¼Œä¸”åç§°å®Œå…¨åŒ¹é…
                if item.get("type") == 1 and item.get("filename") == folder_name:
                    return item.get("fileId")
            
            # è·å–ä¸‹ä¸€é¡µçš„æ¸¸æ ‡
            last_file_id = res_json.get("data", {}).get("lastFileId", 0)
            if last_file_id == 0:
                break
                
    except Exception as e:
        logger.error(f"æŸ¥æ‰¾æ–‡ä»¶å¤¹å¼‚å¸¸: {e}")
        
    return None

# --- å…¨å±€é”ï¼Œç”¨äºä¿æŠ¤æ–‡ä»¶å¤¹åˆ›å»º ---
folder_lock = threading.Lock()

def get_progress_bar(current, total, length=15):
    """ç”Ÿæˆè¿›åº¦æ¡å­—ç¬¦ä¸² [â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘]"""
    if total == 0:
        return "[]"
    percent = current / total
    filled_length = int(length * percent)
    bar = "â–ˆ" * filled_length + "â–‘" * (length - filled_length)
    return f"[{bar}] {int(percent * 100)}%"

def sync_file_worker(client123, file_info, root_123_pid, folder_cache):
    """
    [å­çº¿ç¨‹å·¥ä½œå‡½æ•°] å¤„ç†å•ä¸ªæ–‡ä»¶çš„ç›®å½•æ£€æŸ¥ä¸ç§’ä¼ 
    """   
    # å¢åŠ å‰ç½®æ£€æŸ¥
    if check_ext_filter(file_info['file_name']):
        # è¿”å›ä¸€ä¸ªç‰¹æ®Šçš„è·³è¿‡çŠ¶æ€ï¼Œæˆ–è€…ç›´æ¥å½“åšæˆåŠŸä½†ä¸å¤„ç†
        # è¿™é‡Œå»ºè®®è¿”å› fail æˆ–æ–°å¢ skipped çŠ¶æ€ï¼Œè¿™é‡Œç®€å•è¿”å› skipped
        return {"status": "skipped", "name": file_info['file_name'], "msg": "åç¼€è¿‡æ»¤"}     
    try:
        # === 1. ç›®å½•ç»“æ„å¤„ç† (å¿…é¡»åŠ é”) ===
        relative_path = file_info.get('parent_path', '/').strip('/')
        current_123_parent_id = root_123_pid

        if relative_path:
            # æ¶‰åŠè¯»å–/å†™å…¥ folder_cache å’Œ API åˆ›å»ºï¼Œå¿…é¡»äº’æ–¥
            with folder_lock:
                path_parts = relative_path.split('/')
                current_path_str = ""
                
                for raw_part in path_parts:
                    if not raw_part: continue
                    part = clean_filename(raw_part) # éœ€ç¡®ä¿ clean_filename å·²å®šä¹‰
                    current_path_str += f"/{part}"
                    
                    # æŸ¥ç¼“å­˜
                    if current_path_str in folder_cache:
                        current_123_parent_id = folder_cache[current_path_str]
                    else:
                        # æŸ¥äº‘ç«¯ / åˆ›å»º
                        new_folder_id = None
                        found_id = find_child_folder_id(client123, current_123_parent_id, part)
                        if found_id:
                            new_folder_id = found_id
                        else:
                            try:
                                resp = client123.fs_mkdir(part, parent_id=current_123_parent_id)
                                if resp.get("code") == 0:
                                    new_folder_id = resp["data"]["Info"]["FileId"]
                            except Exception:
                                pass # çº¿ç¨‹ä¸­ä¸å®œè¿‡å¤šæ‰“å°åˆ›å»ºå¤±è´¥æ—¥å¿—
                        
                        if new_folder_id:
                            folder_cache[current_path_str] = new_folder_id
                            current_123_parent_id = new_folder_id
                        else:
                            # å¤±è´¥å›é€€åˆ°æ ¹ç›®å½•
                            current_123_parent_id = root_123_pid

        # === 2. æ‰§è¡Œç§’ä¼  (è€—æ—¶æ“ä½œï¼Œå¹¶è¡Œæ‰§è¡Œ) ===
        rapid_resp = client123.upload_file_fast(
            file_name=file_info['file_name'],
            parent_id=current_123_parent_id,
            file_md5=robust_normalize_md5(file_info['md5']),
            file_size=int(file_info['file_size']),
            duplicate=1
        )

        is_success = False
        if rapid_resp.get("code") == 0:
            data = rapid_resp.get("data", {})
            if data and (data.get("Reuse") or data.get("reuse")):
                is_success = True
        
        if is_success:
            return {"status": "success", "file_id": file_info['file_id'], "name": file_info['file_name']}
        else:
            return {"status": "fail", "name": file_info['file_name']}

    except Exception as e:
        return {"status": "error", "msg": str(e), "name": file_info['file_name']}

from bot189 import Cloud189
from concurrent.futures import ThreadPoolExecutor
# [ä¿®æ”¹å V5] å¤šçº¿ç¨‹å¹¶å‘ + æ™ºèƒ½è¿›åº¦æ¡åé¦ˆ
def process_189_to_123_sync(message):
    user_id = message.from_user.id
    target_189_pid = os.getenv("ENV_189_UPLOAD_PID", "")
    root_123_pid = UPLOAD_TARGET_PID 

    if not target_189_pid:
        reply_thread_pool.submit(send_reply, message, "âŒ æœªé…ç½® ENV_189_UPLOAD_PID")
        return

    # --- 1. åˆå§‹åŒ–å¤©ç¿¼äº‘ ---
    client189 = Cloud189()
    if not client189.check_cookie_valid():
        env_189_id = os.getenv("ENV_189_CLIENT_ID", "")
        env_189_secret = os.getenv("ENV_189_CLIENT_SECRET", "")
        if env_189_id and env_189_secret:
            logger.info("å¤©ç¿¼äº‘Cookieå¤±æ•ˆï¼Œå°è¯•è‡ªåŠ¨ç™»å½•...")
            if not client189.login(env_189_id, env_189_secret):
                reply_thread_pool.submit(send_reply, message, "âŒ å¤©ç¿¼äº‘ç™»å½•å¤±è´¥")
                return
        else:
            reply_thread_pool.submit(send_reply, message, "âŒ å¤©ç¿¼äº‘Cookieå¤±æ•ˆ")
            return

    # å‘é€åˆå§‹æ¶ˆæ¯å¹¶ä¿å­˜å¯¹è±¡ï¼Œåç»­ç”¨äºç¼–è¾‘
    status_msg = bot.reply_to(message, "â™»ï¸ æ­£åœ¨æ‰«æå¤©ç¿¼äº‘ç›˜æºç›®å½•...")

    # --- 2. è·å–æºæ–‡ä»¶ ---
    try:
        files_189 = client189.get_folder_files_for_transfer(target_189_pid)
    except Exception as e:
        bot.edit_message_text(f"âŒ æ‰«æå‡ºé”™: {str(e)}", chat_id=status_msg.chat.id, message_id=status_msg.message_id)
        return

    if not files_189:
        bot.edit_message_text("ğŸ“‚ å¤©ç¿¼äº‘æºç›®å½•ä¸ºç©º", chat_id=status_msg.chat.id, message_id=status_msg.message_id)
        return

    total_files = len(files_189)
    bot.edit_message_text(f"ğŸ” æ‰«æåˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡å¯åŠ¨ 5 çº¿ç¨‹å¹¶å‘ç§’ä¼ ...", chat_id=status_msg.chat.id, message_id=status_msg.message_id)

    # --- 3. åˆå§‹åŒ– 123 å®¢æˆ·ç«¯ & å‡†å¤‡å·¥ä½œ ---
    client123 = init_123_client()
    
    success_count = 0
    fail_count = 0
    processed_count = 0
    delete_list = []
    folder_cache = {} 
    
    # è¿›åº¦æ§åˆ¶
    last_update_time = 0
    start_time = time.time()

    # --- 4. å¤šçº¿ç¨‹æ‰§è¡ŒåŒæ­¥ ---
    # max_workers=5 æ¨èå€¼ï¼Œè¿‡é«˜å¯èƒ½å¯¼è‡´123æ¥å£é™æµ
    with ThreadPoolExecutor(max_workers=5) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [
            executor.submit(sync_file_worker, client123, f, root_123_pid, folder_cache) 
            for f in files_189
        ]
        
        # å¤„ç†ç»“æœ (as_completed ä¼šåœ¨ä»»åŠ¡å®Œæˆæ—¶ç«‹å³ yield)
        for future in concurrent.futures.as_completed(futures):
            processed_count += 1
            res = future.result()
            
            if res['status'] == 'success':
                success_count += 1
                delete_list.append(res['file_id'])
                logger.info(f"âœ… ç§’ä¼ æˆåŠŸ: {res['name']}")
            else:
                fail_count += 1
                logger.warning(f"âŒ ç§’ä¼ å¤±è´¥: {res['name']} ({res.get('msg', 'unknown')})")

            # --- æ™ºèƒ½è¿›åº¦åé¦ˆ (æ¯2ç§’æ›´æ–°ä¸€æ¬¡æ¶ˆæ¯) ---
            current_time = time.time()
            if current_time - last_update_time > 2 or processed_count == total_files:
                last_update_time = current_time
                
                # è®¡ç®—é€Ÿåº¦å’Œå‰©ä½™æ—¶é—´
                elapsed = current_time - start_time
                speed = processed_count / elapsed if elapsed > 0 else 0
                eta = (total_files - processed_count) / speed if speed > 0 else 0
                
                # ç”Ÿæˆè¿›åº¦æ¡
                progress_bar = get_progress_bar(processed_count, total_files)
                
                msg_text = (
                    f"ğŸš€ **åŒæ­¥è¿›è¡Œä¸­...**\n\n"
                    f"{progress_bar}\n"
                    f"ğŸ”¢ è¿›åº¦: {processed_count}/{total_files}\n"
                    f"âœ… æˆåŠŸ: {success_count}  âŒ å¤±è´¥: {fail_count}\n"
                    f"âš¡ é€Ÿåº¦: {speed:.1f} æ–‡ä»¶/ç§’\n"
                    f"â³ å‰©ä½™: {int(eta)} ç§’"
                )
                
                try:
                    bot.edit_message_text(msg_text, chat_id=status_msg.chat.id, message_id=status_msg.message_id, parse_mode='Markdown')
                except Exception:
                    pass # å¿½ç•¥ç¼–è¾‘æ¶ˆæ¯å¯èƒ½å‡ºç°çš„ç½‘ç»œé”™è¯¯

    # --- 5. æ¸…ç†å¤©ç¿¼äº‘æºæ–‡ä»¶ ---
    deleted_files_count = 0
    cleaned_folders_count = 0
    
    if delete_list:
        bot.edit_message_text(f"ğŸ—‘ï¸ ç§’ä¼ å®Œæˆï¼Œæ­£åœ¨åˆ é™¤ {len(delete_list)} ä¸ªæºæ–‡ä»¶...", chat_id=status_msg.chat.id, message_id=status_msg.message_id)
        
        # æ‰¹é‡åˆ é™¤ï¼ˆä¾ç„¶å•çº¿ç¨‹åˆ†æ‰¹å¤„ç†ï¼Œåˆ é™¤æ“ä½œé€šå¸¸å¾ˆå¿«ä¸”å¹¶å‘å®¹æ˜“è§¦å‘é£æ§ï¼‰
        batch_size = 50
        for i in range(0, len(delete_list), batch_size):
            batch_ids = delete_list[i:i + batch_size]
            task_infos = [{"fileId": fid, "fileName": "del", "isFolder": 0} for fid in batch_ids]
            try:
                res = client189.delete_files(task_infos)
                if res.get("success"):
                    deleted_files_count += len(batch_ids)
            except Exception as e:
                logger.error(f"åˆ é™¤æ–‡ä»¶å¼‚å¸¸: {e}")
            time.sleep(1)
        
        # æ¸…ç†ç©ºæ–‡ä»¶å¤¹
        bot.edit_message_text("ğŸ§¹ æ­£åœ¨æ¸…ç†å¤©ç¿¼äº‘æ®‹ç•™çš„ç©ºæ–‡ä»¶å¤¹...", chat_id=status_msg.chat.id, message_id=status_msg.message_id)
        try:
            cleaned_folders_count = client189.delete_empty_folders(target_189_pid)
        except Exception as e:
            logger.error(f"æ¸…ç†ç©ºæ–‡ä»¶å¤¹å¤±è´¥: {e}")

    # --- 6. æœ€ç»ˆæˆ˜æŠ¥ ---
    total_time = int(time.time() - start_time)
    result_msg = (
        f"ğŸ **189âš¡123 åŒæ­¥ä»»åŠ¡ç»“æŸ**\n\n"
        f"â±ï¸ è€—æ—¶: {total_time} ç§’\n"
        f"ğŸ“‚ æ€»æ–‡ä»¶: {total_files}\n"
        f"âœ… ç§’ä¼ æˆåŠŸ: {success_count}\n"
        f"âŒ ç§’ä¼ å¤±è´¥: {fail_count}\n"
        f"ğŸ—‘ï¸ åˆ é™¤æºæ–‡ä»¶: {deleted_files_count}\n"
        f"ğŸ§¹ æ¸…ç†ç©ºç›®å½•: {cleaned_folders_count}"
    )
    
    # åˆ é™¤ä¹‹å‰çš„è¿›åº¦æ¶ˆæ¯ï¼Œå‘é€æœ€ç»ˆæˆ˜æŠ¥
    try:
        bot.delete_message(chat_id=status_msg.chat.id, message_id=status_msg.message_id)
    except:
        pass
    reply_thread_pool.submit(send_reply, message, result_msg)

# [æ–°å¢] æ³¨å†Œ /sync189 å‘½ä»¤
@bot.message_handler(commands=['sync189'])
def handle_sync_189_command(message):
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "ğŸš« æ‚¨æ²¡æœ‰æƒé™æ‰§è¡Œæ­¤æ“ä½œ")
        return
    
    reply_thread_pool.submit(send_reply, message, "â³ æ”¶åˆ°åŒæ­¥æŒ‡ä»¤ï¼Œæ­£åœ¨åå°å¯åŠ¨å¤„ç†è¿›ç¨‹...")
    
    # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œï¼Œé˜²æ­¢é˜»å¡Botä¸»è¿›ç¨‹
    threading.Thread(target=process_189_to_123_sync, args=(message,)).start()

# ================= [ç»“æŸ] æ–°å¢ sync189 é€»è¾‘ =================

from quark_export_share import export_share_info
from share import TMDBHelper
tmdb = TMDBHelper()
# åˆ›å»ºé”å¯¹è±¡ç¡®ä¿æ–‡ä»¶ä¾æ¬¡è½¬å­˜
link_process_lock = threading.Lock()
@bot.message_handler(content_types=['text', 'photo'])
def handle_general_message(message):
    logger.info("è¿›å…¥handle_general_message")
    user_id = message.from_user.id
    if user_id != TG_ADMIN_USER_ID:
        reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤æœºå™¨äººã€‚")
        return
    
    # [æ–°å¢] å…³é”®ä¿®æ”¹ï¼šå¿½ç•¥ä»¥ '-' å¼€å¤´çš„ Userbot å‘½ä»¤
    # é˜²æ­¢ Bot è¯•å›¾è§£æ '-s123' ç­‰å‘½ä»¤å¤±è´¥åå›å¤æŠ¥é”™ï¼Œä¸”å›  Userbot åˆ é™¤äº†æ¶ˆæ¯å¯¼è‡´æ­»å¾ªç¯
    if message.content_type == 'text' and message.text and message.text.startswith('-'):
        logger.info(f"æ£€æµ‹åˆ° Userbot å‘½ä»¤ '{message.text}'ï¼ŒBot ä¸»åŠ¨è·³è¿‡")
        return
    
    with link_process_lock:
        text = f"{message}"
        client = init_123_client()             
        # æ‰§è¡ŒåŒ¹é…
        full_links = extract_123_links_from_full_text(text)
        if full_links:
            for link in full_links:
                parse_share_link(message, link)
            user_state_manager.clear_state(user_id)
            return
        # è°ƒç”¨å‡½æ•°å¹¶è·å–è¿”å›å€¼
        result = add_magnet_links(client,text,get_int_env("ENV_123_MAGNET_UPLOAD_PID", 0),message)

        # æ ¹æ®è¿”å›å€¼çŠ¶æ€æ‰§è¡Œä¸åŒçš„print
        if result['status'] == 'success':
            success_count = 0
            fail_count = 0
            fail_messages = []
            
            # æ£€æŸ¥æ¯ä¸ªé“¾æ¥çš„æ·»åŠ ç»“æœ
            for item in result['data']:
                link = item['link']
                response = item['response']
                if isinstance(response, dict) and response.get('code') == 0:
                    success_count += 1
                else:
                    fail_count += 1
                    # æˆªå–é“¾æ¥çš„å‰40ä¸ªå­—ç¬¦ä½œä¸ºæ ‡è¯†
                    link_identifier = link
                    msg = f"\n{link_identifier}: {response.get('message', 'æœªçŸ¥é”™è¯¯')}" if isinstance(response, dict) else f"{link_identifier}: {str(response)}"
                    fail_messages.append(msg)
            
            # æ‰“å°ç»“æœ
            logger.info(f"123ç£åŠ›é“¾æ¥æ·»åŠ ç»“æœ: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{fail_count}ä¸ª")
            if fail_count > 0:
                logger.error(f"å¤±è´¥è¯¦æƒ…:{', '.join(fail_messages)}")
                reply_thread_pool.submit(send_reply, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}ä¸ª, å¤±è´¥{fail_count}ä¸ª\nå¤±è´¥è¯¦æƒ…: {', '.join(fail_messages)}")
            else:
                reply_thread_pool.submit(send_reply, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ æˆåŠŸ: å…±æ·»åŠ äº†{success_count}ä¸ªé“¾æ¥")
            user_state_manager.clear_state(user_id)
            return
        else:
            if result['message'] == 'æœªæ‰¾åˆ°ç£åŠ›é“¾æ¥':
                #logger.info("æœªæ‰¾åˆ°ä»»ä½•ç£åŠ›é“¾æ¥")
                None
            else:
                logger.error(f"123ç£åŠ›é“¾æ¥æ·»åŠ å¤±è´¥: {result['message']}")
                reply_thread_pool.submit(send_reply_delete, message, f"123ç£åŠ›é“¾æ¥æ·»åŠ å¤±è´¥: {result['message']}")
                user_state_manager.clear_state(user_id)
                return
        if "æå–ç " in text and "www.123" in text:
            reply_thread_pool.submit(send_reply, message, f"ä»…æ”¯æŒå½¢å¦‚ https://www.123pan.com/s/abcde-fghi?pwd=ABCD çš„æå–ç æ ¼å¼")
            return
        target_urls = extract_target_url(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ª123åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è½¬å­˜...")
            success_count = 0
            fail_count = 0
            for url in target_urls:
                try:
                    result = transfer_shared_link_optimize(client, url, UPLOAD_LINK_TARGET_PID)
                    if result:
                        success_count += 1
                        logger.info(f"è½¬å­˜æˆåŠŸ: {url}")
                    else:
                        fail_count += 1
                        logger.error(f"è½¬å­˜å¤±è´¥: {url}")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
                    
            #time.sleep(3)
            reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return
        
        target_urls = extract_kuake_target_url(text)
        if target_urls:
            if not os.getenv("ENV_KUAKE_COOKIE", ""):
                logger.error(f"è¯·å¡«å†™å¤¸å…‹COOKIE")
                reply_thread_pool.submit(send_reply, message, f"è¯·å¡«å†™å¤¸å…‹COOKIE")
                return
            reply_thread_pool.submit(send_reply, message, f"å‘ç°{len(target_urls)}ä¸ªå¤¸å…‹åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹å°è¯•ç§’ä¼ åˆ°123...")
            success_count = 0   
            fail_count = 0
            for url in target_urls:
                try:
                    json_data = export_share_info(url,os.getenv("ENV_KUAKE_COOKIE", ""))
                    if json_data:
                        save_json_file_quark(message,json_data)
                        #parse_share_link(message, kuake_link, get_int_env("ENV_123_KUAKE_UPLOAD_PID", 0))                
                    else:
                        logger.error(f"å¤¸å…‹åˆ†äº«è½¬å­˜123å‡ºé”™")
                        reply_thread_pool.submit(send_reply, message, f"å¤¸å…‹åˆ†äº«è½¬å­˜123å‡ºé”™")
                except Exception as e:
                    fail_count += 1
                    logger.error(f"è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            #time.sleep(3)
            #reply_thread_pool.submit(send_reply, message, f"è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ªï¼Œå¤±è´¥{fail_count}ä¸ª")
            user_state_manager.clear_state(user_id)
            return

        # ... å¤©ç¿¼äº‘ç›˜éƒ¨åˆ† ...       
        from bot189 import save_189_link    
        from bot189 import extract_target_url as extract_target_url_189
        from bot189 import save_189_link, get_share_file_snapshot
        
        target_urls = extract_target_url_189(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ªå¤©ç¿¼äº‘ç›˜åˆ†äº«é“¾æ¥ï¼Œæ­£åœ¨å¤„ç†...")
            
            success_count = 0
            fail_count = 0
            
            client123 = init_123_client()
            
            # 1. 123äº‘ç›˜ç›®æ ‡åŸºç¡€ID (ç§’ä¼ ä½ç½®)
            pid_for_123 = os.getenv("ENV_189GO123_UPLOAD_PID", "")
            if not pid_for_123:
                pid_for_123 = os.getenv("ENV_123_UPLOAD_PID", "0")
            
            # 2. å¤©ç¿¼äº‘ç›®æ ‡ID (å…œåº•è½¬å­˜)
            pid_for_189 = os.getenv("ENV_189_LINK_UPLOAD_PID", "")
            if not pid_for_189:
                pid_for_189 = os.getenv("ENV_189_UPLOAD_PID", "-11")

            logger.info(f"189é…ç½® | 123åŸºç¡€ID: {pid_for_123} | 189å…œåº•ID: {pid_for_189}")

            for url in target_urls:
                try:
                    logger.info(f"æ­£åœ¨è§£æå¤©ç¿¼äº‘é“¾æ¥å…ƒæ•°æ®: {url}")
                    # è·å–æ–‡ä»¶å¿«ç…§ + åˆ†äº«æ ‡é¢˜(ä½œä¸ºæ ¹æ–‡ä»¶å¤¹å)
                    files_in_share, root_share_name = get_share_file_snapshot(client189, url)
                    
                    all_rapid_success = False
                    
                    if files_in_share:
                        total_f = len(files_in_share)
                        success_f = 0
                        logger.info(f"è§£ææˆåŠŸï¼Œå…± {total_f} ä¸ªæ–‡ä»¶ï¼Œå‡†å¤‡ç§’ä¼ ...")
                        
                        # [å…³é”®] æ–‡ä»¶å¤¹IDå…¨å±€ç¼“å­˜ (é¿å…åŒä¸€å±‚çº§é‡å¤è¯·æ±‚API)
                        # Key: "çˆ¶ID_æ–‡ä»¶å¤¹å", Value: "æ–‡ä»¶å¤¹ID"
                        # æ”¾åœ¨å¾ªç¯å¤–ï¼Œç¡®ä¿åŒä¸€ä¸ªåˆ†äº«é“¾æ¥å†…ç¼“å­˜å…±äº«
                        folder_cache = {} 
                        
                        for i, f_info in enumerate(files_in_share):
                            try:
                                # === [æ ¸å¿ƒé€»è¾‘] æ„å»ºå®Œæ•´ç›®å½•é“¾ ===
                                raw_path = f_info.get('path', '').strip('/')
                                path_parts = raw_path.split('/')
                                
                                # 2. æå–æ–‡ä»¶å: "007.mp4"
                                file_name = path_parts.pop() 
                                
                                # 3. æ„å»ºç›®å½•åˆ—è¡¨: ["æˆ‘çš„èµ„æº", "åŠ¨ä½œç‰‡", "007ç³»åˆ—"]
                                # å°† "åˆ†äº«æ ‡é¢˜" ä½œä¸ºç¬¬ä¸€å±‚ï¼Œå‰©ä¸‹çš„ path_parts ä½œä¸ºåç»­å±‚çº§
                                dir_chain = []
                                if root_share_name:
                                    dir_chain.append(root_share_name)
                                dir_chain.extend([p for p in path_parts if p]) # è¿½åŠ å‰©ä½™è·¯å¾„
                                
                                # 4. é€çº§é€’å½’åˆ›å»º/æŸ¥æ‰¾ç›®å½•
                                current_pid = pid_for_123 # ä»é…ç½®çš„æ ¹ç›®å½•å¼€å§‹
                                
                                for folder_name in dir_chain:
                                    # ç”Ÿæˆç¼“å­˜Key (ç¡®ä¿çˆ¶IDå’Œæ–‡ä»¶å¤¹åå”¯ä¸€ç¡®å®šä¸€ä¸ªå­æ–‡ä»¶å¤¹)
                                    cache_key = f"{current_pid}_{folder_name}"
                                    
                                    # A. æŸ¥æœ¬åœ°ç¼“å­˜ (é€Ÿåº¦æœ€å¿«ï¼Œæ”¯æŒåµŒå¥—çš„å…³é”®)
                                    if cache_key in folder_cache:
                                        current_pid = folder_cache[cache_key]
                                        continue
                                    
                                    # B. æŸ¥äº‘ç«¯ / åˆ›å»º
                                    found_id = find_child_folder_id(client123, current_pid, folder_name)
                                    if found_id:
                                        # å­˜åœ¨ -> è®°å½•ç¼“å­˜ï¼Œè¿›å…¥ä¸‹ä¸€çº§
                                        folder_cache[cache_key] = found_id
                                        current_pid = found_id
                                    else:
                                        # ä¸å­˜åœ¨ -> åˆ›å»º
                                        try:
                                            resp = client123.fs_mkdir(folder_name, parent_id=current_pid)
                                            if resp.get("code") == 0:
                                                new_id = resp["data"]["Info"]["FileId"]
                                                folder_cache[cache_key] = new_id
                                                current_pid = new_id
                                                logger.info(f"ğŸ“ åˆ›å»ºç›®å½•: {folder_name} (ID: {new_id})")
                                            else:
                                                logger.warning(f"âš ï¸ åˆ›å»ºç›®å½•å¤±è´¥: {folder_name} - {resp.get('message')}")
                                        except Exception:
                                            pass

                                # === 5. æ‰§è¡Œç§’ä¼  (åˆ°æœ€åä¸€çº§ç›®å½•) ===
                                resp = client123.upload_file_fast(
                                    file_name=file_name,
                                    parent_id=current_pid, 
                                    file_md5=f_info['md5'],
                                    file_size=f_info['size'],
                                    duplicate=1
                                )
                                
                                if resp.get("code") == 0 and \
                                   (resp.get("data", {}).get("Reuse") or resp.get("data", {}).get("reuse")):
                                    success_f += 1
                                    
                            except Exception as e:
                                logger.error(f"âŒ å•æ–‡ä»¶å¤„ç†å¼‚å¸¸ {f_info.get('name')}: {e}")
                                pass 
                        
                        logger.info(f"123ç›´è¿ç§’ä¼ ç»“æœ: {success_f}/{total_f}")
                        
                        if success_f == total_f and total_f > 0:
                            all_rapid_success = True
                            success_count += 1
                            reply_thread_pool.submit(send_reply, message, f"âœ… 123äº‘ç›˜æé€Ÿç§’ä¼ æˆåŠŸï¼\nğŸ“ ç›®å½•: {root_share_name}\né“¾æ¥: {url}\nâœ¨ å®Œç¾ä¿ç•™å¤šå±‚çº§ç›®å½•ç»“æ„")
                            continue 
                    
                    # 2. ç§’ä¼ å¤±è´¥ï¼Œèµ°å…œåº•è½¬å­˜ (ä¿å­˜åˆ° 189)
                    if not all_rapid_success:
                        logger.info("123ç§’ä¼ æœªå®Œå…¨è¦†ç›–ï¼Œæ‰§è¡Œè½¬å­˜åˆ°å¤©ç¿¼äº‘ç›˜...")
                        if files_in_share:
                            reply_thread_pool.submit(send_reply_delete, message, f"âš ï¸ 123äº‘ç›˜æ— æ­¤èµ„æºï¼Œæ­£åœ¨è½¬å­˜åˆ°å¤©ç¿¼äº‘ç›˜ (å ç”¨ç©ºé—´)...")
                        
                        result = save_189_link(client189, url, pid_for_189)
                        
                        if result:
                            success_count += 1
                            logger.info(f"å¤©ç¿¼äº‘è½¬å­˜æˆåŠŸ: {url}")
                            reply_thread_pool.submit(send_reply, message, f"âœ… å·²è½¬å­˜åˆ°å¤©ç¿¼äº‘ç›˜ (123ç§’ä¼ æœªå®Œå…¨è¦†ç›–)\né“¾æ¥: {url}\nè¯·ç¨åä½¿ç”¨ /sync189 è¿›è¡ŒåŒæ­¥ã€‚")
                        else:
                            fail_count += 1
                            logger.error(f"å¤©ç¿¼äº‘è½¬å­˜å¤±è´¥: {url}")
                            reply_thread_pool.submit(send_reply, message, f"âŒ è½¬å­˜å¤±è´¥: {url}")

                except Exception as e:
                    fail_count += 1
                    logger.error(f"å¤„ç†å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            
            user_state_manager.clear_state(user_id)
            return

        # ... 115éƒ¨åˆ† ...
        from bot115 import extract_target_url as  extract_target_url_115
        from bot115 import transfer_shared_link as  transfer_shared_link_115
        from bot115 import init_115_client

        target_urls = extract_target_url_115(text)
        if target_urls:
            reply_thread_pool.submit(send_reply_delete, message, f"å‘ç°{len(target_urls)}ä¸ª115åˆ†äº«é“¾æ¥ï¼Œå¼€å§‹è½¬å­˜...")
            client = init_115_client()
            
            success_count = 0
            fail_count = 0
            skipped_count = 0  # åˆå§‹åŒ–è®¡æ•°å™¨
            
            for url in target_urls:
                try:
                    if not url: continue
                    
                    # ç¡®å®š PID
                    target_pid = os.getenv("ENV_115_LINK_UPLOAD_PID", "0")
                    if url.startswith("ed2k://") or url.startswith("magnet:?"):
                        target_pid = os.getenv("ENV_115_OFFLINE_PID", target_pid)

                    # æ‰§è¡Œè½¬å­˜
                    result = transfer_shared_link_115(client, url, target_pid)
                    
                    # 1. åˆ¤æ–­æ˜¯å¦æˆåŠŸ (åˆ©ç”¨ __bool__)
                    if result:
                        success_count += 1
                        logger.info(f"âœ… 115ç½‘ç›˜è½¬å­˜æˆåŠŸ: {url}")
                        
                        # 2. åˆ¤æ–­æ˜¯å¦è·³è¿‡ (åˆ©ç”¨æ–°å±æ€§ skipped)
                        # getattr æ˜¯ä¸ºäº†é˜²æ­¢ bot115 æœªæ›´æ–°å¯¼è‡´æŠ¥é”™çš„é˜²å¾¡æ€§å†™æ³•
                        if getattr(result, 'skipped', False):
                            skipped_count += 1
                    else:
                        fail_count += 1
                        logger.error(f"115ç½‘ç›˜è½¬å­˜å¤±è´¥: {url}")
                        
                except Exception as e:
                    fail_count += 1
                    logger.error(f"115ç½‘ç›˜è½¬å­˜å¼‚å¸¸: {url}, é”™è¯¯: {str(e)}")
            
            # æ„å»ºç›´è§‚çš„å›å¤
            reply_msg = f"âœ… 115ç½‘ç›˜è½¬å­˜å®Œæˆï¼šæˆåŠŸ{success_count}ä¸ª"
            
            # å¦‚æœæœ‰è·³è¿‡çš„ä»»åŠ¡ï¼Œè¿›è¡Œç‰¹åˆ«æ ‡æ³¨
            if skipped_count > 0:
                reply_msg += f" (å«{skipped_count}ä¸ªä»»åŠ¡å·²å­˜åœ¨)"
            
            reply_msg += f"ï¼Œå¤±è´¥{fail_count}ä¸ª"
            
            reply_thread_pool.submit(send_reply, message, reply_msg)
            user_state_manager.clear_state(user_id)
            return


    state, data = user_state_manager.get_state(user_id)
    if state == "SELECTING_FILE":
        try:
            raw_text = message.text.strip()
            text = raw_text.replace('ã€€', ' ').strip()
            full_width = 'ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™'
            half_width = '0123456789'
            trans_table = str.maketrans(full_width, half_width)
            text = text.translate(trans_table)
            try:
                # æ”¯æŒç©ºæ ¼åˆ†éš”çš„å¤šä¸ªæ•°å­—ï¼Œå¦‚ "1 2 3 5"
                selections = [int(num) - 1 for num in text.split()]
                if not selections:
                    raise ValueError("è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªæœ‰æ•ˆçš„åºå·")
                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„åºå·
                if len(selections) != len(set(selections)):
                    raise ValueError("åºå·ä¸èƒ½é‡å¤")
            except ValueError as e:
                if "invalid literal" in str(e):
                    raise ValueError("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—åºå·ï¼ˆä¾‹å¦‚ï¼š1 2 3 4ï¼‰ï¼Œä¸è¦åŒ…å«å­—æ¯æˆ–ç¬¦å·")
                else:
                    raise e
                
            # [ä¿®æ”¹] å¢å¼ºçš„çŠ¶æ€æ•°æ®è§£æï¼Œæ”¯æŒ mode
            try:
                loaded_data = json.loads(data)
                # åˆ¤æ–­æ˜¯æ—§åˆ—è¡¨æ ¼å¼è¿˜æ˜¯æ–°å­—å…¸æ ¼å¼
                if isinstance(loaded_data, dict) and "results" in loaded_data:
                    results = loaded_data["results"]
                    mode = loaded_data.get("mode", "link")
                elif isinstance(loaded_data, list):
                    results = loaded_data
                    mode = "link" # é»˜è®¤ä¸ºé“¾æ¥æ¨¡å¼
                else:
                    results = []
                    mode = "link"
            except Exception:
                reply_thread_pool.submit(send_reply, message, "æ•°æ®è§£æé”™è¯¯ï¼Œè¯·é‡æ–°æœç´¢")
                return

            if not results:
                reply_thread_pool.submit(send_reply, message, "æœç´¢ç»“æœå·²å¤±æ•ˆï¼Œè¯·é‡æ–°æœç´¢")
                user_state_manager.clear_state(user_id)
                return
            
            # éªŒè¯æ‰€æœ‰é€‰æ‹©æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            for idx in selections:
                if not (0 <= idx < len(results)):
                    raise ValueError(f"åºå· {idx+1} è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡æ–°è¾“å…¥")
            
            # åˆå§‹åŒ–å®¢æˆ·ç«¯ï¼ˆåªéœ€åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
            client = init_123_client()
            
            # éå†æ‰€æœ‰é€‰æ‹©çš„æ–‡ä»¶å¤¹
            for selection in selections:
                selected_item = results[selection]
                file_id = selected_item['id']
                folder_name = selected_item['name']
                logger.info(f"é€‰ä¸­æ–‡ä»¶å¤¹ID: {file_id}, åç§°: {folder_name}, æ¨¡å¼: {mode}")
                
                # åªä¸ºç¬¬ä¸€ä¸ªæ–‡ä»¶å¤¹å‘é€å¤„ç†æ¶ˆæ¯
                if selection == selections[0]:
                    op_text = "ç”ŸæˆJSONæ–‡ä»¶" if mode == "json" else "åˆ›å»ºåˆ†äº«é“¾æ¥"
                    reply_thread_pool.submit(send_reply, message, f"æ­£åœ¨ä¸º {len(selections)} ä¸ªæ–‡ä»¶å¤¹{op_text}...")

                # ==========================
                # åˆ†æ”¯ 1: åˆ†äº«é“¾æ¥æ¨¡å¼ (link)
                # ==========================
                if mode == "link":
                    if get_int_env("ENV_MAKE_NEW_LINK", 1):
                        existing_share = get_existing_shares(client, folder_name)
                    else:
                        existing_share = None
                        
                    if existing_share:
                        # å°è¯•è·å–TMDBå…ƒæ•°æ®
                        file_name=get_first_video_file(client, file_id)
                        metadata = tmdb.get_metadata_optimize(folder_name, file_name)
                        share_data = {
                            "share_url": f"{existing_share['url']}{'?pwd=' + existing_share['password'] if existing_share['password'] else ''}",
                            "folder_name": folder_name,
                            "file_id": file_id  # é€‰ä¸­çš„æ–‡ä»¶å¤¹IDï¼Œç”¨äºåç»­æŸ¥è¯¢æ–‡ä»¶
                        }

                        if not metadata:
                            logger.warning(f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®: {folder_name}/{file_name}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®ï¼Œä¸äºˆåˆ†äº«ï¼Œè¯·è§„èŒƒæ–‡ä»¶å¤¹å: {folder_name}/{file_name}")
                            # æ³¨æ„ï¼šå¦‚æœæ˜¯å¤šé€‰ï¼Œè¿™é‡Œ continue è·³è¿‡å½“å‰ï¼Œä¸æ¸…é™¤çŠ¶æ€
                            continue 

                        # ä»…å½“metadataå­˜åœ¨ä¸”titleåœ¨folder_nameä¸­æ—¶æ‰æ‰§è¡Œ
                        if metadata:
                            # ä½¿ç”¨å°è£…å‡½æ•°æ„å»ºæ¶ˆæ¯
                            share_message, share_message2, poster_url, files = build_share_message(metadata, client, file_id, folder_name, file_name, existing_share)

                            # å‘é€å›¾ç‰‡å’Œæ¶ˆæ¯
                            try:
                                bot.send_photo(message.chat.id, poster_url, caption=share_message, parse_mode='HTML')
                                if TOKENSHARE:
                                    botshare.send_photo(TARGET_CHAT_ID_SHARE, poster_url, caption=share_message, parse_mode='HTML')
                            except Exception as e:
                                logger.error(f"å‘é€å›¾ç‰‡å¤±è´¥: {str(e)}")
                                reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                        else:
                            # æ— å…ƒæ•°æ®çš„å¤‡ç”¨æ˜¾ç¤º
                            files = get_directory_files(client, file_id, folder_name)
                            share_message = f"âœ… å·²å­˜åœ¨åˆ†äº«é“¾æ¥ï¼š\n{folder_name}\n"
                            share_message += f"é“¾æ¥ï¼š{existing_share['url']}{'?pwd=' + existing_share['password'] if existing_share['password'] else ''}\n"
                            if existing_share['password']:
                                share_message += f"æå–ç ï¼š{existing_share['password']}\n"
                            share_message += f"è¿‡æœŸæ—¶é—´ï¼š{existing_share['expiry']}"
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)

                        # [ç§»é™¤] è¿™é‡Œç§»é™¤äº† AUTO_MAKE_JSON çš„é€»è¾‘ï¼Œå› ä¸ºç°åœ¨ç”± json æ¨¡å¼æ¥ç®¡

                        # å‘å¸–è¯¢é—®é€»è¾‘
                        if os.getenv("ENV_123PANFX_COOKIE","") and len(selections)==1:
                            user_state_manager.set_state(user_id, "ASK_POST", json.dumps(share_data))
                            ask_msg = "æ˜¯å¦éœ€è¦å°†è¯¥å†…å®¹å‘å¸ƒåˆ°è®ºå›ï¼Ÿ\n1. æ”¾å¼ƒå‘å¸–\n2. å‘é€åˆ°ç”µå½±æ¿å—\n3. å‘é€åˆ°ç”µè§†å‰§æ¿å—\n4. å‘é€åˆ°åŠ¨æ¼«æ¿å—"
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, ask_msg)
                            return # å‘å¸–éœ€è¦ç­‰å¾…ä¸‹ä¸€æ­¥ï¼Œç›´æ¥è¿”å›ï¼Œä¸æ¸…é™¤çŠ¶æ€

                    else:
                        # åˆ›å»ºæ–°åˆ†äº«é“¾æ¥
                        file_name = get_first_video_file(client,file_id)
                        metadata = tmdb.get_metadata_optimize(folder_name, file_name)
                        porn_result = None

                        if not metadata:
                            logger.warning(f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®: {folder_name}/{file_name}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"æœªè·å–åˆ°TMDBå…ƒæ•°æ®ï¼Œä¸äºˆåˆ†äº«ï¼Œè¯·è§„èŒƒæ–‡ä»¶å¤¹å: {folder_name}/{file_name}")
                            continue

                        # æ£€æŸ¥å†…å®¹æ˜¯å¦æ¶‰åŠè‰²æƒ…
                        if os.getenv("AI_API_KEY", ""):
                            porn_result = check_porn_content(folder_name+"/"+file_name+"ï¼š"+metadata.get('plot'))
                        else:
                            porn_result = check_porn_content(
                                            content=folder_name+"/"+file_name+"ï¼š"+metadata.get('plot'),
                                            api_url="https://api.edgefn.net",
                                            api_key="sk-Mk6CjIVzoCcg2VnK8c5a85Ef49Ca43F1Ba9b9a13E98f30A9",
                                            model_name="DeepSeek-R1-0528-Qwen3-8B",
                                            max_tokens=15000
                                        )
                        
                        # æ ¹æ®æ£€æµ‹ç»“æœå†³å®šåç»­æ“ä½œ
                        if porn_result and porn_result['is_pornographic']:
                            logger.warning(f"æ£€æµ‹åˆ°è‰²æƒ…å†…å®¹ï¼Œå·²æ‹’ç»åˆ†äº«: {folder_name}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"å½±è§†ä»‹ç»ä¸­æ£€æµ‹åˆ°æ¶‰åŠè‰²æƒ…å†…å®¹ï¼Œæ‹’ç»åˆ†äº«ï¼Œåˆ¤æ–­ä¾æ®ï¼š{porn_result['reason']}")
                            continue
                        
                        # éè‰²æƒ…å†…å®¹ï¼Œç»§ç»­åˆ›å»ºåˆ†äº«é“¾æ¥
                        share_info = create_share_link(client, file_id)
                        share_data = {
                            "share_url": share_info["url"],
                            "folder_name": folder_name,
                            "file_id": file_id  # é€‰ä¸­çš„æ–‡ä»¶å¤¹IDï¼Œç”¨äºåç»­æŸ¥è¯¢æ–‡ä»¶
                        }

                        # ä»…å½“metadataå­˜åœ¨ä¸”titleåœ¨folder_nameä¸­æ—¶æ‰æ‰§è¡Œ
                        if metadata:
                            # ä½¿ç”¨å°è£…å‡½æ•°æ„å»ºæ¶ˆæ¯
                            share_message, share_message2, poster_url, files = build_share_message(metadata, client, file_id, folder_name, file_name, share_info)

                            # å‘é€å›¾ç‰‡å’Œæ¶ˆæ¯
                            try:
                                bot.send_photo(message.chat.id, poster_url, caption=share_message, parse_mode='HTML')
                                if TOKENSHARE:
                                    botshare.send_photo(TARGET_CHAT_ID_SHARE, poster_url, caption=share_message, parse_mode='HTML')
                            except Exception as e:
                                logger.error(f"å‘é€å›¾ç‰‡å¤±è´¥: {str(e)}")
                                reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                        else:
                            files = get_directory_files(client, file_id, folder_name)
                            # ä½¿ç”¨åŸæ¥çš„æ¶ˆæ¯æ ¼å¼
                            share_message = f"âœ… åˆ†äº«é“¾æ¥å·²åˆ›å»ºï¼š\n{folder_name}\n"
                            share_message += f"é“¾æ¥ï¼š{share_info['url']}\n"
                            if share_info['password']:
                                share_message += f"æå–ç ï¼š{share_info['password']}\n"
                            share_message += f"è¿‡æœŸæ—¶é—´ï¼š{share_info['expiry']}"
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, share_message)
                        
                        # [ç§»é™¤] åŒæ ·ç§»é™¤äº† AUTO_MAKE_JSON é€»è¾‘

                        if os.getenv("ENV_123PANFX_COOKIE","") and len(selections)==1:
                            user_state_manager.set_state(user_id, "ASK_POST", json.dumps(share_data))
                            ask_msg = "æ˜¯å¦éœ€è¦å°†è¯¥å†…å®¹å‘å¸ƒåˆ°è®ºå›ï¼Ÿ\n1. æ”¾å¼ƒå‘å¸–\n2. å‘é€åˆ°ç”µå½±æ¿å—\n3. å‘é€åˆ°ç”µè§†å‰§æ¿å—\n4. å‘é€åˆ°åŠ¨æ¼«æ¿å—"
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, ask_msg)
                            return

                # ==========================
                # åˆ†æ”¯ 2: JSON æ–‡ä»¶æ¨¡å¼ (json)
                # ==========================
                elif mode == "json":
                    try:
                        # è·å–æ–‡ä»¶å¤¹å†…æ–‡ä»¶åˆ—è¡¨
                        files = get_directory_files(client, file_id, folder_name)
                        if not files:
                            logger.warning(f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                            reply_thread_pool.submit(send_message_with_id, message.chat.id, f"æ–‡ä»¶å¤¹ä¸ºç©º: {folder_name}")
                            continue

                        # è®¡ç®—æ€»æ–‡ä»¶æ•°å’Œæ€»ä½“ç§¯
                        total_files_count = len(files)
                        total_size = sum(file_info["size"] for file_info in files)
                        
                        # [ä¿®æ”¹] åˆ›å»ºç¬¦åˆè§„èŒƒçš„JSONç»“æ„
                        json_data = {
                            "usesBase62EtagsInExport": False,
                            "etagEncrypted": False,
                            "commonPath": f"{folder_name}/",
                            "totalFilesCount": total_files_count,
                            "totalSize": total_size,
                            "formattedTotalSize": get_formatted_size(total_size), # ä½¿ç”¨æ–°è¾…åŠ©å‡½æ•°
                            "files": [
                                {
                                    "path": file_info["path"],
                                    "etag": file_info["etag"],
                                    "size": file_info["size"]
                                }
                                for file_info in files
                            ]
                        }
                        
                        # ä¿å­˜JSONæ–‡ä»¶
                        json_file_path = f"{folder_name}.json"
                        with open(json_file_path, 'w', encoding='utf-8') as f:
                            json.dump(json_data, f, ensure_ascii=False, indent=2)
                        
                        # è®¡ç®—æ˜¾ç¤ºä½“ç§¯
                        size_str = get_formatted_size(total_size)
                        
                        with open(json_file_path, 'rb') as f:
                            # è®¡ç®—å¹³å‡æ–‡ä»¶å¤§å°
                            avg_size = total_size / total_files_count if total_files_count > 0 else 0
                            avg_size_str = get_formatted_size(avg_size)
                            
                            # å‘é€æ–‡ä»¶
                            bot.send_document(
                                message.chat.id, 
                                f, 
                                caption=f"ğŸ“ {folder_name}\nğŸ“æ–‡ä»¶æ•°: {total_files_count}ä¸ª\nğŸ“¦æ€»ä½“ç§¯: {size_str}\nğŸ“Šå¹³å‡æ–‡ä»¶å¤§å°: {avg_size_str}"
                            )
                        
                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                        os.remove(json_file_path)
                    
                    except Exception as e:
                        logger.error(f"ç”Ÿæˆæˆ–å‘é€JSONæ–‡ä»¶å¤±è´¥: {str(e)}")
                        reply_thread_pool.submit(send_message_with_id, message.chat.id, f"ç”Ÿæˆæ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œè¯·é‡è¯•")

            # å¤„ç†å®Œæˆåï¼Œæ¸…é™¤çŠ¶æ€ (ASK_POST çŠ¶æ€å·²åœ¨ä¸Šé¢ returnï¼Œä¸ä¼šèµ°åˆ°è¿™é‡Œ)
            user_state_manager.clear_state(user_id)

        except ValueError as e:
            reply_thread_pool.submit(send_reply, message, str(e))
        except Exception as e:
            reply_thread_pool.submit(send_reply, message, f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: è¯·æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºï¼Œ{str(e)}")
            logger.error(f"åˆ›å»ºåˆ†äº«é“¾æ¥å¤±è´¥: {str(e)}")
    
    elif state == "ASK_POST":
        try:
            selection = message.text.strip()
            if selection not in ["1", "2", "3", "4"]:
                raise ValueError("è¯·è¾“å…¥1ã€2ã€3æˆ–4é€‰æ‹©æ“ä½œ")
            #global json
            # è§£æä¿å­˜çš„åˆ†äº«æ•°æ®
            share_data = json.loads(data)
            share_url = share_data["share_url"]
            folder_name = share_data["folder_name"]
            file_id = share_data["file_id"]

            if selection == "1":
                # æ”¾å¼ƒå‘å¸–
                reply_thread_pool.submit(send_reply, message, "å·²å–æ¶ˆå‘å¸–")
                user_state_manager.clear_state(user_id)
            else:
                # ç¡®å®šåª’ä½“ç±»å‹ï¼ˆ2=ç”µå½±ï¼Œ3=ç”µè§†å‰§ï¼‰
                # æ ¹æ®é€‰æ‹©ç¡®å®šåª’ä½“ç±»å‹ï¼š2->ç”µå½±ï¼Œ3->åŠ¨ç”»ï¼Œå…¶ä»–->ç”µè§†å‰§
                if selection == "2":
                    media_type = "movie"  # é€‰æ‹©2ï¼šç”µå½±
                elif selection == "3":
                    media_type = "tv"  # é€‰æ‹©3ï¼šç”µè§†å‰§
                elif selection == "4":
                    media_type = "anime"  # é€‰æ‹©4ï¼šåŠ¨æ¼«
                else:
                    media_type = None  # é€‰æ‹©1ï¼šæ”¾å¼ƒï¼ˆæ— éœ€å¤„ç†ï¼‰

                # è·å–ç¬¬ä¸€ä¸ªè§†é¢‘æ–‡ä»¶åç§°
                reply_thread_pool.submit(send_reply, message, "æ­£åœ¨æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ä»¥ç¡®å®šå½±è§†çš„åˆ†è¾¨ç‡åŠéŸ³é¢‘ç­‰ä¿¡æ¯...")
                client = init_123_client()
                file_name = get_first_video_file(client, file_id)
                if not file_name:
                    reply_thread_pool.submit(send_reply, message, "æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶ï¼Œæ— æ³•å‘å¸–")
                    user_state_manager.clear_state(user_id)
                    return

                # è°ƒç”¨share.pyä¸­çš„post_to_forumå‘å¸ƒ
                from share import post_to_forum
                reply_thread_pool.submit(send_reply, message, "æ­£åœ¨å‘å¸ƒåˆ°è®ºå›...")
                success, forum_url = post_to_forum(
                    share_url=share_url,
                    folder_name=folder_name,
                    file_name=file_name,
                    media_type=media_type
                )

                # åé¦ˆç»“æœ
                if success:
                    reply_thread_pool.submit(send_reply, message, f"å‘å¸–æˆåŠŸï¼\n{folder_name}\nç¤¾åŒºé“¾æ¥ï¼š{forum_url}\n123èµ„æºç¤¾åŒºå› æ‚¨çš„åˆ†äº«è€Œæ›´ç¾å¥½â¤ï¸")
                else:
                    reply_thread_pool.submit(send_reply, message, f"å‘å¸–å¤±è´¥ï¼Œ{forum_url}, è¯·é‡è¯•")
                user_state_manager.clear_state(user_id)

        except ValueError as e:
            reply_thread_pool.submit(send_reply, message, str(e))
        except Exception as e:
            reply_thread_pool.submit(send_reply, message, f"æ“ä½œå¤±è´¥: {str(e)}")
            logger.error(f"å¤„ç†å‘å¸–é€‰æ‹©é”™è¯¯: {e}")
    else:
        reply_thread_pool.submit(send_reply, message, "æœªè¯†åˆ«çš„å‘½ä»¤")


#  å¯åŠ¨äººå½¢ç›‘å¬çº¿ç¨‹ (å‡çº§ï¼š-s123å›å¤æ”¯æŒTMDBå¯Œæ–‡æœ¬+æµ·æŠ¥ç¼©ç•¥å›¾)
# [ä¿®æ”¹] å¯åŠ¨äººå½¢ç›‘å¬çº¿ç¨‹ (å‡çº§ï¼šæ”¯æŒåç¼€è¿‡æ»¤ + TMDBå¯Œæ–‡æœ¬ + æµ·æŠ¥ç¼©ç•¥å›¾)
def start_userbot_listener():
    """
    å¯åŠ¨ Pyrogram Userbot ç›‘å¬äººå½¢å‘½ä»¤ (ä¿®å¤ç‰ˆ V4ï¼šå¢åŠ åç¼€è¿‡æ»¤)
    """
    import traceback
    import time
    import asyncio
    import requests
    import re
    import os
    import json
    from share import get_quality 
    
    USERBOT_STATE_ID = -TG_ADMIN_USER_ID 

    # ==================== å†…éƒ¨ç±»ï¼šTMDBåˆ†æå·¥å…· ====================
    class TVAnalyzer:
        def __init__(self):
            self.api_key = os.getenv("ENV_TMDB_API_KEY", "") or "93513c7928441ee2a23b6ed943aa1023"
            self.base_url = "https://api.themoviedb.org/3"
            self.language = "zh-CN"

        def fetch_tmdb_info_sync(self, keyword, is_tv=True):
            try:
                media_type = "tv" if is_tv else "movie"
                search_url = f"{self.base_url}/search/{media_type}?api_key={self.api_key}&query={keyword}&language={self.language}"
                resp = requests.get(search_url, timeout=10)
                if resp.status_code != 200: return None
                data = resp.json()
                if not data.get('results'): return None
                
                best_match = data['results'][0]
                tmdb_id = best_match['id']
                
                append_to = "credits,external_ids"
                if is_tv: append_to += ",seasons"
                
                detail_url = f"{self.base_url}/{media_type}/{tmdb_id}?api_key={self.api_key}&language={self.language}&append_to_response={append_to}"
                detail_resp = requests.get(detail_url, timeout=10)
                if detail_resp.status_code != 200: return None
                details = detail_resp.json()
                
                credits = details.get('credits', {})
                cast = [c.get('name', '') for c in credits.get('cast', [])[:5]]
                crew = [c.get('name') for c in credits.get('crew', []) if c.get('job') == 'Director']
                genres = [g.get('name') for g in details.get('genres', [])]
                countries = [c.get('name') for c in details.get('production_countries', [])]
                
                if is_tv and not crew:
                    crew = [c.get('name') for c in details.get('created_by', [])]

                date_str = details.get('release_date') or details.get('first_air_date') or '0000'
                year = date_str[:4] if len(date_str) >= 4 else '0000'

                result = {
                    'id': tmdb_id,
                    'title': details.get('title') or details.get('name'),
                    'overview': details.get('overview', 'æš‚æ— ç®€ä»‹'),
                    'year': year,
                    'vote_average': round(details.get('vote_average', 0.0), 1),
                    'poster_path': f"https://image.tmdb.org/t/p/original{details.get('poster_path')}" if details.get('poster_path') else None,
                    'backdrop_path': f"https://image.tmdb.org/t/p/original{details.get('backdrop_path')}" if details.get('backdrop_path') else None,
                    'media_type': 'ğŸ“º ç”µè§†å‰§' if is_tv else 'ğŸ¬ ç”µå½±',
                    'cast': ', '.join(cast),
                    'director': ', '.join(crew),
                    'genres': ', '.join(genres),
                    'countries': ', '.join(countries),
                    'seasons_count': details.get('number_of_seasons', 0),
                    'episodes_count': details.get('number_of_episodes', 0),
                    'seasons': details.get('seasons', [])
                }
                return result
            except Exception as e:
                logger.error(f"TMDB Fetch Error: {e}")
                return None

        def analyze_files(self, local_files, tmdb_info):
            if not tmdb_info or "ğŸ“º" not in tmdb_info.get('media_type', ''): return ""
            local_seasons = {}
            for file_name in local_files:
                name_lower = file_name.lower()
                s_match = re.search(r's(\d+)', name_lower)
                e_match = re.search(r'e(\d+)', name_lower)
                if s_match and e_match:
                    s, e = int(s_match.group(1)), int(e_match.group(1))
                    local_seasons.setdefault(s, set()).add(e)
                else:
                    cn_s = re.search(r'ç¬¬(\d+)å­£', name_lower)
                    cn_e = re.search(r'ç¬¬(\d+)é›†', name_lower)
                    if cn_s and cn_e:
                        s, e = int(cn_s.group(1)), int(cn_e.group(1))
                        local_seasons.setdefault(s, set()).add(e)

            report = []
            tmdb_seasons = {s['season_number']: s['episode_count'] for s in tmdb_info.get('seasons', []) if s['season_number'] > 0}
            missing_seasons = sorted(set(tmdb_seasons.keys()) - set(local_seasons.keys()))
            if missing_seasons:
                report.append(f"âŒ ç¼ºå¤±å­£åº¦: S{', S'.join(map(str, missing_seasons))}")

            for s_num, total_eps in tmdb_seasons.items():
                if s_num in local_seasons:
                    local_eps = local_seasons[s_num]
                    if len(local_eps) < total_eps:
                        expected_eps = set(range(1, total_eps + 1))
                        missing_eps = sorted(expected_eps - local_eps)
                        if missing_eps:
                            formatted_missing = []
                            if len(missing_eps) > 0:
                                start = prev = missing_eps[0]
                                for ep in missing_eps[1:]:
                                    if ep == prev + 1: prev = ep
                                    else:
                                        formatted_missing.append(f"{start}-{prev}" if start != prev else f"{start}")
                                        start = prev = ep
                                formatted_missing.append(f"{start}-{prev}" if start != prev else f"{start}")
                            missing_str = ', '.join(formatted_missing)
                            if len(missing_str) > 20: missing_str = missing_str[:20] + "..."
                            report.append(f"âš ï¸ S{s_num:02d} ç¼º{len(missing_eps)}é›† (E{missing_str})")

            if not report: return "âœ… å‰§é›†å®Œæ•´"
            return "\n".join(report)

    tv_analyzer = TVAnalyzer()
    # ==================== å†…éƒ¨ç±»ç»“æŸ ====================

    logger.info("â³ [äººå½¢æ¨¡å—] ç­‰å¾… 15 ç§’åå¯åŠ¨ï¼Œé¿å¼€ Bot å¯åŠ¨é«˜å³°...")
    time.sleep(15)
        
    try:
        try:
            from pyrogram import Client, filters, enums
        except ImportError:
            logger.error("âŒ [äººå½¢æ¨¡å—] ç¼ºå°‘ä¾èµ–ï¼Œè¯· pip install pyrogram tgcrypto")
            return

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        session_file = "db/default_session.session"
        if not os.path.exists(session_file): return

        api_id = int(os.getenv("ENV_API_ID") or 0)
        api_hash = os.getenv("ENV_API_HASH")
        if not api_id or not api_hash: return

        app = Client("default_session", api_id=api_id, api_hash=api_hash, workdir="db") 

        # ---------------- s123 å‘½ä»¤ ----------------
        @app.on_message(filters.me & filters.command("s123", prefixes="-"))
        async def userbot_s123_handler(client, message):
            try:
                if len(message.command) < 2:
                    await message.edit_text("âŒ è¯·æä¾›å…³é”®è¯,ä¾‹å¦‚: -s123 ç”µå½±åç§°")
                    return
                keyword = message.text.split(maxsplit=1)[1]
                await message.edit_text(f"ğŸ” æ­£åœ¨æœç´¢: {keyword} ...")
                p123 = init_123_client()
                results = await loop.run_in_executor(None, lambda: asyncio.run(search_123_files(p123, keyword)))
                if not results:
                    await message.edit_text(f"âŒ æœªæ‰¾åˆ°å…³äº '{keyword}' çš„æ–‡ä»¶å¤¹")
                    return
                folder_msg = build_folder_message(results)
                
                # è®°å½•æ¶ˆæ¯IDï¼Œç¡®ä¿åç»­èƒ½åˆ é™¤
                state_data = {
                    "results": results, 
                    "mode": "json",
                    "msg_id": message.id,      # æœç´¢ç»“æœæ¶ˆæ¯ID
                    "chat_id": message.chat.id # èŠå¤©ID
                }
                user_state_manager.set_state(USERBOT_STATE_ID, "SELECTING_FILE", json.dumps(state_data))
                
                await message.edit_text(f"âœ… æœç´¢å®Œæˆ\n{folder_msg}")
            except Exception as e:
                logger.error(f"Userbot -s123 error: {e}")
                await message.edit_text(f"âŒ æœç´¢å‡ºé”™: {e}")

        # ---------------- åºå·é€‰æ‹© (ä¿®å¤ï¼šæ¶ˆæ¯å›æº¯åˆ é™¤ + æ–‡ä»¶è¿‡æ»¤) ----------------
        @app.on_message(filters.me & filters.regex(r"^\d+(\s+\d+)*$"))
        async def userbot_selection_handler(client, message):
            raw_text = message.text or message.caption or ""
            if not raw_text: return

            text = message.text.strip()
            
            state, data = user_state_manager.get_state(USERBOT_STATE_ID)
            if state != "SELECTING_FILE":
                message.continue_propagation()
                return 

            # 1. ç«‹å³åˆ é™¤ç”¨æˆ·å›å¤çš„æ•°å­—
            try: await message.delete()
            except: pass
            
            # 2. å¯»æ‰¾è¦åˆ é™¤çš„åˆ—è¡¨æ¶ˆæ¯
            list_msg = None
            if message.reply_to_message:
                list_msg = message.reply_to_message
            else:
                try:
                    loaded_data = json.loads(data)
                    saved_msg_id = loaded_data.get("msg_id")
                    saved_chat_id = loaded_data.get("chat_id")
                    
                    if saved_chat_id == message.chat.id and saved_msg_id:
                        list_msg = await client.get_messages(message.chat.id, saved_msg_id)
                except Exception as e:
                    logger.warning(f"å›æº¯æ¶ˆæ¯å¤±è´¥: {e}")

            try:
                selections = [int(num) - 1 for num in text.split()]
                try:
                    loaded_data = json.loads(data)
                    results = loaded_data["results"] if isinstance(loaded_data, dict) and "results" in loaded_data else loaded_data
                except: return

                if not results: return

                for idx in selections:
                    if not (0 <= idx < len(results)):
                        if list_msg: await list_msg.edit_text(f"âŒ åºå· {idx+1} è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡æ–°æœç´¢")
                        return

                # 3. åˆ é™¤åˆ—è¡¨æ¶ˆæ¯ï¼Œå‘é€æ–°çš„è¿›åº¦æ¶ˆæ¯
                if list_msg:
                    try: await list_msg.delete()
                    except: pass
                
                status_msg = await client.send_message(message.chat.id, f"âš™ï¸ æ­£åœ¨è·å–å…ƒæ•°æ® (ä»»åŠ¡ {len(selections)} ä¸ª)...")
                
                p123 = init_123_client()

                for selection in selections:
                    selected_item = results[selection]
                    file_id = selected_item['id']
                    folder_name = selected_item['name']

                    try:
                        # è·å–å…¨éƒ¨æ–‡ä»¶åˆ—è¡¨
                        files = await loop.run_in_executor(None, get_directory_files, p123, file_id, folder_name)
                        if not files: continue

                        # === [æ–°å¢] è¿™é‡Œæ‰§è¡Œåç¼€è¿‡æ»¤ ===
                        filtered_files = []
                        skipped_num = 0
                        for f in files:
                            # è°ƒç”¨å…¨å±€å®šä¹‰çš„ check_ext_filter
                            if check_ext_filter(f.get("path", "")):
                                skipped_num += 1
                                continue
                            filtered_files.append(f)
                        
                        files = filtered_files
                        
                        if skipped_num > 0:
                            logger.info(f"Userbotç”ŸæˆJSON: å·²è¿‡æ»¤ {skipped_num} ä¸ªæ–‡ä»¶")
                            
                        if not files:
                            await client.send_message(message.chat.id, f"âŒ æ–‡ä»¶å¤¹ {folder_name} å†…æ‰€æœ‰æ–‡ä»¶å‡è¢«è¿‡æ»¤è§„åˆ™å±è”½")
                            continue
                        # ==============================

                        video_exts = {'.mkv', '.mp4', '.avi', '.mov', '.ts', '.rmvb', '.iso', '.wmv', '.m2ts', '.mpg', '.flv', '.rm'}
                        video_files = [f for f in files if os.path.splitext(f["path"])[1].lower() in video_exts]
                        
                        total_size = sum(f["size"] for f in files)
                        if total_size < 1024**3: size_str = f"{total_size / (1024**2):.2f} MB"
                        else: size_str = f"{total_size / (1024**3):.2f} GB"
                        
                        avg_size = total_size / len(video_files) if video_files else 0
                        if avg_size < 1024**3: avg_str = f"{avg_size / (1024**2):.2f} MB"
                        else: avg_str = f"{avg_size / (1024**3):.2f} GB"

                        file_info_text = f"ğŸ¬ è§†é¢‘æ•°é‡: {len(video_files)} | æ€»å¤§å°: {size_str} | å¹³å‡å¤§å°ï¼š{avg_str}"
                        quality = get_quality(video_files[0]["path"]) if video_files else "æœªçŸ¥"

                        clean_keyword = re.split(r'[ .\[\(]', folder_name.split('/')[0])[0]
                        is_tv = bool(video_files and any(re.search(r's\d+|e\d+|ç¬¬\d+é›†', f['path'].lower()) for f in video_files[:5]))
                        tmdb_info = await loop.run_in_executor(None, tv_analyzer.fetch_tmdb_info_sync, clean_keyword, is_tv)

                        poster_path = None
                        caption = ""

                        if tmdb_info:
                            if tmdb_info.get('poster_path'):
                                try:
                                    poster_resp = await loop.run_in_executor(None, requests.get, tmdb_info['poster_path'])
                                    if poster_resp.status_code == 200:
                                        poster_path = f"thumb_{file_id}.jpg"
                                        with open(poster_path, 'wb') as f: f.write(poster_resp.content)
                                except: pass

                            content_type = tmdb_info.get('media_type', 'ğŸ¬ ç”µå½±')
                            metadata = {'plot': tmdb_info.get('overview', '')}
                            
                            caption = (
                                f"{content_type}ï½œ{tmdb_info.get('title')} ({tmdb_info.get('year')})\n\n"
                                f"â­ï¸ è¯„åˆ†: {tmdb_info.get('vote_average')} ...\n"
                                f"ğŸŒ åœ°åŒº: {tmdb_info.get('countries')}\n"
                                f"ğŸ“½  ç±»å‹: {tmdb_info.get('genres')}\n"
                                f"ğŸ¬ å¯¼æ¼”: {tmdb_info.get('director')}...\n"
                                f"ğŸ‘¥ ä¸»æ¼”: {tmdb_info.get('cast')}...\n"
                                f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:100]}...</blockquote>\n\n"
                                f"{file_info_text}\n"
                                f"ğŸ· è´¨é‡: {quality}\n\n"
                                f"ğŸ™‹ æ¥è‡ªğŸ¤–è‡ªåŠ¨ç”Ÿæˆçš„JSON"
                            )
                        else:
                            caption = f"ğŸ“‚ <b>{folder_name}</b>\n\n{file_info_text}\nğŸ· è§†é¢‘è´¨é‡: {quality}\nğŸ™‹ æ¥è‡ªğŸ¤–è‡ªåŠ¨ç”Ÿæˆçš„JSON"

                        json_data = {
                            "usesBase62EtagsInExport": False,
                            "etagEncrypted": False,
                            "commonPath": f"{folder_name}/",
                            "totalFilesCount": len(files),
                            "totalSize": total_size,
                            "formattedTotalSize": size_str, 
                            "files": [{"path": f["path"], "etag": f["etag"], "size": f["size"]} for f in files]
                        }

                        json_file_path = f"{folder_name}.json"
                        
                        with link_process_lock:
                            with open(json_file_path, 'w', encoding='utf-8') as f:
                                json.dump(json_data, f, ensure_ascii=False, indent=2)
                        
                        await client.send_document(
                            chat_id=message.chat.id,
                            document=json_file_path,
                            caption=caption,
                            thumb=poster_path,
                            parse_mode=enums.ParseMode.HTML
                        )
                        
                        if os.path.exists(json_file_path): os.remove(json_file_path)
                        if poster_path and os.path.exists(poster_path): os.remove(poster_path)

                    except Exception as e:
                        logger.error(f"å¤„ç†å¤±è´¥: {e}")
                        await client.send_message(message.chat.id, f"âŒ å¤„ç†å¤±è´¥ {folder_name}: {e}")

                # 4. ä»»åŠ¡å®Œæˆååˆ é™¤è¿›åº¦æ¶ˆæ¯
                try: await status_msg.delete()
                except: pass
                
                user_state_manager.clear_state(USERBOT_STATE_ID)

            except Exception as e:
                logger.error(f"Userbot é€‰æ‹©å¤„ç†å‡ºé”™: {e}")

        # ---------------- mc å‘½ä»¤ (ä¿®å¤ï¼šåª’ä½“ç»„æ·±å±‚æ‰«æ + çº¯æ–‡æœ¬JSON + åç¼€è¿‡æ»¤) ----------------
        @app.on_message(filters.me & filters.command("mc", prefixes="-"))
        async def userbot_mc_handler(client, message):
            target_msg = message.reply_to_message or message
            try:
                if message.chat.id: await client.get_chat(message.chat.id)
            except: pass

            status_msg = await message.edit_text("â™»ï¸ æ­£åœ¨è§£æ...")
            
            def ub_log_callback(text):
                logger.info(f"[Userbot Log] {text}")
                if "ğŸ“Š" in text or "å¼€å§‹" in text:
                    async def safe_edit():
                        try:
                            import time
                            ts = time.strftime("%H:%M:%S")
                            display_text = text.split('\n')[0] 
                            await status_msg.edit_text(f"â™»ï¸ æ‰§è¡Œä¸­ ({ts})...\n{display_text}")
                        except Exception: pass
                    asyncio.run_coroutine_threadsafe(safe_edit(), loop)

            try:
                transfer_result = None
                json_data = None
                doc = None

                def is_json_doc(msg_obj):
                    if not msg_obj or not msg_obj.document: return False
                    fname = (msg_obj.document.file_name or "").lower()
                    mime = (msg_obj.document.mime_type or "").lower()
                    if fname.endswith(".json") or "json" in mime: return True
                    return False

                if is_json_doc(target_msg):
                    doc = target_msg.document
                
                if not doc and target_msg.media_group_id:
                    try:
                        media_group = await client.get_media_group(target_msg.chat.id, target_msg.id)
                        if media_group:
                            for m in media_group:
                                if is_json_doc(m):
                                    doc = m.document
                                    break
                    except Exception as e:
                        logger.warning(f"è·å–åª’ä½“ç»„å¤±è´¥: {e}")

                if doc:
                    await status_msg.edit_text(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½: {doc.file_name}...")
                    file_path = await client.download_media(doc)
                    with open(file_path, 'r', encoding='utf-8') as f:
                        json_data = json.load(f)
                    os.remove(file_path)
                
                elif target_msg.text or target_msg.caption:
                    text_content = target_msg.text or target_msg.caption
                    stripped = text_content.strip()
                    if (stripped.startswith('{') and stripped.endswith('}')) or \
                       (stripped.startswith('[') and stripped.endswith(']')):
                        try:
                            json_data = json.loads(stripped)
                            await status_msg.edit_text("ğŸ“¥ è¯†åˆ«åˆ°æ–‡æœ¬JSONï¼Œæ­£åœ¨è§£æ...")
                        except: pass

                if json_data:
                    await status_msg.edit_text("âš™ï¸ æ­£åœ¨è½¬å­˜ (è¯·ç¨å€™)...")
                    with link_process_lock:
                        # æ ¸å¿ƒå‡½æ•° core_process_json_data å†…éƒ¨å·²ç»é›†æˆäº†è¿‡æ»¤é€»è¾‘
                        transfer_result = await loop.run_in_executor(None, core_process_json_data, json_data, ub_log_callback)
                    
                    if transfer_result:
                        await status_msg.edit_text("ğŸ¨ æ­£åœ¨ç”Ÿæˆæˆ˜æŠ¥...")
                        
                        folder_name = transfer_result.get('target_dir_name', 'æœªçŸ¥ç›®å½•')
                        success_count = transfer_result.get('success_count', 0)
                        fail_count = transfer_result.get('fail_count', 0)
                        file_list = transfer_result.get('file_list', [])
                        total_size_str = transfer_result.get('total_size_str', '0B')
                        filtered_count = transfer_result.get('filtered_count', 0) # è·å–è¿‡æ»¤æ•°
                        
                        video_exts = {'.mkv', '.mp4', '.avi', '.mov', '.ts', '.rmvb', '.iso', '.wmv', '.m2ts', '.mpg', '.flv', '.rm'}
                        video_files = [f for f in file_list if os.path.splitext(f)[1].lower() in video_exts]
                        is_tv = bool(video_files and any(re.search(r's\d+|e\d+|ç¬¬\d+é›†', f.lower()) for f in video_files[:5]))
                        quality = get_quality(video_files[0]) if video_files else "æœªçŸ¥"
                        
                        clean_keyword = re.split(r'[ .\[\(]', folder_name.split('/')[0])[0]
                        tmdb_info = await loop.run_in_executor(None, tv_analyzer.fetch_tmdb_info_sync, clean_keyword, is_tv)
                        
                        poster_path = None
                        caption = ""
                        
                        filter_msg = f"ğŸš« è¿‡æ»¤: {filtered_count}\n" if filtered_count > 0 else ""

                        if tmdb_info:
                            if tmdb_info.get('poster_path'):
                                try:
                                    poster_resp = await loop.run_in_executor(None, requests.get, tmdb_info['poster_path'])
                                    if poster_resp.status_code == 200:
                                        poster_path = f"thumb_mc.jpg"
                                        with open(poster_path, 'wb') as f: f.write(poster_resp.content)
                                except: pass
                            
                            analysis_report = tv_analyzer.analyze_files(file_list, tmdb_info)
                            metadata = {'plot': tmdb_info.get('overview', '')}
                            
                            caption = (
                                f"{tmdb_info.get('media_type')}ï½œ{tmdb_info.get('title')} ({tmdb_info.get('year')})\n\n"
                                f"â­ï¸ è¯„åˆ†: {tmdb_info.get('vote_average')}\n"
                                f"ğŸŒ åœ°åŒº: {tmdb_info.get('countries')}\n"
                                f"ğŸ“½ ç±»å‹: {tmdb_info.get('genres')}\n"
                                f"ğŸ¬ å¯¼æ¼”: {tmdb_info.get('director')}...\n"
                                f"ğŸ‘¥ ä¸»æ¼”: {tmdb_info.get('cast')}...\n"
                                f"\nğŸ“– ç®€ä»‹: <blockquote expandable=\"\">{metadata.get('plot')[:100]}...</blockquote>\n\n"
                                f"ğŸ“‚ ç›®å½•: {folder_name}\n"
                                f"ğŸ“Š çŠ¶æ€: æˆåŠŸ {success_count} / å¤±è´¥ {fail_count}\n"
                                f"{filter_msg}"
                                f"ğŸ“¦ ä½“ç§¯: {total_size_str} \n"
                                f"ğŸ–¼ï¸ è´¨é‡: {quality}\n"
                                f"ğŸ¦‹ å®Œæ•´æ€§: {analysis_report}\n\n"
                                f"ğŸ™‹ æ¥è‡ªğŸ¤–è½¬å­˜å®Œæˆ"
                            )
                        else:
                            caption = (
                                f"ğŸ“‚ <b>{folder_name}</b>\n\n"
                                f"âš ï¸ æœªæ‰¾åˆ° TMDB ä¿¡æ¯ (å…³é”®è¯: {clean_keyword})\n"
                                f"ğŸ“Š çŠ¶æ€: æˆåŠŸ {success_count} / å¤±è´¥ {fail_count}\n"
                                f"{filter_msg}"
                                f"ğŸ“¦ ä½“ç§¯: {total_size_str} \n"
                                f"ğŸ–¼ï¸ è´¨é‡: {quality}\n"
                                f"ğŸ™‹ æ¥è‡ªğŸ¤–è½¬å­˜å®Œæˆ"
                            )
                        
                        try: await status_msg.delete()
                        except: pass
                        
                        if poster_path:
                            await client.send_photo(message.chat.id, photo=poster_path, caption=caption, parse_mode=enums.ParseMode.HTML)
                            os.remove(poster_path)
                        else:
                            await client.send_message(message.chat.id, caption, parse_mode=enums.ParseMode.HTML)
                    else:
                        await status_msg.edit_text("âœ… JSON è½¬å­˜ç»“æŸ (æ— è¿”å›æ•°æ®)")
                    return

                # C. å¤„ç†é“¾æ¥ (æ™®é€šæ–‡æœ¬)
                if not json_data and (target_msg.text or target_msg.caption):
                    text_content = target_msg.text or target_msg.caption
                    links = extract_123_links_from_full_text(text_content)
                    if links:
                        await status_msg.edit_text(f"ğŸ”— å‘ç° {len(links)} ä¸ªé“¾æ¥ï¼Œå¤„ç†ä¸­...")
                        results = []
                        def process_links_sync():
                            res_list = []
                            with link_process_lock:
                                for link in links:
                                    try:
                                        # parse_share_link å†…éƒ¨ä¹Ÿå·²ç»é›†æˆäº†è¿‡æ»¤é€»è¾‘
                                        parse_share_link(None, link, send_messages=False)
                                        res_list.append(f"âœ… å·²æäº¤: {link[:15]}...")
                                    except Exception as e:
                                        res_list.append(f"âŒ å¤±è´¥: {str(e)[:20]}")
                            return res_list
                        
                        res = await loop.run_in_executor(None, process_links_sync)
                        await status_msg.edit_text(f"âœ… é“¾æ¥å¤„ç†å®Œæ¯•:\n" + "\n".join(res))
                        return

                await status_msg.edit_text("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„ JSON (æ–‡ä»¶/æ–‡æœ¬)ã€‚")

            except Exception as e:
                logger.error(f"Userbot -mc error: {e}")
                await status_msg.edit_text(f"âŒ é”™è¯¯: {e}")

        async def runner():
            try:
                logger.info("ğŸ”„ [äººå½¢æ¨¡å—] è¿æ¥ä¸­...")
                await app.start()
                me = await app.get_me()
                logger.info(f"âœ… [äººå½¢æ¨¡å—] ğŸ‰ğŸ‰å°±ç»ªğŸ‰ğŸ‰ï¼ç”¨æˆ·ğŸ’ƒğŸ»: {me.first_name}")

                await asyncio.Event().wait()

            except Exception as e:
                logger.error(f"âŒ [äººå½¢æ¨¡å—] è¿è¡Œå‡ºé”™: {e}")
            finally:
                if app.is_connected: await app.stop()

        loop.run_until_complete(runner())

    except Exception as e:
        logger.error(f"âŒ [äººå½¢æ¨¡å—] å´©æºƒ: {traceback.format_exc()}")

# æ–°å¢å‡½æ•°ï¼šæŸ¥è¯¢å·²å­˜åœ¨çš„æœªå¤±æ•ˆåˆ†äº«é“¾æ¥
def get_existing_shares(client: P123Client, folder_name: str) -> dict:
    """æŸ¥è¯¢å·²å­˜åœ¨çš„æœªå¤±æ•ˆåˆ†äº«é“¾æ¥"""
    shares = []
    last_share_id = 0
    try:
        while True:
            # è°ƒç”¨åˆ†äº«åˆ—è¡¨API
            response = requests.get(
                f"https://open-api.123pan.com/api/v1/share/list?limit=100&lastShareId={last_share_id}",
                headers={
                    'Authorization': f'Bearer {client.token}',
                    'Platform': 'open_platform'
                },
                timeout=TIMEOUT
            )
            data = response.json()

            if data.get('code') != 0:
                logger.error(f"è·å–åˆ†äº«åˆ—è¡¨å¤±è´¥: {data.get('message')}")
                break

            # æå–å½“å‰é¡µåˆ†äº«æ•°æ®
            share_list = data.get('data', {}).get('shareList', [])
            shares.extend(share_list)

            # å¤„ç†åˆ†é¡µ
            last_share_id = data.get('data', {}).get('lastShareId', -1)
            if last_share_id == -1:
                break  # å·²åˆ°æœ€åä¸€é¡µ

        # ç­›é€‰å‡ºåç§°åŒ¹é…ä¸”æœªå¤±æ•ˆçš„åˆ†äº«
        for share in shares:
            if (share.get('shareName') == folder_name and
                    share.get('expired') == 0 and  # expired=0è¡¨ç¤ºæœªå¤±æ•ˆ
                    share.get('expiration', '') > '2050-06-30 00:00:00'):  # è¿‡æœŸæ—¶é—´å¤§äº2050-06-30 00:00:00
                return {
                    "url": f"https://www.123pan.com/s/{share.get('shareKey')}",
                    "password": share.get('sharePwd'),
                    "expiry": "æ°¸ä¹…æœ‰æ•ˆ"
                }

        # æœªæ‰¾åˆ°åŒ¹é…çš„æœ‰æ•ˆåˆ†äº«
        return None

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å·²å­˜åœ¨åˆ†äº«å¤±è´¥: {str(e)}")
        return None


def core_process_json_data(json_data, log_callback):
    """
    æ‰§è¡Œ JSON è½¬å­˜çš„æ ¸å¿ƒé€»è¾‘ (ä¿®å¤é™¤ä»¥é›¶é”™è¯¯ç‰ˆ)
    """
    try:
        # 1. è§£æ JSON æ•°æ®
        if isinstance(json_data, list):
            # æ ¼å¼2: æ•°ç»„æ ¼å¼ [[etag, size, filename], ...]
            logger.info("æ£€æµ‹åˆ°æ•°ç»„æ ¼å¼çš„å¦™ä¼ æ–‡ä»¶")
            common_path = ''
            files = []
            uses_v2_etag = False
            total_size_json = 0
            
            for item in json_data:
                if isinstance(item, list) and len(item) >= 3:
                    etag, size, filename = item[0], item[1], item[2]
                    files.append({
                        'path': filename,
                        'etag': etag,
                        'size': size
                    })
                    total_size_json += int(size)
        else:
            # æ ¼å¼1: å¯¹è±¡æ ¼å¼ {commonPath, files, ...}
            logger.info("æ£€æµ‹åˆ°å¯¹è±¡æ ¼å¼çš„å¦™ä¼ æ–‡ä»¶")
            common_path = json_data.get('commonPath', '').strip()
            if common_path.endswith('/'):
                common_path = common_path[:-1]
            files = json_data.get('files', [])
            uses_v2_etag = json_data.get('usesBase62EtagsInExport', False)
            total_size_json = json_data.get('totalSize', 0)

        # 2. è¿‡æ»¤æ–‡ä»¶é€»è¾‘
        filtered_files = []
        skipped_count_by_ext = 0
        
        for file_info in files:
            f_path = file_info.get('path', '')
            if check_ext_filter(f_path):
                skipped_count_by_ext += 1
                continue
            filtered_files.append(file_info)
        
        files = filtered_files
        
        if skipped_count_by_ext > 0:
            log_callback(f"ğŸš« æ ¹æ®é…ç½®è¿‡æ»¤äº† {skipped_count_by_ext} ä¸ªä¸éœ€è¦çš„æ–‡ä»¶")

        if not files:
            log_callback("JSONæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ–‡ä»¶ï¼ˆæˆ–å…¨éƒ¨è¢«è¿‡æ»¤ï¼‰ã€‚")
            return None

        # 3. å‡†å¤‡å·¥ä½œ
        total_files_count = len(files) # é”å®šæ€»æ•°ï¼Œé˜²æ­¢å˜åŒ–
        log_callback(f"å¼€å§‹è½¬å­˜JSONæ–‡ä»¶ä¸­çš„ {total_files_count} ä¸ªæ–‡ä»¶...")
        start_time = time.time()
        
        client = init_123_client()
        results = []
        message_batch = []
        batch_size = 0
        total_size = 0
        skip_count = 0
        last_etag = None
        success_filenames = [] 
        folder_cache = {}
        target_dir_name = common_path if common_path else 'JSONè½¬å­˜'
        target_dir_id = UPLOAD_JSON_TARGET_PID

        # 4. éå†è½¬å­˜
        for i, file_info in enumerate(files):
            file_path = file_info.get('path', '')
            if common_path:
                file_path = f"{common_path}/{file_path}"
            etag = file_info.get('etag', '')
            size = int(file_info.get('size', 0))

            if not all([file_path, etag, size]):
                results.append({"success": False, "file_name": file_path, "error": "ä¿¡æ¯ä¸å…¨"})
                continue

            try:
                # 4.1 åˆ›å»ºç›®å½•
                path_parts = file_path.split('/')
                file_name = path_parts.pop()
                parent_id = target_dir_id
                current_path = ""
                
                for part in path_parts:
                    if not part: continue
                    current_path = f"{current_path}/{part}" if current_path else part
                    cache_key = f"{parent_id}/{current_path}"

                    if cache_key in folder_cache:
                        parent_id = folder_cache[cache_key]
                        continue

                    retry_count = 3
                    folder = None
                    while retry_count > 0:
                        try:
                            folder = client.fs_mkdir(part, parent_id=parent_id, duplicate=1)     
                            time.sleep(0.2)                  
                            check_response(folder)
                            break
                        except Exception:
                            retry_count -= 1
                            time.sleep(3)

                    if folder:
                        folder_id = folder["data"]["Info"]["FileId"]
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                
                # 4.2 å¤„ç†ETag
                if uses_v2_etag:
                    etag = optimized_etag_to_hex(etag, True)

                # 4.3 æ‰§è¡Œç§’ä¼ 
                retry_count = 3
                rapid_resp = None
                while retry_count > 0:
                    if last_etag == etag:
                        skip_count += 1
                        rapid_resp = {"data": {"Reuse": True, "Skip": True}, "code": 0}
                        break
                    try:
                        rapid_resp = client.upload_file_fast(
                            file_name=file_name,
                            parent_id=parent_id,
                            file_md5=robust_normalize_md5(etag),
                            file_size=size,
                            duplicate=1
                        )
                        check_response(rapid_resp)
                        break
                    except Exception:
                        retry_count -= 1
                        time.sleep(3)

                # 4.4 è®°å½•ç»“æœ
                dir_p = os.path.dirname(file_path)
                
                if rapid_resp is None:
                    err = "è¯·æ±‚é‡è¯•è€—å°½"
                    results.append({"success": False, "file_name": file_path, "error": err})
                    message_batch.append({'status': 'âŒ', 'dir': dir_p, 'file': f"{file_name} ({err})"})
                    
                elif rapid_resp.get("code") == 0 and rapid_resp.get("data", {}).get("Reuse", False):
                    if rapid_resp.get("data", {}).get("Skip"):
                        message_batch.append({'status': 'ğŸ”„', 'dir': dir_p, 'file': f"{file_name} (é‡å¤)"})
                        success_filenames.append(file_name) 
                    else:
                        last_etag = etag
                        results.append({"success": True, "file_name": file_path, "size": size})
                        total_size += size
                        message_batch.append({'status': 'âœ…', 'dir': dir_p, 'file': file_name})
                        success_filenames.append(file_name) 
                else:
                    err = "æ— æ³•ç§’ä¼ "
                    results.append({"success": False, "file_name": file_path, "error": err})
                    message_batch.append({'status': 'âŒ', 'dir': dir_p, 'file': f"{file_name} ({err})"})
                
                batch_size += 1

                # 4.5 [å…³é”®ä¿®å¤] å®‰å…¨çš„æ—¥å¿—è®¡ç®—
                if batch_size % 10 == 0:
                    tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
                    for entry in message_batch:
                        tree_messages[entry['dir']][entry['status']].append(entry['file'])
                    
                    batch_msg = []
                    for d, s_files in tree_messages.items():
                        for s, fs in s_files.items():
                            if fs:
                                batch_msg.append(f"--- {s} {d}")
                                for idx, f in enumerate(fs):
                                    prefix = '      â””â”€â”€' if idx == len(fs)-1 else '      â”œâ”€â”€'
                                    batch_msg.append(f"{prefix} {f}")
                    batch_msg_str = "\n".join(batch_msg)
                    
                    # ä¿®å¤ç‚¹1ï¼šé˜²æ­¢ total_files_count ä¸º 0
                    if total_files_count > 0:
                        percent = int(batch_size / total_files_count * 100)
                    else:
                        percent = 0
                        
                    log_callback(f"ğŸ“Š {batch_size}/{total_files_count} ({percent}%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg_str}")
                    message_batch = []
                
                # 4.6 [å…³é”®ä¿®å¤] å®‰å…¨çš„é€Ÿç‡ä¼‘çœ 
                # é˜²æ­¢ ENV_FILE_PER_SECOND ä¸º 0 å¯¼è‡´ crash
                rate_limit = get_int_env("ENV_FILE_PER_SECOND", 5)
                if rate_limit > 0:
                    time.sleep(1.0 / rate_limit)

            except Exception as e:
                # æ•è·å•ä¸ªæ–‡ä»¶å¤„ç†ä¸­çš„æ‰€æœ‰å¼‚å¸¸ï¼Œé˜²æ­¢æ‰“æ–­æ•´ä¸ªä»»åŠ¡
                err_str = str(e)
                logger.error(f"å¤„ç†æ–‡ä»¶å‡ºé”™ {file_name}: {err_str}")
                results.append({"success": False, "file_name": file_path, "error": err_str})
                message_batch.append({'status': 'âŒ', 'dir': os.path.dirname(file_path), 'file': f"{file_name} ({err_str})"})
                batch_size += 1

        # 5. å¤„ç†å‰©ä½™æ¶ˆæ¯
        if message_batch:
            tree_messages = defaultdict(lambda: {'âœ…': [], 'âŒ': [], 'ğŸ”„': []})
            for entry in message_batch:
                tree_messages[entry['dir']][entry['status']].append(entry['file'])
            batch_msg = []
            for d, s_files in tree_messages.items():
                for s, fs in s_files.items():
                    if fs:
                        batch_msg.append(f"--- {s} {d}")
                        for idx, f in enumerate(fs):
                            prefix = '      â””â”€â”€' if idx == len(fs)-1 else '      â”œâ”€â”€'
                            batch_msg.append(f"{prefix} {f}")
            batch_msg_str = "\n".join(batch_msg)
            log_callback(f"ğŸ“Š {batch_size}/{total_files_count} (100%) ä¸ªæ–‡ä»¶å·²å¤„ç†\n\n{batch_msg_str}")

        # 6. ç»Ÿè®¡ç»“æœ
        end_time = time.time()
        elapsed_time = end_time - start_time
        hours, remainder = divmod(int(elapsed_time), 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

        success_count = sum(1 for r in results if r.get('success'))
        fail_count = len(results) - success_count
        size_str = get_formatted_size(total_size)

        result_msg = (
            f"âœ… JSONæ–‡ä»¶è½¬å­˜å®Œæˆï¼\n"
            f"âœ…æˆåŠŸ: {success_count}ä¸ª\n"
            f"âŒå¤±è´¥: {fail_count}ä¸ª\n"
            f"ğŸ”„è·³è¿‡é‡å¤: {skip_count}ä¸ª\n"
            f"ğŸš«åç¼€è¿‡æ»¤: {skipped_count_by_ext}ä¸ª\n"
            f"ğŸ“Šä½“ç§¯: {size_str}\n"
            f"â±ï¸è€—æ—¶: {time_str}"
        )
        log_callback(result_msg)

        if fail_count > 0:
            failed_files = [f"â€¢ {r['file_name']}ï¼ˆ{r.get('error')}ï¼‰" for r in results if not r.get("success")]
            # åˆ†æ‰¹å‘é€é”™è¯¯æ—¥å¿—
            for idx in range(0, len(failed_files), 10):
                batch = failed_files[idx:idx+10]
                batch_msg = "âŒ å¤±è´¥è¯¦æƒ…:\n" + "\n".join(batch)
                log_callback(batch_msg)
                time.sleep(0.5)
        
        return {
            "success_count": success_count,
            "fail_count": fail_count,
            "skip_count": skip_count,
            "filtered_count": skipped_count_by_ext,
            "total_size_str": size_str,
            "time_str": time_str,
            "target_dir_name": target_dir_name, 
            "file_list": success_filenames     
        }

    except Exception as e:
        logger.error(f"æ ¸å¿ƒJSONå¤„ç†å¼‚å¸¸: {str(e)}")
        log_callback(f"âŒ æ ¸å¿ƒå¤„ç†å¤±è´¥: {str(e)}")
        return None

@bot.message_handler(content_types=['document'], func=lambda message: message.document.mime_type == 'application/json' or message.document.file_name.endswith('.json'))
def process_json_file(message):
    with link_process_lock:  # è·å–é”ï¼Œç¡®ä¿å¤šä¸ªè¯·æ±‚ä¾æ¬¡å¤„ç†
        user_id = message.from_user.id
        if user_id != TG_ADMIN_USER_ID:
            reply_thread_pool.submit(send_reply, message, "æ‚¨æ²¡æœ‰æƒé™ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
            return
        
        logger.info("è¿›å…¥ Bot æ–‡ä»¶è½¬å­˜ JSON æµç¨‹")
        
        try:
            # 1. è·å–å¹¶ä¸‹è½½æ–‡ä»¶
            file_retry_count = 0
            file_path = None
            while file_retry_count < 10:
                try:
                    file_id = message.document.file_id
                    file_info = bot.get_file(file_id)
                    file_path = file_info.file_path
                    break
                except Exception as e:
                    logger.error(f"ä»TGè·å–æ–‡ä»¶å¤±è´¥: {e}")
                    file_retry_count += 1
                    time.sleep(3)
            
            if not file_path:
                reply_thread_pool.submit(send_reply, message, "ä¸‹è½½æ–‡ä»¶å¤±è´¥ï¼Œè¯·é‡è¯•ã€‚")
                return

            json_url = f'https://api.telegram.org/file/bot{TG_BOT_TOKEN}/{file_path}'
            response = requests.get(json_url)
            response.encoding = 'utf-8' # æ˜¾å¼è®¾ç½®ç¼–ç 
            json_data = response.json()

            # 2. å®šä¹‰å›è°ƒå‡½æ•°ï¼Œé€‚é… core_process_json_data çš„æ—¥å¿—æ¥å£
            def bot_log_adapter(text):
                # å¦‚æœæ˜¯è¿›åº¦æ¡æ¶ˆæ¯ï¼Œå°è¯•ä½¿ç”¨ delete æ¨¡å¼å‘é€ï¼ˆå¦‚æœé€»è¾‘æ”¯æŒï¼‰
                if "ğŸ“Š" in text:
                    reply_thread_pool.submit(send_reply_delete, message, text)
                else:
                    reply_thread_pool.submit(send_reply, message, text)

            # 3. è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
            core_process_json_data(json_data, bot_log_adapter)
            
        except Exception as e:
            logger.error(f"å¤„ç†JSONæ–‡ä»¶å…¨å±€å¼‚å¸¸: {str(e)}")
            reply_thread_pool.submit(send_reply, message, f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")

from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
link_process_lock = threading.Lock()
quark_folder_lock = threading.Lock()
def process_single_quark_file(client, file_info, common_path, target_dir_id, folder_cache, uses_v2_etag):
    """å•ä¸ªå¤¸å…‹æ–‡ä»¶å¤„ç†å‡½æ•° (ç”¨äºå¤šçº¿ç¨‹å¹¶å‘)"""
    file_path = file_info.get('path', '')
    
    if check_ext_filter(file_path):
        return {
            "success": True,   
            "file_name": file_path, 
            "size": 0, 
            "skip": False, 
            "msg": "åç¼€è¿‡æ»¤"  
        }  
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    if common_path:
        file_path = f"{common_path}/{file_path}"
    etag = file_info.get('etag', '')
    size = int(file_info.get('size', 0))

    if not all([file_path, etag, size]):
        return {"success": False, "file_name": file_path or "æœªçŸ¥æ–‡ä»¶", "error": "æ–‡ä»¶ä¿¡æ¯ä¸å®Œæ•´", "path": file_path}

    try:
        # --- 1. ç›®å½•ç»“æ„å¤„ç† (çº¿ç¨‹å®‰å…¨åŒº) ---
        path_parts = file_path.split('/')
        file_name = path_parts.pop()
        parent_id = target_dir_id
        
        current_path = ""
        # éå†è·¯å¾„åˆ›å»ºç›®å½•
        for part in path_parts:
            if not part: continue
            current_path = f"{current_path}/{part}" if current_path else part
            cache_key = f"{parent_id}/{current_path}"

            # åŠ é”æ£€æŸ¥/åˆ›å»ºç›®å½•ï¼Œé˜²æ­¢å¤šçº¿ç¨‹ç«äº‰å¯¼è‡´é‡å¤åˆ›å»º
            with quark_folder_lock:
                if cache_key in folder_cache:
                    parent_id = folder_cache[cache_key]
                else:
                    # åˆ›å»ºæ–°æ–‡ä»¶å¤¹ï¼ˆå¸¦é‡è¯•ï¼‰
                    mk_retry = 2
                    folder_id = None
                    while mk_retry > 0:
                        try:
                            # å°è¯•åˆ›å»º
                            folder = client.fs_mkdir(part, parent_id=parent_id, duplicate=1)
                            # ç®€å•çš„æ£€æŸ¥ï¼Œä¸åšè€—æ—¶çš„ check_response
                            if folder.get("code") == 0:
                                folder_id = folder["data"]["Info"]["FileId"]
                                break
                            else:
                                mk_retry -= 1
                                time.sleep(0.5)
                        except Exception:
                            mk_retry -= 1
                            time.sleep(0.5)
                    
                    if folder_id:
                        folder_cache[cache_key] = folder_id
                        parent_id = folder_id
                    else:
                        # åˆ›å»ºå¤±è´¥åˆ™æ²¿ç”¨ä¸Šçº§IDï¼Œé˜²æ­¢æ•´æ¡è·¯å¾„å¤±è´¥
                        pass

        # --- 2. å¤„ç†ETag ---
        if uses_v2_etag:
            etag = optimized_etag_to_hex(etag, True)
        
        final_md5 = robust_normalize_md5(etag)

        # --- 3. æ‰§è¡Œç§’ä¼  (è€—æ—¶æ“ä½œï¼Œå¹¶å‘æ‰§è¡Œ) ---
        retry_count = 3
        rapid_resp = None
        
        while retry_count > 0:
            try:
                rapid_resp = client.upload_file_fast(
                    file_name=file_name,
                    parent_id=parent_id,
                    file_md5=final_md5,
                    file_size=size,
                    duplicate=1
                )
                
                # æˆåŠŸåˆ¤æ–­ (Reuse=True)
                if rapid_resp.get("code") == 0 and \
                   (rapid_resp.get("data", {}).get("Reuse") or rapid_resp.get("data", {}).get("reuse")):
                    return {
                        "success": True, 
                        "file_name": file_path, 
                        "size": size, 
                        "skip": rapid_resp.get("data", {}).get("Skip", False),
                        "file_id": rapid_resp.get("data", {}).get("FileId", "")
                    }
                
                # æ˜ç¡®çš„å¤±è´¥ (Reuse=False)
                if rapid_resp.get("code") == 0:
                     return {
                        "success": False, 
                        "file_name": file_path, 
                        "error": "äº‘ç«¯æ— æ­¤æ–‡ä»¶ï¼Œç§’ä¼ å¤±è´¥"
                    }
                
                # å…¶ä»–APIé”™è¯¯ï¼Œé‡è¯•
                retry_count -= 1
                time.sleep(2)
                
            except Exception as e:
                retry_count -= 1
                time.sleep(2)
                if retry_count == 0:
                    return {"success": False, "file_name": file_path, "error": str(e)}

        return {"success": False, "file_name": file_path, "error": rapid_resp.get("message", "è¯·æ±‚è¶…æ—¶") if rapid_resp else "æœªçŸ¥é”™è¯¯"}

    except Exception as e:
        return {"success": False, "file_name": file_path, "error": f"å¤„ç†å¼‚å¸¸: {str(e)}"}


def save_json_file_quark(message, json_data):
    logger.info("è¿›å…¥123è½¬å­˜å¤¸å…‹ (æ™ºèƒ½é‡è¯•ç‰ˆ V5 - ç›´æ¥å‘é€JSON)")
    try:
        # 1. åŸºç¡€æ•°æ®æå–
        origin_common_path = json_data.get('commonPath', '').strip()
        if origin_common_path and not origin_common_path.endswith('/'):
            origin_common_path += '/'
            
        files = json_data.get('files', [])
        uses_v2_etag = json_data.get('usesBase62EtagsInExport', False)
        total_files_count = len(files)

        if not files:
            reply_thread_pool.submit(send_reply, message, "å¤¸å…‹åˆ†äº«ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶ä¿¡æ¯ã€‚")
            return

        # å‘é€åˆå§‹æ¶ˆæ¯
        status_msg_text = f"ğŸš€ å¼€å§‹è½¬å­˜å¤¸å…‹æ–‡ä»¶ (å…± {total_files_count} ä¸ª)...\nâš¡ï¸ æ­£åœ¨å¯åŠ¨å¤šçº¿ç¨‹åŠ é€Ÿ..."
        reply_thread_pool.submit(send_reply_delete, message, status_msg_text)
        
        start_time = time.time()
        client = init_123_client()

        # 2. åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
        results = []
        total_size = 0
        skip_count = 0     # é‡å¤è·³è¿‡
        filter_count = 0   # åç¼€è¿‡æ»¤
        success_count = 0  # å®é™…æˆåŠŸ
        fail_count = 0     # å¤±è´¥
        
        # [æ–°å¢] è§†é¢‘ç»Ÿè®¡å˜é‡
        video_count = 0
        video_total_size = 0
        video_exts = {'.mkv', '.mp4', '.avi', '.mov', '.ts', '.rmvb', '.iso', '.wmv', '.m2ts', '.mpg', '.flv', '.rm'}

        # ç”¨äºæ”¶é›†å¤±è´¥æ–‡ä»¶çš„åˆ—è¡¨
        failed_files_data = []
        
        # æ–‡ä»¶å¤¹ç¼“å­˜
        folder_cache = {}
        target_dir_id = get_int_env("ENV_123_KUAKE_UPLOAD_PID", 0)
        
        # 3. å¯åŠ¨å¤šçº¿ç¨‹
        max_workers = 5  
        
        processed_count = 0
        last_report_time = 0
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_file = {
                executor.submit(
                    process_single_quark_file, 
                    client, 
                    file_info, 
                    origin_common_path, 
                    target_dir_id, 
                    folder_cache, 
                    uses_v2_etag
                ): file_info for file_info in files
            }
            
            for future in as_completed(future_to_file):
                processed_count += 1
                file_info = future_to_file[future]
                res = future.result()
                
                # ç»Ÿè®¡é€»è¾‘
                if res['success']:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºåç¼€è¿‡æ»¤
                    if res.get('msg') == "åç¼€è¿‡æ»¤":
                        filter_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤è·³è¿‡
                    elif res.get('skip'):
                        skip_count += 1
                        success_count += 1 # é€»è¾‘ä¸Šç®—æˆåŠŸ
                        # ç»Ÿè®¡è§†é¢‘ä¿¡æ¯ï¼ˆé‡å¤çš„ä¹Ÿç®—åœ¨è§†é¢‘ç»Ÿè®¡é‡Œï¼Œçœ‹éœ€æ±‚ï¼Œé€šå¸¸ç®—äº†æ€»æ•°ä¹Ÿè¦ç®—è¿™ä¸ªï¼‰
                        fname = res.get('file_name', '')
                        fsize = res.get('size', 0)
                        if os.path.splitext(fname)[1].lower() in video_exts:
                            video_count += 1
                            video_total_size += fsize
                        
                    # æ­£å¸¸è½¬å­˜æˆåŠŸ
                    else:
                        success_count += 1
                        total_size += res['size']
                        # ç»Ÿè®¡è§†é¢‘ä¿¡æ¯
                        fname = res.get('file_name', '')
                        fsize = res.get('size', 0)
                        if os.path.splitext(fname)[1].lower() in video_exts:
                            video_count += 1
                            video_total_size += fsize
                        logger.info(f"âœ… [å¤¸å…‹] ç§’ä¼ æˆåŠŸ: {res['file_name']}")
                else:
                    fail_count += 1
                    logger.warning(f"âŒ [å¤¸å…‹] ç§’ä¼ å¤±è´¥: {res['file_name']} ({res.get('error')})")
                    failed_files_data.append(file_info)
                
                results.append(res)
                
                # è¿›åº¦æŠ¥å‘Š
                current_time = time.time()
                if current_time - last_report_time > 3 or processed_count == total_files_count:
                    last_report_time = current_time
                    percent = int(processed_count / total_files_count * 100)
                    progress_msg = (
                        f"ğŸ“Š è½¬å­˜è¿›åº¦: {processed_count}/{total_files_count} ({percent}%)\n"
                        f"âœ… æˆåŠŸ: {success_count} (è·³è¿‡ {skip_count})\n"
                        f"ğŸš« è¿‡æ»¤: {filter_count}\n"
                        f"âŒ å¤±è´¥: {fail_count}"
                    )
                    reply_thread_pool.submit(send_reply_delete, message, progress_msg)

        # 4. æœ€ç»ˆç»Ÿè®¡
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        hours, remainder = divmod(int(elapsed_time), 3600)
        minutes, seconds = divmod(remainder, 60)
        time_str = f"{hours:02d}:{minutes:02d}:{seconds:02d}"

        total_size_gb = total_size / (1024 ** 3)
        size_str = f"{total_size_gb:.2f}GB"
        
        # è®¡ç®—è§†é¢‘å¹³å‡å¤§å°
        video_avg_size_str = "0B"
        video_total_size_str = get_formatted_size(video_total_size)
        if video_count > 0:
            video_avg_size_str = get_formatted_size(video_total_size / video_count)

        result_msg = (
            f"âœ… å¤¸å…‹è½¬å­˜ä»»åŠ¡å®Œæˆï¼\n"
            f"ğŸ“‚ æ€»æ–‡ä»¶: {total_files_count}ä¸ª\n"
            f"âœ… æˆåŠŸ: {success_count}ä¸ª\n"
            f"âŒ å¤±è´¥: {fail_count}ä¸ª\n"
            f"ğŸ”„ è·³è¿‡é‡å¤: {skip_count}ä¸ª\n"
            f"ğŸš« åç¼€è¿‡æ»¤: {filter_count}ä¸ª\n"
            f"----------------------\n"
            f"ğŸ¬ è§†é¢‘ç»Ÿè®¡: {video_count}ä¸ª\n"
            f"ğŸ“¹ è§†é¢‘æ€»å¤§: {video_total_size_str}\n"
            f"ğŸ“ å¹³å‡å¤§å°: {video_avg_size_str}\n"
            f"----------------------\n"
            f"ğŸ“¦ å®é™…è½¬å­˜: {size_str}\n"
            f"â±ï¸ è€—æ—¶: {time_str}"
        )
        reply_thread_pool.submit(send_reply, message, result_msg)
        
        # 5. [ä¼˜åŒ–] ç”Ÿæˆå¹¶å‘é€å¤±è´¥æ–‡ä»¶åˆ—è¡¨ (ç›´æ¥å‘é€JSON)
        if fail_count > 0 and failed_files_data:
            try:
                # --- è®¡ç®—æ–°çš„ commonPath ---
                full_paths = []
                for f in failed_files_data:
                    rel_path = f.get('path', '').replace('\\', '/')
                    if origin_common_path:
                        full_p = f"{origin_common_path}{rel_path}"
                    else:
                        full_p = rel_path
                    full_paths.append(full_p)
                
                new_common_prefix = ""
                if full_paths:
                    try:
                        new_common_prefix = os.path.commonpath(full_paths)
                        new_common_prefix = new_common_prefix.replace('\\', '/')
                        if new_common_prefix:
                            new_common_prefix += '/'
                    except ValueError:
                        new_common_prefix = ""
                
                # --- ä¿®æ­£æ–‡ä»¶è·¯å¾„å¹¶å¼ºåˆ¶å­—å…¸é¡ºåº ---
                processed_files = []
                total_retry_size = 0
                for f, full_p in zip(failed_files_data, full_paths):
                    if new_common_prefix and full_p.startswith(new_common_prefix):
                        final_path = full_p[len(new_common_prefix):]
                    else:
                        final_path = full_p 
                    
                    # æ˜¾å¼æŒ‰é¡ºåºæ„é€ å­—å…¸: path -> etag -> size
                    new_f = {
                        "path": final_path,
                        "etag": f.get('etag'),
                        "size": f.get('size')
                    }
                    processed_files.append(new_f)
                    total_retry_size += int(f.get('size', 0))

                # --- æ„é€ æœ‰åºå­—å…¸ (å¤´éƒ¨åœ¨æœ€å‰ï¼Œfilesåœ¨æœ€å) ---
                retry_json = {}
                retry_json["usesBase62EtagsInExport"] = uses_v2_etag
                retry_json["etagEncrypted"] = False
                retry_json["commonPath"] = new_common_prefix
                retry_json["totalFilesCount"] = len(processed_files)
                retry_json["totalSize"] = total_retry_size
                retry_json["files"] = processed_files 
                
                # --- å†³å®šæ–‡ä»¶å (åç¼€æ”¹ä¸º .json) ---
                if new_common_prefix:
                    filename_base = new_common_prefix.strip('/')
                    filename_base = re.sub(r'[\\/:*?"<>|]', '_', filename_base)
                    retry_filename = f"{filename_base}.json"
                else:
                    timestamp = int(time.time())
                    retry_filename = f"failed_files_{timestamp}.json"
                
                with open(retry_filename, 'w', encoding='utf-8') as f:
                    json.dump(retry_json, f, ensure_ascii=False, indent=2)
                
                # --- å‘é€æ–‡ä»¶ ---
                caption = (
                    f"âš ï¸ `æ£€æµ‹åˆ° {fail_count} ä¸ªæ–‡ä»¶è½¬å­˜å¤±è´¥ã€‚`\n"
                    f"ğŸ“„ `å·²ç”Ÿæˆå¤±è´¥é‡è¯•æ–‡ä»¶ï¼š{retry_filename}`\n"
                    f"ğŸ’¡ `  ğŸ‘‡ğŸ‘‡ğŸ‘‡é£Ÿç”¨æ–¹æ³•ğŸ‘‡ğŸ‘‡ğŸ‘‡`\n\n"
                    f"`å¾… 123 äº‘ç›˜èµ„æºæ›´æ–°åï¼Œç›´æ¥å°†æ­¤ **JSON æ–‡ä»¶è½¬å‘ç»™æœºå™¨äºº** å³å¯é‡è¯•ã€‚`\n"
                )
                
                with open(retry_filename, 'rb') as f:
                    bot.send_document(
                        message.chat.id, 
                        f, 
                        caption=caption,
                        parse_mode='Markdown'
                    )
                
                os.remove(retry_filename)
                
            except Exception as e:
                logger.error(f"ç”Ÿæˆå¤±è´¥é‡è¯•æ–‡ä»¶å‡ºé”™: {e}")
                reply_thread_pool.submit(send_reply, message, f"âŒ ç”Ÿæˆå¤±è´¥åˆ—è¡¨æ–‡ä»¶å‡ºé”™: {str(e)}")

    except Exception as e:
        logger.error(f"å¤¸å…‹è½¬å­˜å…¨å±€å¼‚å¸¸: {str(e)}")
        reply_thread_pool.submit(send_reply, message, f"âŒ å¤„ç†å¼‚å¸¸: {str(e)}")

# Base62å­—ç¬¦è¡¨ï¼ˆ123äº‘ç›˜V2 APIä½¿ç”¨ï¼‰
BASE62_CHARS = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'

def optimized_etag_to_hex(etag, is_v2=False):
    """å°†Base62ç¼–ç çš„ETagè½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼ï¼ˆå‚è€ƒ123pan_botä¸­çš„å®ç°ï¼‰"""
    if not is_v2:
        return etag
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„MD5æ ¼å¼ï¼ˆ32ä½åå…­è¿›åˆ¶ï¼‰
        if len(etag) == 32 and all(c in '0123456789abcdefABCDEF' for c in etag):
            return etag.lower()
        
        # è½¬æ¢Base62åˆ°åå…­è¿›åˆ¶
        num = 0
        for char in etag:
            if char not in BASE62_CHARS:
                logger.error(f"âŒ ETagåŒ…å«æ— æ•ˆå­—ç¬¦: {char}")
                return etag
            num = num * 62 + BASE62_CHARS.index(char)
        
        # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å¹¶ç¡®ä¿32ä½
        hex_str = hex(num)[2:].lower()
        if len(hex_str) > 32:
            # å–å32ä½
            hex_str = hex_str[-32:]
            logger.warning(f"ETagè½¬æ¢åé•¿åº¦è¶…è¿‡32ä½ï¼Œæˆªæ–­ä¸º: {hex_str}")
        elif len(hex_str) < 32:
            # å‰é¢è¡¥é›¶
            hex_str = hex_str.zfill(32)
            logger.warning(f"ETagè½¬æ¢åä¸è¶³32ä½ï¼Œè¡¥é›¶å: {hex_str}")
        
        # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„MD5
        if len(hex_str) != 32 or not all(c in '0123456789abcdef' for c in hex_str):
            logger.error(f"âŒ è½¬æ¢åETagæ ¼å¼æ— æ•ˆ: {hex_str}")
            return etag
        
        return hex_str
    except Exception as e:
        logger.error(f"âŒ ETagè½¬æ¢å¤±è´¥: {str(e)}")
        return etag

# æ³¨å†Œæ–‡æ¡£æ¶ˆæ¯å¤„ç†å™¨ï¼ˆå·²ç§»è‡³start_bot_threadå‡½æ•°å†…éƒ¨ï¼‰
# bot.message_handler(content_types=['document'])(process_json_file)

# å®šä¹‰botçº¿ç¨‹å˜é‡
bot_thread = None

def start_bot_thread():
    global bot
    # ç¡®ä¿botå®ä¾‹å­˜åœ¨
    if not bot:
        bot = telebot.TeleBot(TG_BOT_TOKEN)
    while True:
        try:
            #bot.polling(none_stop=True, interval=1)
            bot.infinity_polling(logger_level=logging.ERROR)
        except Exception as e:
            logger.warning(f"ä»£ç†ç½‘ç»œä¸ç¨³å®šï¼Œä¸TGå°è¯•é‡è¿ä¸­...\né”™è¯¯åŸå› :{str(e)}")
            time.sleep(5)
    return threading.current_thread()



def check_task():
    global bot_thread
    # æ£€æŸ¥botçº¿ç¨‹çŠ¶æ€ï¼ˆå›ºå®š20ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
    if not bot_thread or not bot_thread.is_alive():
        logger.warning(f"ä»£ç†ç½‘ç»œä¸ç¨³å®šï¼Œä¸TGå°è¯•é‡è¿ä¸­...")
        bot_thread = threading.Thread(target=start_bot_thread, daemon=True)
        bot_thread.start()

if __name__ == "__mp_main__":
    from bot115 import tg_115monitor
    from bot189 import Cloud189
    client189 = Cloud189()
    ENV_189_CLIENT_ID = os.getenv("ENV_189_CLIENT_ID","")
    ENV_189_CLIENT_SECRET = os.getenv("ENV_189_CLIENT_SECRET","")

    if (ENV_189_CLIENT_ID and ENV_189_CLIENT_SECRET):
        logger.info("å¤©ç¿¼äº‘ç›˜æ­£åœ¨å°è¯•ç™»å½• ...")
        client189.login(ENV_189_CLIENT_ID, ENV_189_CLIENT_SECRET)

# === [é‡å†™] å…¨èƒ½ç‰ˆå¤©ç¿¼äº‘ç›‘æ§ (é›†æˆç§’ä¼ +ç›®å½•ç»“æ„+å…œåº•+ä¼˜é›…æˆ˜æŠ¥) ===
def tg_189monitor(client189):
    # å¼•ç”¨å¿…è¦ç»„ä»¶
    from bot189 import init_database, get_latest_messages, save_message, TelegramNotifier
    from bot189 import TG_BOT_TOKEN, TG_ADMIN_USER_ID, get_share_file_snapshot, save_189_link
    
    init_database()
    notifier = TelegramNotifier(TG_BOT_TOKEN, TG_ADMIN_USER_ID)
    logger.info("===== å¼€å§‹æ£€æŸ¥ å¤©ç¿¼ç½‘ç›˜ç›‘æ§ (æ™ºèƒ½ç§’ä¼ ç‰ˆ) =====")

    # 1. è·å–æ–°æ¶ˆæ¯
    new_messages = get_latest_messages()
    if not new_messages:
        return

    # 2. åˆå§‹åŒ– 123 å®¢æˆ·ç«¯
    client123 = init_123_client()

    # 3. è·å–ç›®å½•é…ç½®
    # 123ç›®æ ‡ç›®å½• (ç§’ä¼ ç”¨)
    pid_for_123 = os.getenv("ENV_189GO123_UPLOAD_PID", "")
    if not pid_for_123:
        pid_for_123 = os.getenv("ENV_123_UPLOAD_PID", "0")

    # 189å…œåº•ç›®å½• (è½¬å­˜ç”¨)
    pid_for_189 = os.getenv("ENV_189_LINK_UPLOAD_PID", "")
    if not pid_for_189:
        pid_for_189 = os.getenv("ENV_189_UPLOAD_PID", "-11")

    logger.info(f"189ç›‘æ§é…ç½® | 123ç›®æ ‡ID: {pid_for_123} | 189å…œåº•ID: {pid_for_189}")

    # 4. éå†å¤„ç†æ–°æ¶ˆæ¯
    for msg in new_messages:
        message_id, date_str, message_url, target_url, message_text = msg
        logger.info(f"å¤„ç†æ–°æ¶ˆæ¯: {target_url}")
        
        status = "å¤„ç†ä¸­"
        result_msg = ""
        
        try:
            # === A. è·å–å¿«ç…§ (åªè¯»ä¸å­˜) ===
            files_in_share, root_share_name = get_share_file_snapshot(client189, target_url)
            
            all_rapid_success = False
            
            # [æ–°å¢] æå‰å®šä¹‰ç»Ÿè®¡å˜é‡ï¼Œä¾›åç»­å¤ç”¨
            video_count = 0
            total_size_str = "æœªçŸ¥"
            avg_size_str = "æœªçŸ¥"
            display_msg_url = message_url
            
            # [æ–°å¢] å°è¯•é¢„å…ˆè®¡ç®—ç»Ÿè®¡ä¿¡æ¯ (å¦‚æœå¿«ç…§è·å–æˆåŠŸ)
            if files_in_share:
                try:
                    # 1. è¿‡æ»¤åç¼€ (å¦‚æœå…¨å±€å®šä¹‰äº†check_ext_filteråˆ™è°ƒç”¨ï¼Œå¦åˆ™è·³è¿‡)
                    filtered_files = []
                    for f in files_in_share:
                        if 'check_ext_filter' in globals() and check_ext_filter(f.get('name', '')):
                            continue
                        filtered_files.append(f)
                    files_in_share = filtered_files

                    # 2. ç»Ÿè®¡æ•°æ®
                    video_exts = {'.mkv', '.mp4', '.avi', '.mov', '.ts', '.rmvb', '.iso', '.wmv', '.m2ts', '.mpg', '.flv', '.rm'}
                    video_files = [f for f in files_in_share if os.path.splitext(f.get('name', ''))[1].lower() in video_exts]
                    video_count = len(video_files)
                    total_size_bytes = sum(f.get('size', 0) for f in files_in_share)
                    
                    if video_count > 0:
                        avg_size_bytes = total_size_bytes / video_count
                    else:
                        avg_size_bytes = total_size_bytes / len(files_in_share) if files_in_share else 0
                    
                    total_size_str = get_formatted_size(total_size_bytes)
                    avg_size_str = get_formatted_size(avg_size_bytes)
                    
                    # 3. é“¾æ¥ä¿®å¤
                    if display_msg_url and not display_msg_url.startswith('http'):
                        display_msg_url = f"https://t.me/{display_msg_url}"
                except Exception as e:
                    logger.warning(f"ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥: {e}")

            if files_in_share:
                total_f = len(files_in_share)
                success_f = 0
                logger.info(f"è§£ææˆåŠŸï¼Œå…± {total_f} ä¸ªæ–‡ä»¶ï¼Œå°è¯•ç§’ä¼ ...")
                
                # æ–‡ä»¶å¤¹ç¼“å­˜ (é¿å…é‡å¤APIè¯·æ±‚)
                folder_cache = {}
                
                # === B. å°è¯•ç§’ä¼ åˆ° 123 (å¸¦ç›®å½•ç»“æ„) ===
                for i, f_info in enumerate(files_in_share):
                    try:
                        # 1. è·¯å¾„è§£æ
                        raw_path = f_info.get('path', '').strip('/')
                        path_parts = raw_path.split('/')
                        file_name = path_parts.pop()
                        
                        # 2. æ„å»ºç›®å½•é“¾ (æ ¹ç›®å½•å + å­ç›®å½•)
                        dir_chain = []
                        if root_share_name:
                            dir_chain.append(root_share_name)
                        dir_chain.extend([p for p in path_parts if p])
                        
                        # 3. é€’å½’å®šä½ç›®æ ‡æ–‡ä»¶å¤¹ID
                        current_pid = pid_for_123
                        
                        for folder_name in dir_chain:
                            cache_key = f"{current_pid}_{folder_name}"
                            if cache_key in folder_cache:
                                current_pid = folder_cache[cache_key]
                                continue
                            
                            found_id = find_child_folder_id(client123, current_pid, folder_name)
                            if found_id:
                                folder_cache[cache_key] = found_id
                                current_pid = found_id
                            else:
                                try:
                                    resp = client123.fs_mkdir(folder_name, parent_id=current_pid)
                                    if resp.get("code") == 0:
                                        new_id = resp["data"]["Info"]["FileId"]
                                        folder_cache[cache_key] = new_id
                                        current_pid = new_id
                                except Exception:
                                    pass

                        # 4. æ‰§è¡Œç§’ä¼ 
                        resp = client123.upload_file_fast(
                            file_name=file_name,
                            parent_id=current_pid,
                            file_md5=f_info['md5'],
                            file_size=f_info['size'],
                            duplicate=1
                        )
                        
                        if resp.get("code") == 0 and \
                           (resp.get("data", {}).get("Reuse") or resp.get("data", {}).get("reuse")):
                            success_f += 1
                            
                    except Exception as e:
                        pass
                
                logger.info(f"123ç›´è¿ç§’ä¼ ç»“æœ: {success_f}/{total_f}")
                
                # å…¨é‡æˆåŠŸï¼Œæµç¨‹ç»“æŸ
                if success_f == total_f and total_f > 0:
                    all_rapid_success = True
                    status = "âœ… 189âš¡123äº‘ç›˜æé€Ÿç§’ä¼ æˆåŠŸ"
                    # [ä¼˜åŒ–] æé€Ÿç§’ä¼ æˆåŠŸå›å¤
                    result_msg = (
                        f"âœ… 189âš¡123äº‘ç›˜æé€Ÿç§’ä¼ æˆåŠŸï¼\n"
                        f"ğŸ“ åç§°: {root_share_name}\n"
                        f"ğŸ“¨ æ¶ˆæ¯: {display_msg_url}\n"
                        f"ğŸŒ é“¾æ¥: {target_url}\n"
                        f"ğŸ¬ è§†é¢‘: {video_count} ä¸ª\n"
                        f"ğŸ“¦ å¤§å°: {total_size_str} | å¹³å‡: {avg_size_str}\n"
                        f"âœ¨ é›¶æµé‡ Â· ç§’çº§ä¼ è¾“ Â· ä¸å ç©ºé—´"
                    )
                    notifier.send_message(result_msg)
                    save_message(message_id, date_str, message_url, target_url, status, result_msg)
                    continue # è·³è¿‡åç»­çš„å…œåº•é€»è¾‘
            
            # === C. å…œåº•è½¬å­˜ (å­˜åˆ° 189) ===
            # å¦‚æœç§’ä¼ æœªè¦†ç›–æ‰€æœ‰æ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œè€åŠæ³•
            if not all_rapid_success:
                logger.info("123ç§’ä¼ æœªè¦†ç›–ï¼Œæ‰§è¡Œå…œåº•è½¬å­˜åˆ°å¤©ç¿¼äº‘ç›˜...")
                
                # ä½¿ç”¨ä¸“é—¨çš„ 189 å…œåº•ç›®å½• ID
                result = save_189_link(client189, target_url, pid_for_189)
                
                if result:
                    status = "è½¬å­˜æˆåŠŸ"
                    # [ä¼˜åŒ–] å…œåº•è½¬å­˜æˆåŠŸå›å¤
                    result_msg = (
                        f"âœ… å·²è½¬å­˜è‡³å¤©ç¿¼äº‘ç›˜ (123ç§’ä¼ æœªå…¨è¦†ç›–)\n"
                        f"ğŸ“ åç§°: {root_share_name}\n"
                        f"ğŸ“¨ æ¶ˆæ¯: {display_msg_url}\n"
                        f"ğŸŒ é“¾æ¥: {target_url}\n"
                        f"ğŸ¬ ç»Ÿè®¡: {video_count} ä¸ªè§†é¢‘ (éœ€åŒæ­¥)\n"
                        f"ğŸ“¦ å¤§å°: {total_size_str} | å¹³å‡: {avg_size_str}\n"
                        f"ğŸ’¡ æç¤º: è¯·ç¨åä½¿ç”¨ /sync189 å®Œæˆè¿ç§»"
                    )
                else:
                    status = "è½¬å­˜å¤±è´¥"
                    # [ä¼˜åŒ–] å¤±è´¥å›å¤
                    result_msg = (
                        f"âŒ å¤©ç¿¼äº‘è½¬å­˜å¤±è´¥ (ç©ºé—´ä¸è¶³æˆ–å…¶ä»–é”™è¯¯)\n"
                        f"ğŸ“ åç§°: {root_share_name}\n"
                        f"ğŸ“¨ æ¶ˆæ¯: {display_msg_url}\n"
                        f"ğŸŒ é“¾æ¥: {target_url}\n"
                        f"ğŸ“¦ å¤§å°: {total_size_str}\n"
                        f"ğŸ”§ å»ºè®®: è¯·æ£€æŸ¥å¤©ç¿¼äº‘ç©ºé—´æˆ–CookieçŠ¶æ€"
                    )
                
                notifier.send_message(result_msg)
                save_message(message_id, date_str, message_url, target_url, status, result_msg)

        except Exception as e:
            logger.error(f"å¤„ç†ç›‘æ§æ¶ˆæ¯å¼‚å¸¸: {e}")
            save_message(message_id, date_str, message_url, target_url, "æŠ¥é”™", f"å¼‚å¸¸: {e}")


def main():     
    from server import app
    flask_thread = threading.Thread(target=lambda: app.run(host='0.0.0.0', port=12366, debug=False, use_reloader=False))
    flask_thread.daemon = True
    flask_thread.start()
    while (os.getenv("ENV_WEB_PASSPORT", "") == "") or (os.getenv("ENV_123_CLIENT_ID", "") == ""):
        try:
            logger.warning("è¯·æ£€æŸ¥docker-compose.ymlä¸­çš„ ENV_WEB_PASSPORT ä»¥åŠé…ç½®webé¡µé¢çš„ ENV_123_CLIENT_ID æ˜¯å¦å¡«å†™å®Œæ•´ï¼Œå¯å‰å¾€ https://hub.docker.com/r/dydydd/123bot æŸ¥çœ‹éƒ¨ç½²æ–¹æ³•")
            bot.send_message(TG_ADMIN_USER_ID,f"è¯·æ£€æŸ¥docker-compose.ymlä¸­çš„ ENV_WEB_PASSPORT ä»¥åŠé…ç½®webé¡µé¢çš„ ENV_123_CLIENT_ID æ˜¯å¦å¡«å†™å®Œæ•´ï¼Œå¯å‰å¾€ https://hub.docker.com/r/dydydd/123bot æŸ¥çœ‹éƒ¨ç½²æ–¹æ³•")
        except Exception as e:
            logger.error(f"å‘é€æ¶ˆæ¯å¤±è´¥: {str(e)}")
        time.sleep(60)
    threading.Thread(target=ptto123, daemon=True).start()
    logger.info(f"123è½¬å­˜ç›®æ ‡ç›®å½•ID: {UPLOAD_TARGET_PID} | æ£€æŸ¥é—´éš”: {CHECK_INTERVAL}åˆ†é’Ÿ")
    init_database()
    client = init_123_client()

    global bot_thread
    # åˆå§‹å¯åŠ¨botçº¿ç¨‹
    bot_thread = threading.Thread(target=start_bot_thread, daemon=True)
    bot_thread.start()
    # [æ–°å¢] å¯åŠ¨äººå½¢æ¨¡å—çº¿ç¨‹
    logger.info("æ­£åœ¨å¯åŠ¨äººå½¢æ¨¡å—çº¿ç¨‹...")
    threading.Thread(target=start_userbot_listener, daemon=True).start()
    
    schedule.every(20).seconds.do(check_task)

    if get_int_env("ENV_189_TGMONITOR_SWITCH", 0):
        
        try:            
            # è¯»å–189æ¸…ç†é…ç½®
            env_189_clear_pid = os.getenv("ENV_189_CLEAR_PID", "")
            env_189_clear_period = get_int_env("ENV_189_CLEAR_PERIOD", 6)
            clear_folder_ids = [pid.strip() for pid in env_189_clear_pid.split(",") if pid.strip()]
            
            # å®šä¹‰å®šæ—¶æ¸…ç†å‡½æ•°
            def clear_189_folders():
                logger.info(f"===== å¼€å§‹æ‰§è¡Œå¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹æ¸…ç†ä»»åŠ¡ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰=====")
                try:
                    # å°è¯•åˆ é™¤æ–‡ä»¶å¤¹å†…å®¹ï¼ˆä¸æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼Œä¾èµ–æ–¹æ³•å†…éƒ¨å¤„ç†ï¼‰
                    for folder_id in clear_folder_ids:
                        logger.info(f"åˆ é™¤æ–‡ä»¶å¤¹ {folder_id} ä¸­çš„å†…å®¹...")
                        try:
                            client189.delete_folder_contents(folder_id)
                            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶å¤¹ {folder_id} ä¸­çš„å†…å®¹")
                        except Exception as e:
                            logger.error(f"åˆ é™¤æ–‡ä»¶å¤¹ {folder_id} å†…å®¹å¤±è´¥: {str(e)}")
                    
                    # æ¸…ç©ºå›æ”¶ç«™
                    logger.info("æ¸…ç©ºå›æ”¶ç«™...")
                    try:
                        if client189.empty_recycle_bin():
                            logger.info("æˆåŠŸæ‰§è¡Œå¤©ç¿¼ç½‘ç›˜æ–‡ä»¶æ¸…ç†ä»»åŠ¡")
                            reply_thread_pool.submit(send_message, f"âœ…æˆåŠŸæ‰§è¡Œå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™ä»»åŠ¡ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                        else:
                            logger.info("å¤©ç¿¼ç½‘ç›˜æ–‡ä»¶æ¸…ç†å¤±è´¥")
                            reply_thread_pool.submit(send_message, f"âŒå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™å¤±è´¥ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                    except Exception as e:
                        logger.error(f"æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {str(e)}")
                        reply_thread_pool.submit(send_message, f"âŒå¤©ç¿¼ç½‘ç›˜æ¸…ç©ºå›æ”¶ç«™å¤±è´¥: {str(e)}ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')})")
                except Exception as e:
                    logger.error(f"å¤©ç¿¼äº‘ç›˜æ¸…ç†ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
                logger.info("===== å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæ¯• =====")
            
            # è®¾ç½®å®šæ—¶ä»»åŠ¡ï¼Œæ¯env_189_clear_periodå°æ—¶æ‰§è¡Œä¸€æ¬¡
            if clear_folder_ids:
                logger.info(f"è®¾ç½®å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹å®šæ—¶æ¸…ç†ä»»åŠ¡ï¼Œæ¯{env_189_clear_period}å°æ—¶æ‰§è¡Œä¸€æ¬¡")
                schedule.every(env_189_clear_period).hours.do(clear_189_folders)
                # ç«‹å³æ‰§è¡Œä¸€æ¬¡æ¸…ç†ä»»åŠ¡
                clear_189_folders()
            else:
                logger.info("æœªé…ç½®ENV_189_CLEAR_PIDï¼Œè·³è¿‡å¤©ç¿¼äº‘ç›˜æ–‡ä»¶å¤¹å®šæ—¶æ¸…ç†ä»»åŠ¡")
        except Exception as e:
            logger.error(f"ç™»å½•å‡ºç°é”™è¯¯: {e}")

    try:
        while True:
            logger.info(f"===== å¼€å§‹æ£€æŸ¥ï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰ï¼Œå½“å‰ç‰ˆæœ¬ {version}=====")
            if AUTHORIZATION:
                client = init_123_client()
                new_messages = get_latest_messages()
                schedule.run_pending()
                if new_messages:
                    for msg in new_messages:
                        message_id, date_str, message_url, target_url, message_text = msg
                        logger.info(f"å¤„ç†æ–°æ¶ˆæ¯: {message_id} | {target_url}")
                        # è·å–æ’é™¤å…³é”®è¯ç¯å¢ƒå˜é‡ï¼ˆå¤šä¸ªå…³é”®è¯ç”¨|åˆ†éš”ï¼‰
                        # å½“æ’é™¤å…³é”®è¯ä¸ºç©ºæ—¶ï¼Œå…¨éƒ½ä¸æ’é™¤
                        exclude_filter = os.environ.get('ENV_EXCLUDE_FILTER', '')
                        exclude_pattern = re.compile(exclude_filter) if exclude_filter else None

                        # æ£€æŸ¥æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶ä¸”ä¸åŒ…å«æ’é™¤å…³é”®è¯
                        is_match = filter_pattern.search(target_url) or filter_pattern.search(message_text)
                        is_excluded = exclude_pattern and (exclude_pattern.search(target_url) or exclude_pattern.search(message_text))

                        if not is_match:
                            status = "æœªè½¬å­˜"
                            result_msg = f"æœªåŒ¹é…è¿‡æ»¤æ¡ä»¶ï¼ˆ{FILTER}ï¼‰ï¼Œè·³è¿‡è½¬å­˜"
                            logger.info(result_msg)
                            time.sleep(1)
                        elif is_excluded:
                            status = "æœªè½¬å­˜"
                            result_msg = f"åŒ…å«æ’é™¤å…³é”®è¯ï¼ˆ{exclude_filter}ï¼‰ï¼Œè·³è¿‡è½¬å­˜"
                            logger.info(result_msg)
                            time.sleep(1)
                        else:
                            logger.info(f"æ¶ˆæ¯åŒ¹é…è¿‡æ»¤æ¡ä»¶ï¼ˆ{FILTER}ï¼‰ï¼Œå¼€å§‹è½¬å­˜...")
                            
                            # äºŒæ¬¡è¿‡æ»¤å…³é”®è¯é…ç½®ï¼ˆå½“æŸæ¡æ¶ˆæ¯è§¦å‘è½¬å­˜åï¼Œå¦‚è¿›ä¸€æ­¥æ»¡è¶³ä¸‹é¢çš„è¦æ±‚ï¼Œåˆ™è½¬ç§»åˆ°ç‰¹å®šçš„æ–‡ä»¶å¤¹ï¼‰
                            # æ ¼å¼ä¸ºï¼šDV:1,DOLBY VISION:2,SSTA:3 å³æ»¡è¶³DVå…³é”®è¯è½¬ç§»åˆ°IDä¸º1çš„æ–‡ä»¶å¤¹ï¼Œæ»¡è¶³SSTAå…³é”®è¯è½¬ç§»åˆ°IDä¸º3çš„æ–‡ä»¶å¤¹
                            # å¦‚æœENV_SECOND_FILTERä¸ºç©ºï¼Œåˆ™å…¨éƒ¨è½¬ç§»è‡³ENV_123_UPLOAD_PID
                            ENV_SECOND_FILTER = os.getenv("ENV_SECOND_FILTER", "")
                            transfer_id=UPLOAD_TARGET_PID
                            
                            # æ ¹æ®å…³é”®è¯ç­›é€‰å¹¶è®¾ç½®transfer_id
                            # ENV_SECOND_FILTER.strip() ç”¨äºå»é™¤å­—ç¬¦ä¸²å‰åçš„ç©ºç™½å­—ç¬¦ï¼ˆç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ç­‰ï¼‰
                            # è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿ç¯å¢ƒå˜é‡å€¼å‰åæœ‰ç©ºæ ¼ä¹Ÿèƒ½æ­£ç¡®å¤„ç†ï¼Œé¿å…å› ç©ºç™½å­—ç¬¦å¯¼è‡´çš„é€»è¾‘é”™è¯¯
                            # å¦‚æœå»é™¤ç©ºç™½åå­—ç¬¦ä¸²ä¸ä¸ºç©ºï¼Œåˆ™æ‰§è¡ŒäºŒæ¬¡è¿‡æ»¤é€»è¾‘
                            if ENV_SECOND_FILTER.strip():
                                try:
                                    # è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™ï¼Œæ ¼å¼ä¸ºï¼šå…³é”®è¯:æ–‡ä»¶å¤¹ID,å…³é”®è¯:æ–‡ä»¶å¤¹ID,...
                                    filter_rules = ENV_SECOND_FILTER.split(',')
                                    for rule in filter_rules:
                                        if ':' in rule:
                                            # åˆ†å‰²å…³é”®è¯å’Œæ–‡ä»¶å¤¹IDï¼Œä½†ä¿ç•™å…³é”®è¯ä¸­çš„ç©ºæ ¼ï¼ˆå¦‚"DOLBY VISION"ä¸­çš„ç©ºæ ¼ä¼šè¢«ä¿ç•™ï¼‰
                                            keyword, folder_id = rule.split(':', 1)
                                            # keyword.strip() ç”¨äºç¡®ä¿å…³é”®è¯ä¸ä¸ºç©ºå­—ç¬¦ä¸²
                                            # æ³¨æ„ï¼šå…³é”®è¯å†…éƒ¨çš„ç©ºæ ¼ï¼ˆå¦‚"DOLBY VISION"ä¸­çš„ç©ºæ ¼ï¼‰ä¸ä¼šè¢«å»é™¤ï¼Œä¼šä½œä¸ºå…³é”®è¯çš„ä¸€éƒ¨åˆ†è¿›è¡ŒåŒ¹é…
                                            if (keyword.strip() and 
                                                (keyword in message_text or 
                                                 (target_url and keyword in target_url))):
                                                transfer_id = int(folder_id.strip())
                                                logger.info(f"æ¶ˆæ¯åŒ¹é…äºŒæ¬¡è¿‡æ»¤å…³é”®è¯ '{keyword}'ï¼Œå°†è½¬å­˜åˆ°æ–‡ä»¶å¤¹ID: {folder_id}")
                                                reply_thread_pool.submit(send_message, f"æ¶ˆæ¯åŒ¹é…äºŒæ¬¡è¿‡æ»¤å…³é”®è¯ '{keyword}'ï¼Œå°†è½¬å­˜åˆ°æ–‡ä»¶å¤¹ID: {folder_id}")
                                                break
                                except Exception as e:
                                    logger.error(f"è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
                                    reply_thread_pool.submit(send_message, f"è§£æäºŒæ¬¡è¿‡æ»¤è§„åˆ™å¤±è´¥: {e}")
                            if target_url:                                
                                result = transfer_shared_link_optimize(client, target_url, transfer_id)
                                if result:
                                    status = "è½¬å­˜æˆåŠŸ"
                                    result_msg = f"âœ…123äº‘ç›˜è½¬å­˜æˆåŠŸ\næ¶ˆæ¯å†…å®¹: {message_url}\né“¾æ¥: {target_url}"
                                    reply_thread_pool.submit(send_message, result_msg)
                                else:                               
                                    status = "è½¬å­˜å¤±è´¥"
                                    result_msg = f"âŒ123äº‘ç›˜è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\né“¾æ¥: {target_url}"
                                    reply_thread_pool.submit(send_message, result_msg)
                            else:
                                full_links = extract_123_links_from_full_text(message_text)
                                if full_links:
                                    for link in full_links:
                                        if parse_share_link(message_text, link, transfer_id, False):
                                            status = "è½¬å­˜æˆåŠŸ"
                                            result_msg = f"âœ…123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜æˆåŠŸ\næ¶ˆæ¯å†…å®¹: {message_url}\n"
                                            reply_thread_pool.submit(send_message, result_msg)
                                        else:
                                            status = "è½¬å­˜å¤±è´¥"
                                            result_msg = f"âŒ123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\n"  
                                            #notifier.send_message(result_msg)     
                                else:
                                    status = "è½¬å­˜å¤±è´¥"
                                    result_msg = f"âŒ123äº‘ç›˜ç§’ä¼ é“¾æ¥è½¬å­˜å¤±è´¥\næ¶ˆæ¯å†…å®¹: {message_url}\n"  
                                    #notifier.send_message(result_msg)     
                            time.sleep(2)
                        save_message(message_id, date_str, message_url, target_url, status, result_msg)
                else:
                    logger.info("æœªå‘ç°æ–°çš„123åˆ†äº«é“¾æ¥")

            if get_int_env("ENV_115_TGMONITOR_SWITCH", 0):
                            try:
                                # ç¡®ä¿å¯¼å…¥äº†æ¨¡å—
                                from bot115 import tg_115monitor
                                tg_115monitor()
                            except Exception as e:
                                # [å…³é”®] æ•è·å¼‚å¸¸ï¼Œåªæ‰“å°æ—¥å¿—ï¼Œä¸è®©ç¨‹åºé€€å‡º
                                logger.error(f"115ç›‘æ§ä»»åŠ¡å‡ºé”™ (å·²è·³è¿‡ï¼Œé˜²æ­¢å®¹å™¨é‡å¯): {str(e)}")          
            
            if get_int_env("ENV_189_TGMONITOR_SWITCH", 0):
                try:
                    # ç›´æ¥è°ƒç”¨æœ¬æ–‡ä»¶å®šä¹‰çš„ tg_189monitor (ä¸Šé¢é‚£ä¸ªå…¨èƒ½ç‰ˆ)
                    tg_189monitor(client189)
                except Exception as e:
                    logger.error(f"å¤©ç¿¼äº‘ç›‘æ§ä»»åŠ¡å‡ºé”™: {e}")
            
            logger.info(f"ä¼‘æ¯{CHECK_INTERVAL}åˆ†é’Ÿï¼Œå½“å‰ç‰ˆæœ¬ {version}...")
            
            try:
                next_time = datetime.now() + timedelta(minutes=CHECK_INTERVAL)
                logger.info(f"ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´æ˜¯ï¼š{next_time.strftime('%Y-%m-%d %H:%M:%S')}")
            except Exception as e:
                logger.error(f"è®¡ç®—ä¸‹æ¬¡æ£€æŸ¥æ—¶é—´å‡ºé”™: {e}")

            total_wait_seconds = CHECK_INTERVAL * 60
            elapsed_seconds = 0
            # æ‹†åˆ†ç­‰å¾…æ—¶é—´ï¼Œæ¯1ç§’æ£€æŸ¥ä¸€æ¬¡å®šæ—¶ä»»åŠ¡ï¼ˆ20ç§’å†…ä¼šæ£€æŸ¥20æ¬¡ï¼Œæ»¡è¶³20ç§’æ£€æŸ¥ä¸€æ¬¡çš„éœ€æ±‚ï¼‰
            exit=0
            while elapsed_seconds < total_wait_seconds:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é€€å‡ºï¼ˆåœ¨ä¼‘æ¯å‰æ£€æŸ¥ï¼Œç¡®ä¿åªåœ¨è®°å½•æ—¥å¿—åé€€å‡ºï¼‰
                try:
                    # ç›´æ¥è®¿é—®should_exitå˜é‡è€Œä¸æ˜¯é€šè¿‡globals()æ£€æŸ¥
                    with should_exit.get_lock():
                        if link_process_lock.acquire(blocking=False):
                            try:
                                if should_exit.value:
                                    logger.info("æ£€æµ‹åˆ°é€€å‡ºæ ‡å¿—ï¼Œå­è¿›ç¨‹å°†åœ¨ä¼‘æ¯å‰é€€å‡º")
                                    exit=1
                                    break   
                            finally:
                                link_process_lock.release()
                except Exception as e:
                    logger.error(f"æ£€æŸ¥é€€å‡ºæ ‡å¿—æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                time.sleep(1)  # çŸ­é—´éš”ä¼‘çœ ï¼Œä¿è¯20ç§’å†…è‡³å°‘æ£€æŸ¥ä¸€æ¬¡
                elapsed_seconds += 1
            if exit:
                break

    except KeyboardInterrupt:
        logger.info("ç¨‹åºå·²åœæ­¢")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {str(e)}")
        #notifier.send_message(f"tgto123ï¼šç¨‹åºå¼‚å¸¸ç»ˆæ­¢: {str(e)}")

    
from ptto115 import ptto123process
def ptto123():
    while get_int_env("ENV_PTTO123_SWITCH", 0) or get_int_env("ENV_PTTO115_SWITCH", 0):
        try:
            ptto123process()
        except Exception as e:
            logger.error(f"ptto123çº¿ç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
            bot.send_message(TG_ADMIN_USER_ID, f"ptto123çº¿ç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}")
            time.sleep(300)

import threading
import multiprocessing
import signal

if __name__ == "__main__":
    # è®¾ç½®å…¨å±€é»˜è®¤æ¨¡å¼ä¸º spawn
    multiprocessing.set_start_method('spawn')
# å…¨å±€å…±äº«æ ‡å¿—ï¼Œç”¨äºé€šçŸ¥å­è¿›ç¨‹é€€å‡º
should_exit = multiprocessing.Value('b', False)

# å­è¿›ç¨‹è¿è¡Œçš„å‡½æ•°
def run_main(exit_flag):
    # å°†å…±äº«å˜é‡è®¾ç½®ä¸ºå…¨å±€å˜é‡ï¼Œä»¥ä¾¿mainå‡½æ•°å¯ä»¥è®¿é—®
    global should_exit
    should_exit = exit_flag
    try:
        main()
    except Exception as e:
        logger.error(f"å­è¿›ç¨‹è¿è¡Œå¼‚å¸¸: {str(e)}")

if __name__ == "__main__":
    # æ£€æŸ¥db\user.envæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä»templete.envåˆ›å»º
    user_state_manager.clear_state(TG_ADMIN_USER_ID)
    user_env_path = os.path.join('db', 'user.env')
    if not os.path.exists(user_env_path):
        logger.info(f"user.envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä»templete.envåˆ›å»º...")
        # ç¡®ä¿dbç›®å½•å­˜åœ¨
        os.makedirs('db', exist_ok=True)
        # å¤åˆ¶templete.envåˆ°dbç›®å½•å¹¶é‡å‘½åä¸ºuser.env
        if os.path.exists('templete.env'):
            shutil.copy2('templete.env', user_env_path)
            logger.info(f"æˆåŠŸåˆ›å»ºuser.envæ–‡ä»¶")
        else:
            logger.warning(f"è­¦å‘Š: templete.envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºuser.env")
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)

    while True:
        try:            
            # [ä¿®æ”¹] æ„é€ æŒ‰é’®é”®ç›˜å¹¶å‘é€ç®€æ´æ¶ˆæ¯
            markup = InlineKeyboardMarkup()
            markup.row(InlineKeyboardButton("ğŸ“– ä½¿ç”¨è¯´æ˜", callback_data="show_usage"),
                       InlineKeyboardButton("âš ï¸ å…è´£å£°æ˜", callback_data="show_disclaimer"))
            markup.row(InlineKeyboardButton("ğŸ¤– äººå½¢å‘½ä»¤", callback_data="show_userbot_help"),
                       InlineKeyboardButton("ğŸŒŸ é¡¹ç›®åœ°å€", url="https://t.me/xx123pan1"))
            
            # å‘é€ç®€æ´çš„å¯åŠ¨æ¶ˆæ¯
            bot.send_message(
                TG_ADMIN_USER_ID, 
                f"å®å’šï¼Œæˆ‘å·²æˆåŠŸå¯åŠ¨ï¼Œæ¬¢è¿ä½¿ç”¨123botï¼\n\n â•â•â•â•â•å½“å‰ç‰ˆæœ¬â€{version}â•â•â•â•â•\n\n",
                parse_mode='HTML', 
                reply_markup=markup
            )
            break
            
        except Exception as e:
            logger.error(f"ç”±äºç½‘ç»œç­‰åŸå› æ— æ³•ä¸TG Botå»ºç«‹é€šä¿¡ï¼Œ30ç§’åé‡è¯•...: {str(e)}")
            time.sleep(30)

    # ä¸»è¿›ç¨‹æ§åˆ¶é€»è¾‘
    restart_time = time_datetime(3, 0, 0)  # è®¾ç½®åœ¨æ¯å¤©ä¸‹åˆ6:50:00é‡å¯
    
    # è®¡ç®—åˆå§‹çš„ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´æˆ³
    def calculate_next_restart_time():
        today = datetime.now().date()
        # è®¡ç®—ä»Šå¤©çš„é‡å¯æ—¶é—´æ—¶é—´æˆ³
        today_restart_time = datetime.combine(today, restart_time).timestamp()
        # å½“å‰æ—¶é—´æˆ³
        now = datetime.now().timestamp()
        # å¦‚æœå½“å‰æ—¶é—´åœ¨ä»Šå¤©çš„é‡å¯æ—¶é—´ä¹‹å‰ï¼Œåˆ™ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´ä¸ºä»Šå¤©é‡å¯æ—¶é—´
        # å¦‚æœå½“å‰æ—¶é—´å·²è¿‡ä»Šå¤©çš„é‡å¯æ—¶é—´ï¼Œåˆ™ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´ä¸ºæ˜å¤©é‡å¯æ—¶é—´
        if now < today_restart_time:
            next_restart = today_restart_time
        else:
            next_restart = datetime.combine(today + timedelta(days=1), restart_time).timestamp()
        return next_restart
    
    next_restart_time = calculate_next_restart_time()
    
    while True:
        try:
            # åˆ›å»ºå¹¶å¯åŠ¨å­è¿›ç¨‹ï¼Œä¼ é€’å…±äº«å˜é‡
            main_process = multiprocessing.Process(target=run_main, args=(should_exit,))
            main_process.daemon = False
            main_process.start()
            logger.info(f"å­è¿›ç¨‹ {main_process.pid} å·²å¯åŠ¨")
            logger.info(f"ä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ç›‘æ§å­è¿›ç¨‹å’Œé‡å¯æ—¶é—´
            while main_process.is_alive():
                # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾é‡å¯æ—¶é—´
                now = datetime.now().timestamp()
                
                if now >= next_restart_time:
                    # è®¾ç½®é€€å‡ºæ ‡å¿—ï¼Œé€šçŸ¥å­è¿›ç¨‹
                    with should_exit.get_lock():
                        should_exit.value = True
                    
                    # ç­‰å¾…å­è¿›ç¨‹é€€å‡ºï¼Œæœ€å¤šç­‰å¾…60ç§’
                    wait_time = 0
                    max_wait = 1800
                    while main_process.is_alive() and wait_time < max_wait:
                        time.sleep(1)
                        wait_time += 1
                    
                    # å¦‚æœå­è¿›ç¨‹è¿˜åœ¨è¿è¡Œï¼Œè·³è¿‡æ­¤æ¬¡é‡å¯
                    if main_process.is_alive():
                        logger.warning(f"å­è¿›ç¨‹ {main_process.pid} æœªèƒ½åœ¨è§„å®šæ—¶é—´å†…è‡ªè¡Œé€€å‡º,è·³è¿‡æ­¤æ¬¡é‡å¯")
                        with should_exit.get_lock():
                            should_exit.value = False
                        next_restart_time = calculate_next_restart_time()
                        logger.info(f"ä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
                        continue

                    # é‡ç½®é€€å‡ºæ ‡å¿—
                    with should_exit.get_lock():
                        should_exit.value = False                    
                    # è®¡ç®—ä¸‹ä¸€æ¬¡é‡å¯æ—¶é—´
                    next_restart_time = calculate_next_restart_time()
                    logger.info(f"å·²å®Œæˆæ¸…ç†å†…å­˜ï¼Œä¸‹ä¸€æ¬¡è®¡åˆ’æ¸…ç†å†…å­˜æ—¶é—´: {datetime.fromtimestamp(next_restart_time).strftime('%Y-%m-%d %H:%M:%S')}")
                    break
                
                # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
                time.sleep(10)
            
            # å­è¿›ç¨‹é€€å‡ºåï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡å¯
            if not main_process.is_alive():
                logger.info(f"å­è¿›ç¨‹ {main_process.pid} å·²é€€å‡ºï¼Œç­‰å¾…5ç§’åé‡å¯")
                time.sleep(5)
            
        except KeyboardInterrupt:
            logger.info("æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ç»ˆæ­¢å­è¿›ç¨‹...")
            if 'main_process' in locals() and main_process.is_alive():
                try:
                    main_process.terminate()
                    main_process.join(timeout=10)
                except Exception as e:
                    logger.error(f"ç»ˆæ­¢å­è¿›ç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            logger.info("ç¨‹åºå·²åœæ­¢")
            break
